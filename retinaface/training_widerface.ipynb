{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -U gdown\n",
        "\n",
        "!gdown --id 11UGV3nbVv1x9IC--_tK3Uxf7hA6rlbsS\n",
        "!unzip -qq widerface.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blhDWtEJ_tt-",
        "outputId": "99b749c5-1a84-4e0d-a64c-8f3027423d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.6.0\n",
            "/usr/local/lib/python3.8/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11UGV3nbVv1x9IC--_tK3Uxf7hA6rlbsS\n",
            "To: /content/widerface.zip\n",
            "100% 1.83G/1.83G [00:20<00:00, 91.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvInvj1H3XA1"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import math\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "from itertools import product as product\n",
        "from math import ceil\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.models as models\n",
        "import torchvision.models._utils as _utils\n",
        "import torchvision.models.detection.backbone_utils as backbone_utils\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = {\n",
        "    'name': 'mobilenet0.25',\n",
        "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
        "    'steps': [8, 16, 32],\n",
        "    'variance': [0.1, 0.2],\n",
        "    'clip': False,\n",
        "    'loc_weight': 2.0,\n",
        "    'gpu_train': True,\n",
        "    'batch_size': 32,\n",
        "    'ngpu': 1,\n",
        "    'epoch': 250,\n",
        "    'decay1': 190,\n",
        "    'decay2': 220,\n",
        "    'image_size': 640,\n",
        "    'pretrain': True,\n",
        "    'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
        "    'in_channel': 32,\n",
        "    'out_channel': 64\n",
        "}\n",
        "\n",
        "GPU = cfg[\"gpu_train\"]"
      ],
      "metadata": {
        "id": "U_60QrGw3XaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utils"
      ],
      "metadata": {
        "id": "sV-NpdL03zGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def intersect(box_a, box_b):\n",
        "    \"\"\" We resize both tensors to [A,B,2] without new malloc:\n",
        "    [A,2] -> [A,1,2] -> [A,B,2]\n",
        "    [B,2] -> [1,B,2] -> [A,B,2]\n",
        "    Then we compute the area of intersect between box_a and box_b.\n",
        "    Args:\n",
        "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
        "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
        "    Return:\n",
        "      (tensor) intersection area, Shape: [A,B].\n",
        "    \"\"\"\n",
        "    A = box_a.size(0)\n",
        "    B = box_b.size(0)\n",
        "    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n",
        "    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n",
        "                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n",
        "    inter = torch.clamp((max_xy - min_xy), min=0)\n",
        "    return inter[:, :, 0] * inter[:, :, 1]\n",
        "\n",
        "def log_sum_exp(x):\n",
        "    \"\"\"Utility function for computing log_sum_exp while determining\n",
        "    This will be used to determine unaveraged confidence loss across\n",
        "    all examples in a batch.\n",
        "    Args:\n",
        "        x (Variable(tensor)): conf_preds from conf layers\n",
        "    \"\"\"\n",
        "    x_max = x.data.max()\n",
        "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
        "\n",
        "def point_form(boxes):\n",
        "    \"\"\" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n",
        "    representation for comparison to point form ground truth data.\n",
        "    Args:\n",
        "        boxes: (tensor) center-size default boxes from priorbox layers.\n",
        "    Return:\n",
        "        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n",
        "    \"\"\"\n",
        "    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n",
        "                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n",
        "\n",
        "def jaccard(box_a, box_b):\n",
        "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
        "    is simply the intersection over union of two boxes.  Here we operate on\n",
        "    ground truth boxes and default boxes.\n",
        "    E.g.:\n",
        "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
        "    Args:\n",
        "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
        "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
        "    Return:\n",
        "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
        "    \"\"\"\n",
        "    inter = intersect(box_a, box_b)\n",
        "    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n",
        "              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n",
        "    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n",
        "              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n",
        "    union = area_a + area_b - inter\n",
        "    return inter / union  # [A,B]\n",
        "\n",
        "def _crop(image, boxes, labels, landm, img_dim):\n",
        "    height, width, _ = image.shape\n",
        "    pad_image_flag = True\n",
        "\n",
        "    for _ in range(250):\n",
        "        \"\"\"\n",
        "        if random.uniform(0, 1) <= 0.2:\n",
        "            scale = 1.0\n",
        "        else:\n",
        "            scale = random.uniform(0.3, 1.0)\n",
        "        \"\"\"\n",
        "        PRE_SCALES = [0.3, 0.45, 0.6, 0.8, 1.0]\n",
        "        scale = random.choice(PRE_SCALES)\n",
        "        short_side = min(width, height)\n",
        "        w = int(scale * short_side)\n",
        "        h = w\n",
        "\n",
        "        if width == w:\n",
        "            l = 0\n",
        "        else:\n",
        "            l = random.randrange(width - w)\n",
        "        if height == h:\n",
        "            t = 0\n",
        "        else:\n",
        "            t = random.randrange(height - h)\n",
        "        roi = np.array((l, t, l + w, t + h))\n",
        "\n",
        "        value = matrix_iof(boxes, roi[np.newaxis])\n",
        "        flag = (value >= 1)\n",
        "        if not flag.any():\n",
        "            continue\n",
        "\n",
        "        centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n",
        "        mask_a = np.logical_and(roi[:2] < centers, centers < roi[2:]).all(axis=1)\n",
        "        boxes_t = boxes[mask_a].copy()\n",
        "        labels_t = labels[mask_a].copy()\n",
        "        landms_t = landm[mask_a].copy()\n",
        "        landms_t = landms_t.reshape([-1, 5, 2])\n",
        "\n",
        "        if boxes_t.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n",
        "\n",
        "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n",
        "        boxes_t[:, :2] -= roi[:2]\n",
        "        boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n",
        "        boxes_t[:, 2:] -= roi[:2]\n",
        "\n",
        "        # landm\n",
        "        landms_t[:, :, :2] = landms_t[:, :, :2] - roi[:2]\n",
        "        landms_t[:, :, :2] = np.maximum(landms_t[:, :, :2], np.array([0, 0]))\n",
        "        landms_t[:, :, :2] = np.minimum(landms_t[:, :, :2], roi[2:] - roi[:2])\n",
        "        landms_t = landms_t.reshape([-1, 10])\n",
        "\n",
        "\n",
        "\t# make sure that the cropped image contains at least one face > 16 pixel at training image scale\n",
        "        b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim\n",
        "        b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim\n",
        "        mask_b = np.minimum(b_w_t, b_h_t) > 0.0\n",
        "        boxes_t = boxes_t[mask_b]\n",
        "        labels_t = labels_t[mask_b]\n",
        "        landms_t = landms_t[mask_b]\n",
        "\n",
        "        if boxes_t.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        pad_image_flag = False\n",
        "\n",
        "        return image_t, boxes_t, labels_t, landms_t, pad_image_flag\n",
        "    return image, boxes, labels, landm, pad_image_flag\n",
        "\n",
        "def encode_landm(matched, priors, variances):\n",
        "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
        "    we have matched (based on jaccard overlap) with the prior boxes.\n",
        "    Args:\n",
        "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
        "            Shape: [num_priors, 10].\n",
        "        priors: (tensor) Prior boxes in center-offset form\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        encoded landm (tensor), Shape: [num_priors, 10]\n",
        "    \"\"\"\n",
        "\n",
        "    # dist b/t match center and prior's center\n",
        "    matched = torch.reshape(matched, (matched.size(0), 5, 2))\n",
        "    priors_cx = priors[:, 0].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
        "    priors_cy = priors[:, 1].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
        "    priors_w = priors[:, 2].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
        "    priors_h = priors[:, 3].unsqueeze(1).expand(matched.size(0), 5).unsqueeze(2)\n",
        "    priors = torch.cat([priors_cx, priors_cy, priors_w, priors_h], dim=2)\n",
        "    g_cxcy = matched[:, :, :2] - priors[:, :, :2]\n",
        "    # encode variance\n",
        "    g_cxcy /= (variances[0] * priors[:, :, 2:])\n",
        "    # g_cxcy /= priors[:, :, 2:]\n",
        "    g_cxcy = g_cxcy.reshape(g_cxcy.size(0), -1)\n",
        "    # return target for smooth_l1_loss\n",
        "    return g_cxcy\n",
        "\n",
        "\n",
        "# Adapted from https://github.com/Hakuyume/chainer-ssd\n",
        "def decode(loc, priors, variances):\n",
        "    \"\"\"Decode locations from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        loc (tensor): location predictions for loc layers,\n",
        "            Shape: [num_priors,4]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded bounding box predictions\n",
        "    \"\"\"\n",
        "\n",
        "    boxes = torch.cat((\n",
        "        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n",
        "        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2\n",
        "    boxes[:, 2:] += boxes[:, :2]\n",
        "    return boxes\n",
        "\n",
        "def decode_landm(pre, priors, variances):\n",
        "    \"\"\"Decode landm from predictions using priors to undo\n",
        "    the encoding we did for offset regression at train time.\n",
        "    Args:\n",
        "        pre (tensor): landm predictions for loc layers,\n",
        "            Shape: [num_priors,10]\n",
        "        priors (tensor): Prior boxes in center-offset form.\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        decoded landm predictions\n",
        "    \"\"\"\n",
        "    landms = torch.cat((priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],\n",
        "                        priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:],\n",
        "                        ), dim=1)\n",
        "    return landms\n",
        "\n",
        "def matrix_iof(a, b):\n",
        "    \"\"\"\n",
        "    return iof of a and b, numpy version for data augenmentation\n",
        "    \"\"\"\n",
        "    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n",
        "    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n",
        "\n",
        "    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n",
        "    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n",
        "    return area_i / np.maximum(area_a[:, np.newaxis], 1)\n",
        "\n",
        "def encode(matched, priors, variances):\n",
        "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
        "    we have matched (based on jaccard overlap) with the prior boxes.\n",
        "    Args:\n",
        "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
        "            Shape: [num_priors, 4].\n",
        "        priors: (tensor) Prior boxes in center-offset form\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
        "    \"\"\"\n",
        "\n",
        "    # dist b/t match center and prior's center\n",
        "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
        "    # encode variance\n",
        "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
        "    # match wh / prior wh\n",
        "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
        "    g_wh = torch.log(g_wh) / variances[1]\n",
        "    # return target for smooth_l1_loss\n",
        "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
        "\n",
        "def match(threshold, truths, priors, variances, labels, landms, loc_t, conf_t, landm_t, idx):\n",
        "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
        "    overlap, encode the bounding boxes, then return the matched indices\n",
        "    corresponding to both confidence and location preds.\n",
        "    Args:\n",
        "        threshold: (float) The overlap threshold used when mathing boxes.\n",
        "        truths: (tensor) Ground truth boxes, Shape: [num_obj, 4].\n",
        "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
        "        variances: (tensor) Variances corresponding to each prior coord,\n",
        "            Shape: [num_priors, 4].\n",
        "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
        "        landms: (tensor) Ground truth landms, Shape [num_obj, 10].\n",
        "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
        "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
        "        landm_t: (tensor) Tensor to be filled w/ endcoded landm targets.\n",
        "        idx: (int) current batch index\n",
        "    Return:\n",
        "        The matched indices corresponding to 1)location 2)confidence 3)landm preds.\n",
        "    \"\"\"\n",
        "    # jaccard index\n",
        "    overlaps = jaccard(\n",
        "        truths,\n",
        "        point_form(priors)\n",
        "    )\n",
        "    # (Bipartite Matching)\n",
        "    # [1,num_objects] best prior for each ground truth\n",
        "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
        "\n",
        "    # ignore hard gt\n",
        "    valid_gt_idx = best_prior_overlap[:, 0] >= 0.2\n",
        "    best_prior_idx_filter = best_prior_idx[valid_gt_idx, :]\n",
        "    if best_prior_idx_filter.shape[0] <= 0:\n",
        "        loc_t[idx] = 0\n",
        "        conf_t[idx] = 0\n",
        "        return\n",
        "\n",
        "    # [1,num_priors] best ground truth for each prior\n",
        "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
        "    best_truth_idx.squeeze_(0)\n",
        "    best_truth_overlap.squeeze_(0)\n",
        "    best_prior_idx.squeeze_(1)\n",
        "    best_prior_idx_filter.squeeze_(1)\n",
        "    best_prior_overlap.squeeze_(1)\n",
        "    best_truth_overlap.index_fill_(0, best_prior_idx_filter, 2)  # ensure best prior\n",
        "    # TODO refactor: index  best_prior_idx with long tensor\n",
        "    # ensure every gt matches with its prior of max overlap\n",
        "    for j in range(best_prior_idx.size(0)):     # 判别此anchor是预测哪一个boxes\n",
        "        best_truth_idx[best_prior_idx[j]] = j\n",
        "    matches = truths[best_truth_idx]            # Shape: [num_priors,4] 此处为每一个anchor对应的bbox取出来\n",
        "    conf = labels[best_truth_idx]               # Shape: [num_priors]      此处为每一个anchor对应的label取出来\n",
        "    conf[best_truth_overlap < threshold] = 0    # label as background   overlap<0.35的全部作为负样本\n",
        "    loc = encode(matches, priors, variances)\n",
        "\n",
        "    matches_landm = landms[best_truth_idx]\n",
        "    landm = encode_landm(matches_landm, priors, variances)\n",
        "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
        "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
        "    landm_t[idx] = landm\n",
        "\n",
        "def _distort(image):\n",
        "\n",
        "    def _convert(image, alpha=1, beta=0):\n",
        "        tmp = image.astype(float) * alpha + beta\n",
        "        tmp[tmp < 0] = 0\n",
        "        tmp[tmp > 255] = 255\n",
        "        image[:] = tmp\n",
        "\n",
        "    image = image.copy()\n",
        "\n",
        "    if random.randrange(2):\n",
        "\n",
        "        #brightness distortion\n",
        "        if random.randrange(2):\n",
        "            _convert(image, beta=random.uniform(-32, 32))\n",
        "\n",
        "        #contrast distortion\n",
        "        if random.randrange(2):\n",
        "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        #saturation distortion\n",
        "        if random.randrange(2):\n",
        "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
        "\n",
        "        #hue distortion\n",
        "        if random.randrange(2):\n",
        "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
        "            tmp %= 180\n",
        "            image[:, :, 0] = tmp\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "    else:\n",
        "\n",
        "        #brightness distortion\n",
        "        if random.randrange(2):\n",
        "            _convert(image, beta=random.uniform(-32, 32))\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "        #saturation distortion\n",
        "        if random.randrange(2):\n",
        "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
        "\n",
        "        #hue distortion\n",
        "        if random.randrange(2):\n",
        "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
        "            tmp %= 180\n",
        "            image[:, :, 0] = tmp\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
        "\n",
        "        #contrast distortion\n",
        "        if random.randrange(2):\n",
        "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def _expand(image, boxes, fill, p):\n",
        "    if random.randrange(2):\n",
        "        return image, boxes\n",
        "\n",
        "    height, width, depth = image.shape\n",
        "\n",
        "    scale = random.uniform(1, p)\n",
        "    w = int(scale * width)\n",
        "    h = int(scale * height)\n",
        "\n",
        "    left = random.randint(0, w - width)\n",
        "    top = random.randint(0, h - height)\n",
        "\n",
        "    boxes_t = boxes.copy()\n",
        "    boxes_t[:, :2] += (left, top)\n",
        "    boxes_t[:, 2:] += (left, top)\n",
        "    expand_image = np.empty(\n",
        "        (h, w, depth),\n",
        "        dtype=image.dtype)\n",
        "    expand_image[:, :] = fill\n",
        "    expand_image[top:top + height, left:left + width] = image\n",
        "    image = expand_image\n",
        "\n",
        "    return image, boxes_t\n",
        "\n",
        "\n",
        "def _mirror(image, boxes, landms):\n",
        "    _, width, _ = image.shape\n",
        "    if random.randrange(2):\n",
        "        image = image[:, ::-1]\n",
        "        boxes = boxes.copy()\n",
        "        boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
        "\n",
        "        # landm\n",
        "        landms = landms.copy()\n",
        "        landms = landms.reshape([-1, 5, 2])\n",
        "        landms[:, :, 0] = width - landms[:, :, 0]\n",
        "        tmp = landms[:, 1, :].copy()\n",
        "        landms[:, 1, :] = landms[:, 0, :]\n",
        "        landms[:, 0, :] = tmp\n",
        "        tmp1 = landms[:, 4, :].copy()\n",
        "        landms[:, 4, :] = landms[:, 3, :]\n",
        "        landms[:, 3, :] = tmp1\n",
        "        landms = landms.reshape([-1, 10])\n",
        "\n",
        "    return image, boxes, landms\n",
        "\n",
        "\n",
        "def _pad_to_square(image, rgb_mean, pad_image_flag):\n",
        "    if not pad_image_flag:\n",
        "        return image\n",
        "    height, width, _ = image.shape\n",
        "    long_side = max(width, height)\n",
        "    image_t = np.empty((long_side, long_side, 3), dtype=image.dtype)\n",
        "    image_t[:, :] = rgb_mean\n",
        "    image_t[0:0 + height, 0:0 + width] = image\n",
        "    return image_t"
      ],
      "metadata": {
        "id": "pytorgqh44nP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PriorBox(object):\n",
        "    def __init__(self, cfg, image_size=None, phase='train'):\n",
        "        super(PriorBox, self).__init__()\n",
        "        self.min_sizes = cfg['min_sizes']\n",
        "        self.steps = cfg['steps']\n",
        "        self.clip = cfg['clip']\n",
        "        self.image_size = image_size\n",
        "        self.feature_maps = [[ceil(self.image_size[0]/step), ceil(self.image_size[1]/step)] for step in self.steps]\n",
        "        self.name = \"s\"\n",
        "\n",
        "    def forward(self):\n",
        "        anchors = []\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            min_sizes = self.min_sizes[k]\n",
        "            for i, j in product(range(f[0]), range(f[1])):\n",
        "                for min_size in min_sizes:\n",
        "                    s_kx = min_size / self.image_size[1]\n",
        "                    s_ky = min_size / self.image_size[0]\n",
        "                    dense_cx = [x * self.steps[k] / self.image_size[1] for x in [j + 0.5]]\n",
        "                    dense_cy = [y * self.steps[k] / self.image_size[0] for y in [i + 0.5]]\n",
        "                    for cy, cx in product(dense_cy, dense_cx):\n",
        "                        anchors += [cx, cy, s_kx, s_ky]\n",
        "\n",
        "        # back to torch land\n",
        "        output = torch.Tensor(anchors).view(-1, 4)\n",
        "        if self.clip:\n",
        "            output.clamp_(max=1, min=0)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "9IP9DKQy4E2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSD Weighted Loss Function\n",
        "    Compute Targets:\n",
        "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
        "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
        "           (default threshold: 0.5).\n",
        "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
        "           truth boxes and their matched  'priorboxes'.\n",
        "        3) Hard negative mining to filter the excessive number of negative examples\n",
        "           that comes with using a large number of default bounding boxes.\n",
        "           (default negative:positive ratio 3:1)\n",
        "    Objective Loss:\n",
        "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
        "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
        "        weighted by α which is set to 1 by cross val.\n",
        "        Args:\n",
        "            c: class confidences,\n",
        "            l: predicted boxes,\n",
        "            g: ground truth boxes\n",
        "            N: number of matched default boxes\n",
        "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.threshold = overlap_thresh\n",
        "        self.background_label = bkg_label\n",
        "        self.encode_target = encode_target\n",
        "        self.use_prior_for_matching = prior_for_matching\n",
        "        self.do_neg_mining = neg_mining\n",
        "        self.negpos_ratio = neg_pos\n",
        "        self.neg_overlap = neg_overlap\n",
        "        self.variance = [0.1, 0.2]\n",
        "\n",
        "    def forward(self, predictions, priors, targets):\n",
        "        \"\"\"Multibox Loss\n",
        "        Args:\n",
        "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
        "            and prior boxes from SSD net.\n",
        "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
        "                loc shape: torch.size(batch_size,num_priors,4)\n",
        "                priors shape: torch.size(num_priors,4)\n",
        "\n",
        "            ground_truth (tensor): Ground truth boxes and labels for a batch,\n",
        "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
        "        \"\"\"\n",
        "\n",
        "        loc_data, conf_data, landm_data = predictions\n",
        "        priors = priors\n",
        "        num = loc_data.size(0)\n",
        "        num_priors = (priors.size(0))\n",
        "\n",
        "        # match priors (default boxes) and ground truth boxes\n",
        "        loc_t = torch.Tensor(num, num_priors, 4)\n",
        "        landm_t = torch.Tensor(num, num_priors, 10)\n",
        "        conf_t = torch.LongTensor(num, num_priors)\n",
        "        for idx in range(num):\n",
        "            truths = targets[idx][:, :4].data\n",
        "            labels = targets[idx][:, -1].data\n",
        "            landms = targets[idx][:, 4:14].data\n",
        "            defaults = priors.data\n",
        "            match(self.threshold, truths, defaults, self.variance, labels, landms, loc_t, conf_t, landm_t, idx)\n",
        "        if GPU:\n",
        "            loc_t = loc_t.cuda()\n",
        "            conf_t = conf_t.cuda()\n",
        "            landm_t = landm_t.cuda()\n",
        "\n",
        "        zeros = torch.tensor(0).cuda()\n",
        "        # landm Loss (Smooth L1)\n",
        "        # Shape: [batch,num_priors,10]\n",
        "        pos1 = conf_t > zeros\n",
        "        num_pos_landm = pos1.long().sum(1, keepdim=True)\n",
        "        N1 = max(num_pos_landm.data.sum().float(), 1)\n",
        "        pos_idx1 = pos1.unsqueeze(pos1.dim()).expand_as(landm_data)\n",
        "        landm_p = landm_data[pos_idx1].view(-1, 10)\n",
        "        landm_t = landm_t[pos_idx1].view(-1, 10)\n",
        "        loss_landm = F.smooth_l1_loss(landm_p, landm_t, reduction='sum')\n",
        "\n",
        "\n",
        "        pos = conf_t != zeros\n",
        "        conf_t[pos] = 1\n",
        "\n",
        "        # Localization Loss (Smooth L1)\n",
        "        # Shape: [batch,num_priors,4]\n",
        "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # Compute max conf across batch for hard negative mining\n",
        "        batch_conf = conf_data.view(-1, self.num_classes)\n",
        "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        loss_c[pos.view(-1, 1)] = 0 # filter out pos boxes for now\n",
        "        loss_c = loss_c.view(num, -1)\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "        num_pos = pos.long().sum(1, keepdim=True)\n",
        "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
        "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
        "\n",
        "        # Confidence Loss Including Positive and Negative Examples\n",
        "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
        "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n",
        "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
        "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
        "\n",
        "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
        "        N = max(num_pos.data.sum().float(), 1)\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "        loss_landm /= N1\n",
        "\n",
        "        return loss_l, loss_c, loss_landm\n"
      ],
      "metadata": {
        "id": "vPdgHuph31_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size):\n",
        "    \"\"\"Sets the learning rate\n",
        "    # Adapted from PyTorch Imagenet example:\n",
        "    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "    \"\"\"\n",
        "    warmup_epoch = -1\n",
        "    if epoch <= warmup_epoch:\n",
        "        lr = 1e-6 + (initial_lr-1e-6) * iteration / (epoch_size * warmup_epoch)\n",
        "    else:\n",
        "        lr = initial_lr * (gamma ** (step_index))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return lr"
      ],
      "metadata": {
        "id": "IXFP4aFp4VVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RetinaFace Model"
      ],
      "metadata": {
        "id": "PXl3ge3G3Xom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_bn(inp, oup, stride = 1, leaky = 0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_bn_no_relu(inp, oup, stride):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "    )\n",
        "\n",
        "def conv_bn1X1(inp, oup, stride, leaky=0):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, stride, padding=0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.LeakyReLU(negative_slope=leaky, inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_dw(inp, oup, stride, leaky=0.1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
        "        nn.BatchNorm2d(inp),\n",
        "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
        "\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.LeakyReLU(negative_slope= leaky,inplace=True),\n",
        "    )\n",
        "\n",
        "class SSH(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel):\n",
        "        super(SSH, self).__init__()\n",
        "        assert out_channel % 4 == 0\n",
        "        leaky = 0\n",
        "        if (out_channel <= 64):\n",
        "            leaky = 0.1\n",
        "        self.conv3X3 = conv_bn_no_relu(in_channel, out_channel//2, stride=1)\n",
        "\n",
        "        self.conv5X5_1 = conv_bn(in_channel, out_channel//4, stride=1, leaky = leaky)\n",
        "        self.conv5X5_2 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
        "\n",
        "        self.conv7X7_2 = conv_bn(out_channel//4, out_channel//4, stride=1, leaky = leaky)\n",
        "        self.conv7x7_3 = conv_bn_no_relu(out_channel//4, out_channel//4, stride=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        conv3X3 = self.conv3X3(input)\n",
        "\n",
        "        conv5X5_1 = self.conv5X5_1(input)\n",
        "        conv5X5 = self.conv5X5_2(conv5X5_1)\n",
        "\n",
        "        conv7X7_2 = self.conv7X7_2(conv5X5_1)\n",
        "        conv7X7 = self.conv7x7_3(conv7X7_2)\n",
        "\n",
        "        out = torch.cat([conv3X3, conv5X5, conv7X7], dim=1)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class FPN(nn.Module):\n",
        "    def __init__(self,in_channels_list,out_channels):\n",
        "        super(FPN,self).__init__()\n",
        "        leaky = 0\n",
        "        if (out_channels <= 64):\n",
        "            leaky = 0.1\n",
        "        self.output1 = conv_bn1X1(in_channels_list[0], out_channels, stride = 1, leaky = leaky)\n",
        "        self.output2 = conv_bn1X1(in_channels_list[1], out_channels, stride = 1, leaky = leaky)\n",
        "        self.output3 = conv_bn1X1(in_channels_list[2], out_channels, stride = 1, leaky = leaky)\n",
        "\n",
        "        self.merge1 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
        "        self.merge2 = conv_bn(out_channels, out_channels, leaky = leaky)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # names = list(input.keys())\n",
        "        input = list(input.values())\n",
        "\n",
        "        output1 = self.output1(input[0])\n",
        "        output2 = self.output2(input[1])\n",
        "        output3 = self.output3(input[2])\n",
        "\n",
        "        up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\")\n",
        "        output2 = output2 + up3\n",
        "        output2 = self.merge2(output2)\n",
        "\n",
        "        up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\")\n",
        "        output1 = output1 + up2\n",
        "        output1 = self.merge1(output1)\n",
        "\n",
        "        out = [output1, output2, output3]\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "class MobileNetV1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MobileNetV1, self).__init__()\n",
        "        self.stage1 = nn.Sequential(\n",
        "            conv_bn(3, 8, 2, leaky = 0.1),    # 3\n",
        "            conv_dw(8, 16, 1),   # 7\n",
        "            conv_dw(16, 32, 2),  # 11\n",
        "            conv_dw(32, 32, 1),  # 19\n",
        "            conv_dw(32, 64, 2),  # 27\n",
        "            conv_dw(64, 64, 1),  # 43\n",
        "        )\n",
        "        self.stage2 = nn.Sequential(\n",
        "            conv_dw(64, 128, 2),  # 43 + 16 = 59\n",
        "            conv_dw(128, 128, 1), # 59 + 32 = 91\n",
        "            conv_dw(128, 128, 1), # 91 + 32 = 123\n",
        "            conv_dw(128, 128, 1), # 123 + 32 = 155\n",
        "            conv_dw(128, 128, 1), # 155 + 32 = 187\n",
        "            conv_dw(128, 128, 1), # 187 + 32 = 219\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            conv_dw(128, 256, 2), # 219 +3 2 = 241\n",
        "            conv_dw(256, 256, 1), # 241 + 64 = 301\n",
        "        )\n",
        "        self.avg = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(256, 1000)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stage1(x)\n",
        "        x = self.stage2(x)\n",
        "        x = self.stage3(x)\n",
        "        x = self.avg(x)\n",
        "        # x = self.model(x)\n",
        "        x = x.view(-1, 256)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "Ycqo7Pxh3fxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassHead(nn.Module):\n",
        "    def __init__(self,inchannels=512,num_anchors=3):\n",
        "        super(ClassHead,self).__init__()\n",
        "        self.num_anchors = num_anchors\n",
        "        self.conv1x1 = nn.Conv2d(inchannels,self.num_anchors*2,kernel_size=(1,1),stride=1,padding=0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.conv1x1(x)\n",
        "        out = out.permute(0,2,3,1).contiguous()\n",
        "        \n",
        "        return out.view(out.shape[0], -1, 2)\n",
        "\n",
        "class BboxHead(nn.Module):\n",
        "    def __init__(self,inchannels=512,num_anchors=3):\n",
        "        super(BboxHead,self).__init__()\n",
        "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*4,kernel_size=(1,1),stride=1,padding=0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.conv1x1(x)\n",
        "        out = out.permute(0,2,3,1).contiguous()\n",
        "\n",
        "        return out.view(out.shape[0], -1, 4)\n",
        "\n",
        "class LandmarkHead(nn.Module):\n",
        "    def __init__(self,inchannels=512,num_anchors=3):\n",
        "        super(LandmarkHead,self).__init__()\n",
        "        self.conv1x1 = nn.Conv2d(inchannels,num_anchors*10,kernel_size=(1,1),stride=1,padding=0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.conv1x1(x)\n",
        "        out = out.permute(0,2,3,1).contiguous()\n",
        "\n",
        "        return out.view(out.shape[0], -1, 10)\n",
        "\n",
        "class RetinaFace(nn.Module):\n",
        "    def __init__(self, cfg = None, phase = 'train'):\n",
        "        \"\"\"\n",
        "        :param cfg:  Network related settings.\n",
        "        :param phase: train or test.\n",
        "        \"\"\"\n",
        "        super(RetinaFace,self).__init__()\n",
        "        self.phase = phase\n",
        "        backbone = None\n",
        "        # if cfg['name'] == 'mobilenet0.25':\n",
        "        #     backbone = MobileNetV1()\n",
        "        #     if cfg['pretrain']:\n",
        "        #         checkpoint = torch.load(\"./weights/mobilenetV1X0.25_pretrain.tar\", map_location=torch.device('cpu'))\n",
        "        #         from collections import OrderedDict\n",
        "        #         new_state_dict = OrderedDict()\n",
        "        #         for k, v in checkpoint['state_dict'].items():\n",
        "        #             name = k[7:]  # remove module.\n",
        "        #             new_state_dict[name] = v\n",
        "        #         # load params\n",
        "        #         backbone.load_state_dict(new_state_dict)\n",
        "        # elif cfg['name'] == 'Resnet50':\n",
        "        #     import torchvision.models as models\n",
        "        #     backbone = models.resnet50(pretrained=cfg['pretrain'])\n",
        "        backbone = MobileNetV1()\n",
        "\n",
        "        self.body = _utils.IntermediateLayerGetter(backbone, cfg['return_layers'])\n",
        "        in_channels_stage2 = cfg['in_channel']\n",
        "        in_channels_list = [\n",
        "            in_channels_stage2 * 2,\n",
        "            in_channels_stage2 * 4,\n",
        "            in_channels_stage2 * 8,\n",
        "        ]\n",
        "        out_channels = cfg['out_channel']\n",
        "        self.fpn = FPN(in_channels_list,out_channels)\n",
        "        self.ssh1 = SSH(out_channels, out_channels)\n",
        "        self.ssh2 = SSH(out_channels, out_channels)\n",
        "        self.ssh3 = SSH(out_channels, out_channels)\n",
        "\n",
        "        self.ClassHead = self._make_class_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
        "        self.BboxHead = self._make_bbox_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
        "        self.LandmarkHead = self._make_landmark_head(fpn_num=3, inchannels=cfg['out_channel'])\n",
        "\n",
        "    def _make_class_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
        "        classhead = nn.ModuleList()\n",
        "        for i in range(fpn_num):\n",
        "            classhead.append(ClassHead(inchannels,anchor_num))\n",
        "        return classhead\n",
        "    \n",
        "    def _make_bbox_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
        "        bboxhead = nn.ModuleList()\n",
        "        for i in range(fpn_num):\n",
        "            bboxhead.append(BboxHead(inchannels,anchor_num))\n",
        "        return bboxhead\n",
        "\n",
        "    def _make_landmark_head(self,fpn_num=3,inchannels=64,anchor_num=2):\n",
        "        landmarkhead = nn.ModuleList()\n",
        "        for i in range(fpn_num):\n",
        "            landmarkhead.append(LandmarkHead(inchannels,anchor_num))\n",
        "        return landmarkhead\n",
        "\n",
        "    def forward(self,inputs):\n",
        "        out = self.body(inputs)\n",
        "\n",
        "        # FPN\n",
        "        fpn = self.fpn(out)\n",
        "\n",
        "        # SSH\n",
        "        feature1 = self.ssh1(fpn[0])\n",
        "        feature2 = self.ssh2(fpn[1])\n",
        "        feature3 = self.ssh3(fpn[2])\n",
        "        features = [feature1, feature2, feature3]\n",
        "\n",
        "        bbox_regressions = torch.cat([self.BboxHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
        "        classifications = torch.cat([self.ClassHead[i](feature) for i, feature in enumerate(features)],dim=1)\n",
        "        ldm_regressions = torch.cat([self.LandmarkHead[i](feature) for i, feature in enumerate(features)], dim=1)\n",
        "\n",
        "        if self.phase == 'train':\n",
        "            output = (bbox_regressions, classifications, ldm_regressions)\n",
        "        else:\n",
        "            output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Eh7z0Eh23aOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuSjneyz3pSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataloader"
      ],
      "metadata": {
        "id": "IccyoUD-3sKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WiderFaceDetection(data.Dataset):\n",
        "    def __init__(self, txt_path, preproc=None):\n",
        "        self.preproc = preproc\n",
        "        self.imgs_path = []\n",
        "        self.words = []\n",
        "        f = open(txt_path,'r')\n",
        "        lines = f.readlines()\n",
        "        isFirst = True\n",
        "        labels = []\n",
        "        for line in lines:\n",
        "            line = line.rstrip()\n",
        "            if line.startswith('#'):\n",
        "                if isFirst is True:\n",
        "                    isFirst = False\n",
        "                else:\n",
        "                    labels_copy = labels.copy()\n",
        "                    self.words.append(labels_copy)\n",
        "                    labels.clear()\n",
        "                path = line[2:]\n",
        "                path = txt_path.replace('label.txt','images/') + path\n",
        "                self.imgs_path.append(path)\n",
        "            else:\n",
        "                line = line.split(' ')\n",
        "                label = [float(x) for x in line]\n",
        "                labels.append(label)\n",
        "\n",
        "        self.words.append(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs_path)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = cv2.imread(self.imgs_path[index])\n",
        "        height, width, _ = img.shape\n",
        "\n",
        "        labels = self.words[index]\n",
        "        annotations = np.zeros((0, 15))\n",
        "        if len(labels) == 0:\n",
        "            return annotations\n",
        "        for idx, label in enumerate(labels):\n",
        "            annotation = np.zeros((1, 15))\n",
        "            # bbox\n",
        "            annotation[0, 0] = label[0]  # x1\n",
        "            annotation[0, 1] = label[1]  # y1\n",
        "            annotation[0, 2] = label[0] + label[2]  # x2\n",
        "            annotation[0, 3] = label[1] + label[3]  # y2\n",
        "\n",
        "            # landmarks\n",
        "            annotation[0, 4] = label[4]    # l0_x\n",
        "            annotation[0, 5] = label[5]    # l0_y\n",
        "            annotation[0, 6] = label[7]    # l1_x\n",
        "            annotation[0, 7] = label[8]    # l1_y\n",
        "            annotation[0, 8] = label[10]   # l2_x\n",
        "            annotation[0, 9] = label[11]   # l2_y\n",
        "            annotation[0, 10] = label[13]  # l3_x\n",
        "            annotation[0, 11] = label[14]  # l3_y\n",
        "            annotation[0, 12] = label[16]  # l4_x\n",
        "            annotation[0, 13] = label[17]  # l4_y\n",
        "            if (annotation[0, 4]<0):\n",
        "                annotation[0, 14] = -1\n",
        "            else:\n",
        "                annotation[0, 14] = 1\n",
        "\n",
        "            annotations = np.append(annotations, annotation, axis=0)\n",
        "        target = np.array(annotations)\n",
        "        if self.preproc is not None:\n",
        "            img, target = self.preproc(img, target)\n",
        "\n",
        "        return torch.from_numpy(img), target\n",
        "\n"
      ],
      "metadata": {
        "id": "yPhGTdUu3tSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detection_collate(batch):\n",
        "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
        "    number of associated object annotations (bounding boxes).\n",
        "\n",
        "    Arguments:\n",
        "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
        "\n",
        "    Return:\n",
        "        A tuple containing:\n",
        "            1) (tensor) batch of images stacked on their 0 dim\n",
        "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
        "    \"\"\"\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        for _, tup in enumerate(sample):\n",
        "            if torch.is_tensor(tup):\n",
        "                imgs.append(tup)\n",
        "            elif isinstance(tup, type(np.empty(0))):\n",
        "                annos = torch.from_numpy(tup).float()\n",
        "                targets.append(annos)\n",
        "\n",
        "    return (torch.stack(imgs, 0), targets)\n"
      ],
      "metadata": {
        "id": "WmrAftGA5EPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _resize_subtract_mean(image, insize, rgb_mean):\n",
        "    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n",
        "    interp_method = interp_methods[random.randrange(5)]\n",
        "    image = cv2.resize(image, (insize, insize), interpolation=interp_method)\n",
        "    image = image.astype(np.float32)\n",
        "    image -= rgb_mean\n",
        "    return image.transpose(2, 0, 1)\n",
        "\n",
        "class preproc(object):\n",
        "\n",
        "    def __init__(self, img_dim, rgb_means):\n",
        "        self.img_dim = img_dim\n",
        "        self.rgb_means = rgb_means\n",
        "\n",
        "    def __call__(self, image, targets):\n",
        "        assert targets.shape[0] > 0, \"this image does not have gt\"\n",
        "\n",
        "        boxes = targets[:, :4].copy()\n",
        "        labels = targets[:, -1].copy()\n",
        "        landm = targets[:, 4:-1].copy()\n",
        "\n",
        "        image_t, boxes_t, labels_t, landm_t, pad_image_flag = _crop(image, boxes, labels, landm, self.img_dim)\n",
        "        image_t = _distort(image_t)\n",
        "        image_t = _pad_to_square(image_t,self.rgb_means, pad_image_flag)\n",
        "        image_t, boxes_t, landm_t = _mirror(image_t, boxes_t, landm_t)\n",
        "        height, width, _ = image_t.shape\n",
        "        image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means)\n",
        "        boxes_t[:, 0::2] /= width\n",
        "        boxes_t[:, 1::2] /= height\n",
        "\n",
        "        landm_t[:, 0::2] /= width\n",
        "        landm_t[:, 1::2] /= height\n",
        "\n",
        "        labels_t = np.expand_dims(labels_t, 1)\n",
        "        targets_t = np.hstack((boxes_t, landm_t, labels_t))\n",
        "\n",
        "        return image_t, targets_t"
      ],
      "metadata": {
        "id": "flGIN9uy5Rb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Loop"
      ],
      "metadata": {
        "id": "FiBVZ6gb32ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 4\n",
        "momentum = 0.9\n",
        "weight_decay = 5e-4\n",
        "initial_lr = 1e-3\n",
        "gamma = 0.1\n",
        "training_dataset = \"/content/widerface/train/label.txt\"\n",
        "save_folder = \"./weights/\"\n",
        "\n",
        "if not os.path.exists(save_folder):\n",
        "    os.mkdir(save_folder)\n",
        "\n",
        "rgb_mean = (104, 117, 123) # bgr order\n",
        "num_classes = 2\n",
        "img_dim = cfg['image_size']\n",
        "num_gpu = cfg['ngpu']\n",
        "batch_size = cfg['batch_size']\n",
        "max_epoch = cfg['epoch']\n",
        "gpu_train = cfg['gpu_train']\n",
        "\n",
        "\n",
        "\n",
        "net = RetinaFace(cfg=cfg)\n",
        "print(\"Printing net...\")\n",
        "print(net)\n",
        "\n",
        "# if resume_net is not None:\n",
        "#     print('Loading resume network...')\n",
        "#     state_dict = torch.load(resume_net)\n",
        "#     # create new OrderedDict that does not contain `module.`\n",
        "#     from collections import OrderedDict\n",
        "#     new_state_dict = OrderedDict()\n",
        "#     for k, v in state_dict.items():\n",
        "#         head = k[:7]\n",
        "#         if head == 'module.':\n",
        "#             name = k[7:] # remove `module.`\n",
        "#         else:\n",
        "#             name = k\n",
        "#         new_state_dict[name] = v\n",
        "#     net.load_state_dict(new_state_dict)\n",
        "resume_epoch = 0\n",
        "\n",
        "if num_gpu > 1 and gpu_train:\n",
        "    net = torch.nn.DataParallel(net).cuda()\n",
        "else:\n",
        "    net = net.cuda()\n",
        "\n",
        "cudnn.benchmark = True\n",
        "\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=initial_lr, momentum=momentum, weight_decay=weight_decay)\n",
        "criterion = MultiBoxLoss(num_classes, 0.35, True, 0, True, 7, 0.35, False)\n",
        "\n",
        "priorbox = PriorBox(cfg, image_size=(img_dim, img_dim))\n",
        "with torch.no_grad():\n",
        "    priors = priorbox.forward()\n",
        "    priors = priors.cuda()\n",
        "\n",
        "net.train()\n",
        "epoch = 0 + resume_epoch\n",
        "print('Loading Dataset...')\n",
        "\n",
        "dataset = WiderFaceDetection( training_dataset,preproc(img_dim, rgb_mean))\n",
        "\n",
        "epoch_size = math.ceil(len(dataset) / batch_size)\n",
        "max_iter = max_epoch * epoch_size\n",
        "\n",
        "stepvalues = (cfg['decay1'] * epoch_size, cfg['decay2'] * epoch_size)\n",
        "step_index = 0\n",
        "\n",
        "if resume_epoch > 0:\n",
        "    start_iter = resume_epoch * epoch_size\n",
        "else:\n",
        "    start_iter = 0\n",
        "\n",
        "for iteration in range(start_iter, max_iter):\n",
        "    if iteration % epoch_size == 0:\n",
        "        # create batch iterator\n",
        "        batch_iterator = iter(data.DataLoader(dataset, batch_size, shuffle=True, num_workers=num_workers, collate_fn=detection_collate))\n",
        "        if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 == 0 and epoch > cfg['decay1']):\n",
        "            torch.save(net.state_dict(), save_folder + cfg['name']+ '_epoch_' + str(epoch) + '.pth')\n",
        "        epoch += 1\n",
        "\n",
        "    load_t0 = time.time()\n",
        "    if iteration in stepvalues:\n",
        "        step_index += 1\n",
        "    lr = adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size)\n",
        "\n",
        "    # load train data\n",
        "    images, targets = next(batch_iterator)\n",
        "    images = images.cuda()\n",
        "    targets = [anno.cuda() for anno in targets]\n",
        "\n",
        "    # forward\n",
        "    out = net(images)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad()\n",
        "    loss_l, loss_c, loss_landm = criterion(out, priors, targets)\n",
        "    loss = cfg['loc_weight'] * loss_l + loss_c + loss_landm\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    load_t1 = time.time()\n",
        "    batch_time = load_t1 - load_t0\n",
        "    eta = int(batch_time * (max_iter - iteration))\n",
        "    print('Epoch:{}/{} || Epochiter: {}/{} || Iter: {}/{} || Loc: {:.4f} Cla: {:.4f} Landm: {:.4f} || LR: {:.8f} || Batchtime: {:.4f} s || ETA: {}'\n",
        "            .format(epoch, max_epoch, (iteration % epoch_size) + 1,\n",
        "            epoch_size, iteration + 1, max_iter, loss_l.item(), loss_c.item(), loss_landm.item(), lr, batch_time, str(datetime.timedelta(seconds=eta))))\n",
        "\n",
        "torch.save(net.state_dict(), save_folder + cfg['name'] + '_Final.pth')\n",
        "# torch.save(net.state_dict(), save_folder + 'Final_Retinaface.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWNg7rtf33qq",
        "outputId": "4b46d76b-0764-4509-b334-5b74e20b2b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing net...\n",
            "RetinaFace(\n",
            "  (body): IntermediateLayerGetter(\n",
            "    (stage1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
            "        (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
            "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (stage2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
            "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (stage3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "        (3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fpn): FPN(\n",
            "    (output1): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (output2): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (output3): Sequential(\n",
            "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (merge1): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (merge2): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (ssh1): SSH(\n",
            "    (conv3X3): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv5X5_1): Sequential(\n",
            "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (conv5X5_2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv7X7_2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (conv7x7_3): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (ssh2): SSH(\n",
            "    (conv3X3): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv5X5_1): Sequential(\n",
            "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (conv5X5_2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv7X7_2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (conv7x7_3): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (ssh3): SSH(\n",
            "    (conv3X3): Sequential(\n",
            "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv5X5_1): Sequential(\n",
            "      (0): Conv2d(64, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (conv5X5_2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (conv7X7_2): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
            "    )\n",
            "    (conv7x7_3): Sequential(\n",
            "      (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (ClassHead): ModuleList(\n",
            "    (0): ClassHead(\n",
            "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): ClassHead(\n",
            "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): ClassHead(\n",
            "      (conv1x1): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (BboxHead): ModuleList(\n",
            "    (0): BboxHead(\n",
            "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): BboxHead(\n",
            "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): BboxHead(\n",
            "      (conv1x1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            "  (LandmarkHead): ModuleList(\n",
            "    (0): LandmarkHead(\n",
            "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (1): LandmarkHead(\n",
            "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "    (2): LandmarkHead(\n",
            "      (conv1x1): Conv2d(64, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "Loading Dataset...\n",
            "Epoch:1/250 || Epochiter: 1/403 || Iter: 1/100750 || Loc: 4.2248 Cla: 9.8534 Landm: 20.6479 || LR: 0.00100000 || Batchtime: 18.1172 s || ETA: 21 days, 3:01:43\n",
            "Epoch:1/250 || Epochiter: 2/403 || Iter: 2/100750 || Loc: 4.3207 Cla: 9.7387 Landm: 20.0228 || LR: 0.00100000 || Batchtime: 0.3117 s || ETA: 8:43:21\n",
            "Epoch:1/250 || Epochiter: 3/403 || Iter: 3/100750 || Loc: 4.5398 Cla: 8.7965 Landm: 20.7392 || LR: 0.00100000 || Batchtime: 0.3841 s || ETA: 10:45:01\n",
            "Epoch:1/250 || Epochiter: 4/403 || Iter: 4/100750 || Loc: 4.2445 Cla: 8.4104 Landm: 20.6394 || LR: 0.00100000 || Batchtime: 0.4530 s || ETA: 12:40:41\n",
            "Epoch:1/250 || Epochiter: 5/403 || Iter: 5/100750 || Loc: 4.4174 Cla: 8.4876 Landm: 19.0601 || LR: 0.00100000 || Batchtime: 0.5806 s || ETA: 16:14:58\n",
            "Epoch:1/250 || Epochiter: 6/403 || Iter: 6/100750 || Loc: 4.6363 Cla: 7.9088 Landm: 20.4848 || LR: 0.00100000 || Batchtime: 0.6507 s || ETA: 18:12:38\n",
            "Epoch:1/250 || Epochiter: 7/403 || Iter: 7/100750 || Loc: 4.3274 Cla: 7.3354 Landm: 20.7203 || LR: 0.00100000 || Batchtime: 0.3229 s || ETA: 9:02:09\n",
            "Epoch:1/250 || Epochiter: 8/403 || Iter: 8/100750 || Loc: 4.1291 Cla: 7.1597 Landm: 19.6053 || LR: 0.00100000 || Batchtime: 0.4896 s || ETA: 13:42:04\n",
            "Epoch:1/250 || Epochiter: 9/403 || Iter: 9/100750 || Loc: 4.3156 Cla: 6.4183 Landm: 20.7598 || LR: 0.00100000 || Batchtime: 0.5234 s || ETA: 14:38:49\n",
            "Epoch:1/250 || Epochiter: 10/403 || Iter: 10/100750 || Loc: 4.2596 Cla: 6.2865 Landm: 20.2476 || LR: 0.00100000 || Batchtime: 1.0507 s || ETA: 1 day, 5:24:07\n",
            "Epoch:1/250 || Epochiter: 11/403 || Iter: 11/100750 || Loc: 4.1831 Cla: 5.9552 Landm: 20.5824 || LR: 0.00100000 || Batchtime: 1.3784 s || ETA: 1 day, 14:34:24\n",
            "Epoch:1/250 || Epochiter: 12/403 || Iter: 12/100750 || Loc: 4.3616 Cla: 5.9334 Landm: 20.6087 || LR: 0.00100000 || Batchtime: 0.6670 s || ETA: 18:39:54\n",
            "Epoch:1/250 || Epochiter: 13/403 || Iter: 13/100750 || Loc: 4.4310 Cla: 5.8285 Landm: 21.1302 || LR: 0.00100000 || Batchtime: 0.6732 s || ETA: 18:50:19\n",
            "Epoch:1/250 || Epochiter: 14/403 || Iter: 14/100750 || Loc: 4.1986 Cla: 5.7182 Landm: 19.8256 || LR: 0.00100000 || Batchtime: 1.7562 s || ETA: 2 days, 1:08:35\n",
            "Epoch:1/250 || Epochiter: 15/403 || Iter: 15/100750 || Loc: 4.3611 Cla: 5.3973 Landm: 19.8905 || LR: 0.00100000 || Batchtime: 1.8134 s || ETA: 2 days, 2:44:37\n",
            "Epoch:1/250 || Epochiter: 16/403 || Iter: 16/100750 || Loc: 4.1861 Cla: 5.1722 Landm: 18.8034 || LR: 0.00100000 || Batchtime: 0.6010 s || ETA: 16:49:05\n",
            "Epoch:1/250 || Epochiter: 17/403 || Iter: 17/100750 || Loc: 4.2949 Cla: 4.9577 Landm: 19.2235 || LR: 0.00100000 || Batchtime: 0.5372 s || ETA: 15:01:50\n",
            "Epoch:1/250 || Epochiter: 18/403 || Iter: 18/100750 || Loc: 4.1477 Cla: 4.8438 Landm: 19.0472 || LR: 0.00100000 || Batchtime: 1.9660 s || ETA: 2 days, 7:00:38\n",
            "Epoch:1/250 || Epochiter: 19/403 || Iter: 19/100750 || Loc: 4.2567 Cla: 4.7387 Landm: 19.6381 || LR: 0.00100000 || Batchtime: 1.7689 s || ETA: 2 days, 1:29:43\n",
            "Epoch:1/250 || Epochiter: 20/403 || Iter: 20/100750 || Loc: 4.2943 Cla: 4.5341 Landm: 19.3910 || LR: 0.00100000 || Batchtime: 0.6100 s || ETA: 17:04:02\n",
            "Epoch:1/250 || Epochiter: 21/403 || Iter: 21/100750 || Loc: 4.0054 Cla: 4.4889 Landm: 18.9928 || LR: 0.00100000 || Batchtime: 0.6157 s || ETA: 17:13:38\n",
            "Epoch:1/250 || Epochiter: 22/403 || Iter: 22/100750 || Loc: 4.0486 Cla: 4.3942 Landm: 19.2321 || LR: 0.00100000 || Batchtime: 1.5047 s || ETA: 1 day, 18:06:07\n",
            "Epoch:1/250 || Epochiter: 23/403 || Iter: 23/100750 || Loc: 4.1586 Cla: 4.2495 Landm: 18.6889 || LR: 0.00100000 || Batchtime: 1.4771 s || ETA: 1 day, 17:19:46\n",
            "Epoch:1/250 || Epochiter: 24/403 || Iter: 24/100750 || Loc: 4.2524 Cla: 4.2089 Landm: 19.6036 || LR: 0.00100000 || Batchtime: 0.7202 s || ETA: 20:09:04\n",
            "Epoch:1/250 || Epochiter: 25/403 || Iter: 25/100750 || Loc: 3.9691 Cla: 4.2232 Landm: 19.2208 || LR: 0.00100000 || Batchtime: 0.7000 s || ETA: 19:35:04\n",
            "Epoch:1/250 || Epochiter: 26/403 || Iter: 26/100750 || Loc: 4.2012 Cla: 4.1327 Landm: 20.1450 || LR: 0.00100000 || Batchtime: 1.7941 s || ETA: 2 days, 2:11:50\n",
            "Epoch:1/250 || Epochiter: 27/403 || Iter: 27/100750 || Loc: 4.2810 Cla: 4.1557 Landm: 19.8489 || LR: 0.00100000 || Batchtime: 0.6950 s || ETA: 19:26:39\n",
            "Epoch:1/250 || Epochiter: 28/403 || Iter: 28/100750 || Loc: 4.3800 Cla: 4.1143 Landm: 20.2634 || LR: 0.00100000 || Batchtime: 1.1493 s || ETA: 1 day, 8:09:21\n",
            "Epoch:1/250 || Epochiter: 29/403 || Iter: 29/100750 || Loc: 4.3521 Cla: 4.0428 Landm: 18.4167 || LR: 0.00100000 || Batchtime: 0.5935 s || ETA: 16:36:17\n",
            "Epoch:1/250 || Epochiter: 30/403 || Iter: 30/100750 || Loc: 4.2012 Cla: 4.0003 Landm: 18.4246 || LR: 0.00100000 || Batchtime: 2.6693 s || ETA: 3 days, 2:40:53\n",
            "Epoch:1/250 || Epochiter: 31/403 || Iter: 31/100750 || Loc: 4.1702 Cla: 3.9211 Landm: 18.4370 || LR: 0.00100000 || Batchtime: 0.8104 s || ETA: 22:40:20\n",
            "Epoch:1/250 || Epochiter: 32/403 || Iter: 32/100750 || Loc: 4.3277 Cla: 3.9487 Landm: 19.2563 || LR: 0.00100000 || Batchtime: 0.5603 s || ETA: 15:40:32\n",
            "Epoch:1/250 || Epochiter: 33/403 || Iter: 33/100750 || Loc: 4.3038 Cla: 3.8492 Landm: 18.3207 || LR: 0.00100000 || Batchtime: 0.6072 s || ETA: 16:59:16\n",
            "Epoch:1/250 || Epochiter: 34/403 || Iter: 34/100750 || Loc: 4.1524 Cla: 3.8731 Landm: 18.3770 || LR: 0.00100000 || Batchtime: 1.9662 s || ETA: 2 days, 7:00:29\n",
            "Epoch:1/250 || Epochiter: 35/403 || Iter: 35/100750 || Loc: 4.0191 Cla: 3.7996 Landm: 18.7644 || LR: 0.00100000 || Batchtime: 0.6507 s || ETA: 18:12:18\n",
            "Epoch:1/250 || Epochiter: 36/403 || Iter: 36/100750 || Loc: 4.2886 Cla: 3.7089 Landm: 18.8176 || LR: 0.00100000 || Batchtime: 0.8565 s || ETA: 23:57:41\n",
            "Epoch:1/250 || Epochiter: 37/403 || Iter: 37/100750 || Loc: 4.4200 Cla: 3.7604 Landm: 19.1375 || LR: 0.00100000 || Batchtime: 1.1579 s || ETA: 1 day, 8:23:40\n",
            "Epoch:1/250 || Epochiter: 38/403 || Iter: 38/100750 || Loc: 4.1140 Cla: 3.7504 Landm: 19.0014 || LR: 0.00100000 || Batchtime: 1.8431 s || ETA: 2 days, 3:33:44\n",
            "Epoch:1/250 || Epochiter: 39/403 || Iter: 39/100750 || Loc: 4.2224 Cla: 3.7059 Landm: 18.7581 || LR: 0.00100000 || Batchtime: 0.6031 s || ETA: 16:52:24\n",
            "Epoch:1/250 || Epochiter: 40/403 || Iter: 40/100750 || Loc: 4.1427 Cla: 3.6634 Landm: 17.8833 || LR: 0.00100000 || Batchtime: 1.1634 s || ETA: 1 day, 8:32:42\n",
            "Epoch:1/250 || Epochiter: 41/403 || Iter: 41/100750 || Loc: 4.3211 Cla: 3.6659 Landm: 19.0369 || LR: 0.00100000 || Batchtime: 1.4434 s || ETA: 1 day, 16:22:46\n",
            "Epoch:1/250 || Epochiter: 42/403 || Iter: 42/100750 || Loc: 4.1141 Cla: 3.6920 Landm: 18.6367 || LR: 0.00100000 || Batchtime: 1.9049 s || ETA: 2 days, 5:17:23\n",
            "Epoch:1/250 || Epochiter: 43/403 || Iter: 43/100750 || Loc: 4.3428 Cla: 3.7261 Landm: 18.2805 || LR: 0.00100000 || Batchtime: 0.6189 s || ETA: 17:18:50\n",
            "Epoch:1/250 || Epochiter: 44/403 || Iter: 44/100750 || Loc: 4.1866 Cla: 3.6290 Landm: 18.3116 || LR: 0.00100000 || Batchtime: 0.6629 s || ETA: 18:32:36\n",
            "Epoch:1/250 || Epochiter: 45/403 || Iter: 45/100750 || Loc: 4.1811 Cla: 3.5989 Landm: 17.3489 || LR: 0.00100000 || Batchtime: 1.8681 s || ETA: 2 days, 4:15:28\n",
            "Epoch:1/250 || Epochiter: 46/403 || Iter: 46/100750 || Loc: 4.0062 Cla: 3.6193 Landm: 18.4828 || LR: 0.00100000 || Batchtime: 1.5211 s || ETA: 1 day, 18:32:58\n",
            "Epoch:1/250 || Epochiter: 47/403 || Iter: 47/100750 || Loc: 4.0227 Cla: 3.5893 Landm: 17.7953 || LR: 0.00100000 || Batchtime: 0.5772 s || ETA: 16:08:47\n",
            "Epoch:1/250 || Epochiter: 48/403 || Iter: 48/100750 || Loc: 4.1090 Cla: 3.6040 Landm: 19.1527 || LR: 0.00100000 || Batchtime: 0.5489 s || ETA: 15:21:12\n",
            "Epoch:1/250 || Epochiter: 49/403 || Iter: 49/100750 || Loc: 4.1519 Cla: 3.5897 Landm: 18.6230 || LR: 0.00100000 || Batchtime: 2.2382 s || ETA: 2 days, 14:36:27\n",
            "Epoch:1/250 || Epochiter: 50/403 || Iter: 50/100750 || Loc: 4.2594 Cla: 3.5919 Landm: 18.5741 || LR: 0.00100000 || Batchtime: 1.2152 s || ETA: 1 day, 9:59:29\n",
            "Epoch:1/250 || Epochiter: 51/403 || Iter: 51/100750 || Loc: 4.2800 Cla: 3.6105 Landm: 19.5558 || LR: 0.00100000 || Batchtime: 0.5852 s || ETA: 16:22:08\n",
            "Epoch:1/250 || Epochiter: 52/403 || Iter: 52/100750 || Loc: 3.8879 Cla: 3.5728 Landm: 16.9060 || LR: 0.00100000 || Batchtime: 0.5376 s || ETA: 15:02:12\n",
            "Epoch:1/250 || Epochiter: 53/403 || Iter: 53/100750 || Loc: 4.2215 Cla: 3.5539 Landm: 18.1865 || LR: 0.00100000 || Batchtime: 4.2198 s || ETA: 4 days, 22:02:03\n",
            "Epoch:1/250 || Epochiter: 54/403 || Iter: 54/100750 || Loc: 4.2407 Cla: 3.6465 Landm: 17.9243 || LR: 0.00100000 || Batchtime: 0.5811 s || ETA: 16:15:16\n",
            "Epoch:1/250 || Epochiter: 55/403 || Iter: 55/100750 || Loc: 3.9869 Cla: 3.4909 Landm: 18.4438 || LR: 0.00100000 || Batchtime: 0.5978 s || ETA: 16:43:11\n",
            "Epoch:1/250 || Epochiter: 56/403 || Iter: 56/100750 || Loc: 4.0029 Cla: 3.5617 Landm: 18.1596 || LR: 0.00100000 || Batchtime: 0.5158 s || ETA: 14:25:39\n",
            "Epoch:1/250 || Epochiter: 57/403 || Iter: 57/100750 || Loc: 4.0695 Cla: 3.5303 Landm: 18.1812 || LR: 0.00100000 || Batchtime: 2.5953 s || ETA: 3 days, 0:35:29\n",
            "Epoch:1/250 || Epochiter: 58/403 || Iter: 58/100750 || Loc: 3.9869 Cla: 3.5174 Landm: 18.0423 || LR: 0.00100000 || Batchtime: 0.5610 s || ETA: 15:41:27\n",
            "Epoch:1/250 || Epochiter: 59/403 || Iter: 59/100750 || Loc: 4.3037 Cla: 3.4342 Landm: 17.0275 || LR: 0.00100000 || Batchtime: 0.6412 s || ETA: 17:55:59\n",
            "Epoch:1/250 || Epochiter: 60/403 || Iter: 60/100750 || Loc: 4.0227 Cla: 3.4679 Landm: 18.0691 || LR: 0.00100000 || Batchtime: 0.7893 s || ETA: 22:04:32\n",
            "Epoch:1/250 || Epochiter: 61/403 || Iter: 61/100750 || Loc: 4.1678 Cla: 3.5554 Landm: 18.3070 || LR: 0.00100000 || Batchtime: 2.1942 s || ETA: 2 days, 13:22:10\n",
            "Epoch:1/250 || Epochiter: 62/403 || Iter: 62/100750 || Loc: 4.3059 Cla: 3.5001 Landm: 17.7127 || LR: 0.00100000 || Batchtime: 0.6429 s || ETA: 17:58:55\n",
            "Epoch:1/250 || Epochiter: 63/403 || Iter: 63/100750 || Loc: 3.8688 Cla: 3.5376 Landm: 17.1884 || LR: 0.00100000 || Batchtime: 0.5551 s || ETA: 15:31:30\n",
            "Epoch:1/250 || Epochiter: 64/403 || Iter: 64/100750 || Loc: 3.9954 Cla: 3.4445 Landm: 16.8882 || LR: 0.00100000 || Batchtime: 0.5365 s || ETA: 15:00:19\n",
            "Epoch:1/250 || Epochiter: 65/403 || Iter: 65/100750 || Loc: 4.1554 Cla: 3.5030 Landm: 17.6538 || LR: 0.00100000 || Batchtime: 2.3892 s || ETA: 2 days, 18:49:16\n",
            "Epoch:1/250 || Epochiter: 66/403 || Iter: 66/100750 || Loc: 4.0700 Cla: 3.5277 Landm: 17.2677 || LR: 0.00100000 || Batchtime: 0.5583 s || ETA: 15:36:51\n",
            "Epoch:1/250 || Epochiter: 67/403 || Iter: 67/100750 || Loc: 3.9319 Cla: 3.4391 Landm: 17.5953 || LR: 0.00100000 || Batchtime: 0.6171 s || ETA: 17:15:27\n",
            "Epoch:1/250 || Epochiter: 68/403 || Iter: 68/100750 || Loc: 4.1461 Cla: 3.5370 Landm: 17.9956 || LR: 0.00100000 || Batchtime: 0.6969 s || ETA: 19:29:27\n",
            "Epoch:1/250 || Epochiter: 69/403 || Iter: 69/100750 || Loc: 4.2303 Cla: 3.5496 Landm: 16.6746 || LR: 0.00100000 || Batchtime: 3.0990 s || ETA: 3 days, 14:40:08\n",
            "Epoch:1/250 || Epochiter: 70/403 || Iter: 70/100750 || Loc: 4.0663 Cla: 3.4988 Landm: 16.9414 || LR: 0.00100000 || Batchtime: 0.4408 s || ETA: 12:19:35\n",
            "Epoch:1/250 || Epochiter: 71/403 || Iter: 71/100750 || Loc: 4.0354 Cla: 3.4490 Landm: 17.7523 || LR: 0.00100000 || Batchtime: 0.7166 s || ETA: 20:02:27\n",
            "Epoch:1/250 || Epochiter: 72/403 || Iter: 72/100750 || Loc: 4.2069 Cla: 3.5234 Landm: 17.1657 || LR: 0.00100000 || Batchtime: 0.6306 s || ETA: 17:38:11\n",
            "Epoch:1/250 || Epochiter: 73/403 || Iter: 73/100750 || Loc: 3.6540 Cla: 3.5220 Landm: 17.5221 || LR: 0.00100000 || Batchtime: 2.6939 s || ETA: 3 days, 3:20:21\n",
            "Epoch:1/250 || Epochiter: 74/403 || Iter: 74/100750 || Loc: 4.0306 Cla: 3.5292 Landm: 17.0265 || LR: 0.00100000 || Batchtime: 0.6597 s || ETA: 18:26:54\n",
            "Epoch:1/250 || Epochiter: 75/403 || Iter: 75/100750 || Loc: 4.2129 Cla: 3.5513 Landm: 17.9162 || LR: 0.00100000 || Batchtime: 0.5949 s || ETA: 16:38:08\n",
            "Epoch:1/250 || Epochiter: 76/403 || Iter: 76/100750 || Loc: 4.1185 Cla: 3.4658 Landm: 17.3304 || LR: 0.00100000 || Batchtime: 0.6477 s || ETA: 18:06:42\n",
            "Epoch:1/250 || Epochiter: 77/403 || Iter: 77/100750 || Loc: 3.9461 Cla: 3.4029 Landm: 17.0454 || LR: 0.00100000 || Batchtime: 2.4699 s || ETA: 2 days, 21:04:11\n",
            "Epoch:1/250 || Epochiter: 78/403 || Iter: 78/100750 || Loc: 3.8782 Cla: 3.4456 Landm: 17.5960 || LR: 0.00100000 || Batchtime: 0.5154 s || ETA: 14:24:41\n",
            "Epoch:1/250 || Epochiter: 79/403 || Iter: 79/100750 || Loc: 3.9017 Cla: 3.5037 Landm: 15.5654 || LR: 0.00100000 || Batchtime: 0.6131 s || ETA: 17:08:44\n",
            "Epoch:1/250 || Epochiter: 80/403 || Iter: 80/100750 || Loc: 4.0503 Cla: 3.4553 Landm: 17.3418 || LR: 0.00100000 || Batchtime: 0.6569 s || ETA: 18:22:10\n",
            "Epoch:1/250 || Epochiter: 81/403 || Iter: 81/100750 || Loc: 3.9241 Cla: 3.3924 Landm: 16.8581 || LR: 0.00100000 || Batchtime: 2.5340 s || ETA: 2 days, 22:51:42\n",
            "Epoch:1/250 || Epochiter: 82/403 || Iter: 82/100750 || Loc: 4.0148 Cla: 3.4363 Landm: 16.8188 || LR: 0.00100000 || Batchtime: 0.5502 s || ETA: 15:23:08\n",
            "Epoch:1/250 || Epochiter: 83/403 || Iter: 83/100750 || Loc: 4.0421 Cla: 3.4330 Landm: 16.2606 || LR: 0.00100000 || Batchtime: 0.5798 s || ETA: 16:12:47\n",
            "Epoch:1/250 || Epochiter: 84/403 || Iter: 84/100750 || Loc: 3.9721 Cla: 3.2921 Landm: 16.0832 || LR: 0.00100000 || Batchtime: 0.9321 s || ETA: 1 day, 2:03:56\n",
            "Epoch:1/250 || Epochiter: 85/403 || Iter: 85/100750 || Loc: 4.1728 Cla: 3.5052 Landm: 17.0494 || LR: 0.00100000 || Batchtime: 2.3090 s || ETA: 2 days, 16:33:58\n",
            "Epoch:1/250 || Epochiter: 86/403 || Iter: 86/100750 || Loc: 4.0049 Cla: 3.4071 Landm: 17.2082 || LR: 0.00100000 || Batchtime: 0.6578 s || ETA: 18:23:42\n",
            "Epoch:1/250 || Epochiter: 87/403 || Iter: 87/100750 || Loc: 4.2045 Cla: 3.3993 Landm: 16.9360 || LR: 0.00100000 || Batchtime: 0.6298 s || ETA: 17:36:39\n",
            "Epoch:1/250 || Epochiter: 88/403 || Iter: 88/100750 || Loc: 3.9099 Cla: 3.4707 Landm: 16.7934 || LR: 0.00100000 || Batchtime: 1.2177 s || ETA: 1 day, 10:02:53\n",
            "Epoch:1/250 || Epochiter: 89/403 || Iter: 89/100750 || Loc: 4.1028 Cla: 3.4920 Landm: 17.4589 || LR: 0.00100000 || Batchtime: 2.8750 s || ETA: 3 days, 8:23:18\n",
            "Epoch:1/250 || Epochiter: 90/403 || Iter: 90/100750 || Loc: 3.9058 Cla: 3.4511 Landm: 17.3228 || LR: 0.00100000 || Batchtime: 0.6536 s || ETA: 18:16:35\n",
            "Epoch:1/250 || Epochiter: 91/403 || Iter: 91/100750 || Loc: 4.1831 Cla: 3.4534 Landm: 17.6640 || LR: 0.00100000 || Batchtime: 0.5006 s || ETA: 13:59:53\n",
            "Epoch:1/250 || Epochiter: 92/403 || Iter: 92/100750 || Loc: 3.8010 Cla: 3.4041 Landm: 16.0580 || LR: 0.00100000 || Batchtime: 1.3054 s || ETA: 1 day, 12:30:02\n",
            "Epoch:1/250 || Epochiter: 93/403 || Iter: 93/100750 || Loc: 4.0484 Cla: 3.4244 Landm: 15.8895 || LR: 0.00100000 || Batchtime: 2.6941 s || ETA: 3 days, 3:19:44\n",
            "Epoch:1/250 || Epochiter: 94/403 || Iter: 94/100750 || Loc: 3.9252 Cla: 3.4408 Landm: 16.8794 || LR: 0.00100000 || Batchtime: 0.5032 s || ETA: 14:04:15\n",
            "Epoch:1/250 || Epochiter: 95/403 || Iter: 95/100750 || Loc: 3.8873 Cla: 3.4154 Landm: 17.3291 || LR: 0.00100000 || Batchtime: 0.6214 s || ETA: 17:22:31\n",
            "Epoch:1/250 || Epochiter: 96/403 || Iter: 96/100750 || Loc: 3.9343 Cla: 3.4133 Landm: 16.5891 || LR: 0.00100000 || Batchtime: 0.7895 s || ETA: 22:04:23\n",
            "Epoch:1/250 || Epochiter: 97/403 || Iter: 97/100750 || Loc: 3.9959 Cla: 3.3435 Landm: 17.9829 || LR: 0.00100000 || Batchtime: 2.6603 s || ETA: 3 days, 2:22:54\n",
            "Epoch:1/250 || Epochiter: 98/403 || Iter: 98/100750 || Loc: 3.7823 Cla: 3.3911 Landm: 16.0074 || LR: 0.00100000 || Batchtime: 0.7112 s || ETA: 19:53:02\n",
            "Epoch:1/250 || Epochiter: 99/403 || Iter: 99/100750 || Loc: 4.0126 Cla: 3.3872 Landm: 16.7605 || LR: 0.00100000 || Batchtime: 0.6811 s || ETA: 19:02:33\n",
            "Epoch:1/250 || Epochiter: 100/403 || Iter: 100/100750 || Loc: 4.3539 Cla: 3.3873 Landm: 18.6205 || LR: 0.00100000 || Batchtime: 0.7125 s || ETA: 19:55:12\n",
            "Epoch:1/250 || Epochiter: 101/403 || Iter: 101/100750 || Loc: 3.9496 Cla: 3.3864 Landm: 16.8994 || LR: 0.00100000 || Batchtime: 2.3175 s || ETA: 2 days, 16:47:33\n",
            "Epoch:1/250 || Epochiter: 102/403 || Iter: 102/100750 || Loc: 4.0970 Cla: 3.3756 Landm: 16.6172 || LR: 0.00100000 || Batchtime: 0.5841 s || ETA: 16:19:50\n",
            "Epoch:1/250 || Epochiter: 103/403 || Iter: 103/100750 || Loc: 4.1030 Cla: 3.4038 Landm: 15.8764 || LR: 0.00100000 || Batchtime: 0.7289 s || ETA: 20:22:39\n",
            "Epoch:1/250 || Epochiter: 104/403 || Iter: 104/100750 || Loc: 3.8095 Cla: 3.3407 Landm: 16.1485 || LR: 0.00100000 || Batchtime: 0.6356 s || ETA: 17:46:15\n",
            "Epoch:1/250 || Epochiter: 105/403 || Iter: 105/100750 || Loc: 3.9399 Cla: 3.3620 Landm: 16.0565 || LR: 0.00100000 || Batchtime: 2.9538 s || ETA: 3 days, 10:34:52\n",
            "Epoch:1/250 || Epochiter: 106/403 || Iter: 106/100750 || Loc: 4.1260 Cla: 3.3355 Landm: 15.9529 || LR: 0.00100000 || Batchtime: 0.5698 s || ETA: 15:55:47\n",
            "Epoch:1/250 || Epochiter: 107/403 || Iter: 107/100750 || Loc: 4.0019 Cla: 3.3578 Landm: 16.8886 || LR: 0.00100000 || Batchtime: 0.5712 s || ETA: 15:58:07\n",
            "Epoch:1/250 || Epochiter: 108/403 || Iter: 108/100750 || Loc: 4.0358 Cla: 3.4275 Landm: 16.4724 || LR: 0.00100000 || Batchtime: 0.5678 s || ETA: 15:52:20\n",
            "Epoch:1/250 || Epochiter: 109/403 || Iter: 109/100750 || Loc: 3.8519 Cla: 3.4696 Landm: 15.9700 || LR: 0.00100000 || Batchtime: 3.5257 s || ETA: 4 days, 2:33:49\n",
            "Epoch:1/250 || Epochiter: 110/403 || Iter: 110/100750 || Loc: 4.0354 Cla: 3.3473 Landm: 16.9340 || LR: 0.00100000 || Batchtime: 0.6796 s || ETA: 18:59:56\n",
            "Epoch:1/250 || Epochiter: 111/403 || Iter: 111/100750 || Loc: 4.0483 Cla: 3.3415 Landm: 16.3404 || LR: 0.00100000 || Batchtime: 0.8443 s || ETA: 23:36:08\n",
            "Epoch:1/250 || Epochiter: 112/403 || Iter: 112/100750 || Loc: 3.9047 Cla: 3.3510 Landm: 15.9430 || LR: 0.00100000 || Batchtime: 0.8314 s || ETA: 23:14:32\n",
            "Epoch:1/250 || Epochiter: 113/403 || Iter: 113/100750 || Loc: 3.8555 Cla: 3.3122 Landm: 16.9136 || LR: 0.00100000 || Batchtime: 3.9030 s || ETA: 4 days, 13:06:33\n",
            "Epoch:1/250 || Epochiter: 114/403 || Iter: 114/100750 || Loc: 3.8910 Cla: 3.3072 Landm: 16.1766 || LR: 0.00100000 || Batchtime: 0.6661 s || ETA: 18:37:13\n",
            "Epoch:1/250 || Epochiter: 115/403 || Iter: 115/100750 || Loc: 4.2644 Cla: 3.3052 Landm: 15.5770 || LR: 0.00100000 || Batchtime: 0.7647 s || ETA: 21:22:40\n",
            "Epoch:1/250 || Epochiter: 116/403 || Iter: 116/100750 || Loc: 3.9676 Cla: 3.3497 Landm: 16.7100 || LR: 0.00100000 || Batchtime: 0.5956 s || ETA: 16:38:59\n",
            "Epoch:1/250 || Epochiter: 117/403 || Iter: 117/100750 || Loc: 3.9345 Cla: 3.3564 Landm: 16.0345 || LR: 0.00100000 || Batchtime: 2.4594 s || ETA: 2 days, 20:45:01\n",
            "Epoch:1/250 || Epochiter: 118/403 || Iter: 118/100750 || Loc: 3.8005 Cla: 3.3690 Landm: 15.4634 || LR: 0.00100000 || Batchtime: 0.5574 s || ETA: 15:34:48\n",
            "Epoch:1/250 || Epochiter: 119/403 || Iter: 119/100750 || Loc: 3.9061 Cla: 3.3700 Landm: 16.5396 || LR: 0.00100000 || Batchtime: 0.4640 s || ETA: 12:58:11\n",
            "Epoch:1/250 || Epochiter: 120/403 || Iter: 120/100750 || Loc: 3.8085 Cla: 3.2838 Landm: 16.0499 || LR: 0.00100000 || Batchtime: 0.5819 s || ETA: 16:15:57\n",
            "Epoch:1/250 || Epochiter: 121/403 || Iter: 121/100750 || Loc: 3.8203 Cla: 3.3072 Landm: 15.8634 || LR: 0.00100000 || Batchtime: 3.0252 s || ETA: 3 days, 12:33:41\n",
            "Epoch:1/250 || Epochiter: 122/403 || Iter: 122/100750 || Loc: 4.0648 Cla: 3.3548 Landm: 16.6486 || LR: 0.00100000 || Batchtime: 0.5786 s || ETA: 16:10:23\n",
            "Epoch:1/250 || Epochiter: 123/403 || Iter: 123/100750 || Loc: 3.7329 Cla: 3.3181 Landm: 15.1409 || LR: 0.00100000 || Batchtime: 0.5381 s || ETA: 15:02:26\n",
            "Epoch:1/250 || Epochiter: 124/403 || Iter: 124/100750 || Loc: 3.6423 Cla: 3.2664 Landm: 16.1256 || LR: 0.00100000 || Batchtime: 0.5563 s || ETA: 15:32:53\n",
            "Epoch:1/250 || Epochiter: 125/403 || Iter: 125/100750 || Loc: 3.8998 Cla: 3.2718 Landm: 15.9096 || LR: 0.00100000 || Batchtime: 3.0785 s || ETA: 3 days, 14:02:52\n",
            "Epoch:1/250 || Epochiter: 126/403 || Iter: 126/100750 || Loc: 4.0755 Cla: 3.3214 Landm: 16.2045 || LR: 0.00100000 || Batchtime: 0.5240 s || ETA: 14:38:49\n",
            "Epoch:1/250 || Epochiter: 127/403 || Iter: 127/100750 || Loc: 3.8592 Cla: 3.3214 Landm: 16.5584 || LR: 0.00100000 || Batchtime: 0.5195 s || ETA: 14:31:13\n",
            "Epoch:1/250 || Epochiter: 128/403 || Iter: 128/100750 || Loc: 4.1655 Cla: 3.3263 Landm: 17.0847 || LR: 0.00100000 || Batchtime: 0.4374 s || ETA: 12:13:29\n",
            "Epoch:1/250 || Epochiter: 129/403 || Iter: 129/100750 || Loc: 3.8929 Cla: 3.2473 Landm: 15.9400 || LR: 0.00100000 || Batchtime: 2.6521 s || ETA: 3 days, 2:07:36\n",
            "Epoch:1/250 || Epochiter: 130/403 || Iter: 130/100750 || Loc: 3.7145 Cla: 3.3232 Landm: 15.9164 || LR: 0.00100000 || Batchtime: 0.6053 s || ETA: 16:55:09\n",
            "Epoch:1/250 || Epochiter: 131/403 || Iter: 131/100750 || Loc: 4.0035 Cla: 3.3079 Landm: 16.0016 || LR: 0.00100000 || Batchtime: 0.4102 s || ETA: 11:27:56\n",
            "Epoch:1/250 || Epochiter: 132/403 || Iter: 132/100750 || Loc: 3.8128 Cla: 3.2852 Landm: 15.6982 || LR: 0.00100000 || Batchtime: 0.5423 s || ETA: 15:09:22\n",
            "Epoch:1/250 || Epochiter: 133/403 || Iter: 133/100750 || Loc: 4.0555 Cla: 3.2923 Landm: 17.4091 || LR: 0.00100000 || Batchtime: 2.9842 s || ETA: 3 days, 11:24:20\n",
            "Epoch:1/250 || Epochiter: 134/403 || Iter: 134/100750 || Loc: 4.2861 Cla: 3.1920 Landm: 15.8729 || LR: 0.00100000 || Batchtime: 0.5755 s || ETA: 16:05:08\n",
            "Epoch:1/250 || Epochiter: 135/403 || Iter: 135/100750 || Loc: 4.1611 Cla: 3.3144 Landm: 15.4302 || LR: 0.00100000 || Batchtime: 0.6803 s || ETA: 19:00:50\n",
            "Epoch:1/250 || Epochiter: 136/403 || Iter: 136/100750 || Loc: 4.2079 Cla: 3.2888 Landm: 16.1786 || LR: 0.00100000 || Batchtime: 0.5250 s || ETA: 14:40:25\n",
            "Epoch:1/250 || Epochiter: 137/403 || Iter: 137/100750 || Loc: 4.3185 Cla: 3.2803 Landm: 16.1105 || LR: 0.00100000 || Batchtime: 2.1840 s || ETA: 2 days, 13:02:22\n",
            "Epoch:1/250 || Epochiter: 138/403 || Iter: 138/100750 || Loc: 3.8613 Cla: 3.2850 Landm: 15.2092 || LR: 0.00100000 || Batchtime: 0.6838 s || ETA: 19:06:37\n",
            "Epoch:1/250 || Epochiter: 139/403 || Iter: 139/100750 || Loc: 3.9686 Cla: 3.3180 Landm: 16.1458 || LR: 0.00100000 || Batchtime: 0.9156 s || ETA: 1 day, 1:35:23\n",
            "Epoch:1/250 || Epochiter: 140/403 || Iter: 140/100750 || Loc: 3.8780 Cla: 3.3097 Landm: 16.1377 || LR: 0.00100000 || Batchtime: 0.6280 s || ETA: 17:33:05\n",
            "Epoch:1/250 || Epochiter: 141/403 || Iter: 141/100750 || Loc: 3.6352 Cla: 3.3303 Landm: 16.1710 || LR: 0.00100000 || Batchtime: 2.5305 s || ETA: 2 days, 22:43:11\n",
            "Epoch:1/250 || Epochiter: 142/403 || Iter: 142/100750 || Loc: 3.4750 Cla: 3.3123 Landm: 15.3242 || LR: 0.00100000 || Batchtime: 0.5691 s || ETA: 15:54:19\n",
            "Epoch:1/250 || Epochiter: 143/403 || Iter: 143/100750 || Loc: 3.8216 Cla: 3.3073 Landm: 15.5862 || LR: 0.00100000 || Batchtime: 0.6685 s || ETA: 18:40:59\n",
            "Epoch:1/250 || Epochiter: 144/403 || Iter: 144/100750 || Loc: 3.8064 Cla: 3.2807 Landm: 16.0406 || LR: 0.00100000 || Batchtime: 0.6199 s || ETA: 17:19:23\n",
            "Epoch:1/250 || Epochiter: 145/403 || Iter: 145/100750 || Loc: 4.3406 Cla: 3.3266 Landm: 17.2798 || LR: 0.00100000 || Batchtime: 2.1788 s || ETA: 2 days, 12:53:16\n",
            "Epoch:1/250 || Epochiter: 146/403 || Iter: 146/100750 || Loc: 3.8683 Cla: 3.2755 Landm: 15.2745 || LR: 0.00100000 || Batchtime: 0.5848 s || ETA: 16:20:31\n",
            "Epoch:1/250 || Epochiter: 147/403 || Iter: 147/100750 || Loc: 3.8027 Cla: 3.2584 Landm: 15.9616 || LR: 0.00100000 || Batchtime: 0.6095 s || ETA: 17:01:55\n",
            "Epoch:1/250 || Epochiter: 148/403 || Iter: 148/100750 || Loc: 3.6971 Cla: 3.2651 Landm: 14.5578 || LR: 0.00100000 || Batchtime: 1.9535 s || ETA: 2 days, 6:35:32\n",
            "Epoch:1/250 || Epochiter: 149/403 || Iter: 149/100750 || Loc: 3.8628 Cla: 3.2534 Landm: 16.9249 || LR: 0.00100000 || Batchtime: 1.3603 s || ETA: 1 day, 14:00:48\n",
            "Epoch:1/250 || Epochiter: 150/403 || Iter: 150/100750 || Loc: 3.7932 Cla: 3.2514 Landm: 15.3675 || LR: 0.00100000 || Batchtime: 0.5992 s || ETA: 16:44:41\n",
            "Epoch:1/250 || Epochiter: 151/403 || Iter: 151/100750 || Loc: 4.0221 Cla: 3.2771 Landm: 15.2215 || LR: 0.00100000 || Batchtime: 1.4557 s || ETA: 1 day, 16:40:38\n",
            "Epoch:1/250 || Epochiter: 152/403 || Iter: 152/100750 || Loc: 3.8020 Cla: 3.3034 Landm: 16.9134 || LR: 0.00100000 || Batchtime: 1.3728 s || ETA: 1 day, 14:21:42\n",
            "Epoch:1/250 || Epochiter: 153/403 || Iter: 153/100750 || Loc: 3.7420 Cla: 3.2047 Landm: 15.5356 || LR: 0.00100000 || Batchtime: 0.8793 s || ETA: 1 day, 0:34:18\n",
            "Epoch:1/250 || Epochiter: 154/403 || Iter: 154/100750 || Loc: 3.7970 Cla: 3.2327 Landm: 14.8813 || LR: 0.00100000 || Batchtime: 0.5583 s || ETA: 15:35:58\n",
            "Epoch:1/250 || Epochiter: 155/403 || Iter: 155/100750 || Loc: 3.9353 Cla: 3.2615 Landm: 16.2655 || LR: 0.00100000 || Batchtime: 2.0027 s || ETA: 2 days, 7:57:47\n",
            "Epoch:1/250 || Epochiter: 156/403 || Iter: 156/100750 || Loc: 3.8034 Cla: 3.2412 Landm: 15.4211 || LR: 0.00100000 || Batchtime: 1.6888 s || ETA: 1 day, 23:11:23\n",
            "Epoch:1/250 || Epochiter: 157/403 || Iter: 157/100750 || Loc: 3.8159 Cla: 3.2904 Landm: 15.1784 || LR: 0.00100000 || Batchtime: 0.7401 s || ETA: 20:40:46\n",
            "Epoch:1/250 || Epochiter: 158/403 || Iter: 158/100750 || Loc: 3.8901 Cla: 3.2694 Landm: 16.1956 || LR: 0.00100000 || Batchtime: 0.6973 s || ETA: 19:29:00\n",
            "Epoch:1/250 || Epochiter: 159/403 || Iter: 159/100750 || Loc: 3.7567 Cla: 3.2784 Landm: 15.7397 || LR: 0.00100000 || Batchtime: 1.4426 s || ETA: 1 day, 16:18:36\n",
            "Epoch:1/250 || Epochiter: 160/403 || Iter: 160/100750 || Loc: 3.9409 Cla: 3.2552 Landm: 16.8338 || LR: 0.00100000 || Batchtime: 1.8551 s || ETA: 2 days, 3:50:02\n",
            "Epoch:1/250 || Epochiter: 161/403 || Iter: 161/100750 || Loc: 3.8115 Cla: 3.2862 Landm: 15.2439 || LR: 0.00100000 || Batchtime: 1.0760 s || ETA: 1 day, 6:03:57\n",
            "Epoch:1/250 || Epochiter: 162/403 || Iter: 162/100750 || Loc: 3.7507 Cla: 3.2283 Landm: 15.7944 || LR: 0.00100000 || Batchtime: 0.4965 s || ETA: 13:52:21\n",
            "Epoch:1/250 || Epochiter: 163/403 || Iter: 163/100750 || Loc: 3.7900 Cla: 3.2269 Landm: 15.6107 || LR: 0.00100000 || Batchtime: 2.2269 s || ETA: 2 days, 14:13:19\n",
            "Epoch:1/250 || Epochiter: 164/403 || Iter: 164/100750 || Loc: 3.7179 Cla: 3.2473 Landm: 16.1111 || LR: 0.00100000 || Batchtime: 0.4577 s || ETA: 12:47:19\n",
            "Epoch:1/250 || Epochiter: 165/403 || Iter: 165/100750 || Loc: 3.9523 Cla: 3.2793 Landm: 16.2342 || LR: 0.00100000 || Batchtime: 1.1357 s || ETA: 1 day, 7:43:58\n",
            "Epoch:1/250 || Epochiter: 166/403 || Iter: 166/100750 || Loc: 3.9343 Cla: 3.1988 Landm: 15.5280 || LR: 0.00100000 || Batchtime: 0.6446 s || ETA: 18:00:37\n",
            "Epoch:1/250 || Epochiter: 167/403 || Iter: 167/100750 || Loc: 3.9848 Cla: 3.2277 Landm: 15.2875 || LR: 0.00100000 || Batchtime: 2.1487 s || ETA: 2 days, 12:02:05\n",
            "Epoch:1/250 || Epochiter: 168/403 || Iter: 168/100750 || Loc: 3.8367 Cla: 3.2486 Landm: 14.8458 || LR: 0.00100000 || Batchtime: 0.7543 s || ETA: 21:04:32\n",
            "Epoch:1/250 || Epochiter: 169/403 || Iter: 169/100750 || Loc: 4.0865 Cla: 3.2231 Landm: 15.9132 || LR: 0.00100000 || Batchtime: 0.7631 s || ETA: 21:19:12\n",
            "Epoch:1/250 || Epochiter: 170/403 || Iter: 170/100750 || Loc: 4.0846 Cla: 3.2779 Landm: 16.8652 || LR: 0.00100000 || Batchtime: 1.0932 s || ETA: 1 day, 6:32:37\n",
            "Epoch:1/250 || Epochiter: 171/403 || Iter: 171/100750 || Loc: 3.6691 Cla: 3.2237 Landm: 15.4679 || LR: 0.00100000 || Batchtime: 2.0642 s || ETA: 2 days, 9:40:12\n",
            "Epoch:1/250 || Epochiter: 172/403 || Iter: 172/100750 || Loc: 3.8773 Cla: 3.2261 Landm: 16.1416 || LR: 0.00100000 || Batchtime: 0.7107 s || ETA: 19:51:25\n",
            "Epoch:1/250 || Epochiter: 173/403 || Iter: 173/100750 || Loc: 3.7219 Cla: 3.2780 Landm: 14.7315 || LR: 0.00100000 || Batchtime: 1.2730 s || ETA: 1 day, 11:34:00\n",
            "Epoch:1/250 || Epochiter: 174/403 || Iter: 174/100750 || Loc: 3.8679 Cla: 3.2866 Landm: 16.1192 || LR: 0.00100000 || Batchtime: 0.5459 s || ETA: 15:15:06\n",
            "Epoch:1/250 || Epochiter: 175/403 || Iter: 175/100750 || Loc: 3.7854 Cla: 3.2457 Landm: 16.4577 || LR: 0.00100000 || Batchtime: 2.5113 s || ETA: 2 days, 22:09:33\n",
            "Epoch:1/250 || Epochiter: 176/403 || Iter: 176/100750 || Loc: 3.4144 Cla: 3.2085 Landm: 15.2214 || LR: 0.00100000 || Batchtime: 0.5596 s || ETA: 15:38:06\n",
            "Epoch:1/250 || Epochiter: 177/403 || Iter: 177/100750 || Loc: 3.5942 Cla: 3.2559 Landm: 15.3891 || LR: 0.00100000 || Batchtime: 1.1963 s || ETA: 1 day, 9:25:17\n",
            "Epoch:1/250 || Epochiter: 178/403 || Iter: 178/100750 || Loc: 3.9736 Cla: 3.2268 Landm: 16.5448 || LR: 0.00100000 || Batchtime: 0.5956 s || ETA: 16:38:18\n",
            "Epoch:1/250 || Epochiter: 179/403 || Iter: 179/100750 || Loc: 3.6500 Cla: 3.2697 Landm: 15.2730 || LR: 0.00100000 || Batchtime: 2.8823 s || ETA: 3 days, 8:31:21\n",
            "Epoch:1/250 || Epochiter: 180/403 || Iter: 180/100750 || Loc: 3.9678 Cla: 3.2222 Landm: 15.1969 || LR: 0.00100000 || Batchtime: 0.5675 s || ETA: 15:51:09\n",
            "Epoch:1/250 || Epochiter: 181/403 || Iter: 181/100750 || Loc: 3.8364 Cla: 3.2110 Landm: 15.1567 || LR: 0.00100000 || Batchtime: 0.4433 s || ETA: 12:23:05\n",
            "Epoch:1/250 || Epochiter: 182/403 || Iter: 182/100750 || Loc: 3.9411 Cla: 3.2615 Landm: 15.3375 || LR: 0.00100000 || Batchtime: 0.5819 s || ETA: 16:15:23\n",
            "Epoch:1/250 || Epochiter: 183/403 || Iter: 183/100750 || Loc: 3.7512 Cla: 3.2009 Landm: 16.1236 || LR: 0.00100000 || Batchtime: 3.6279 s || ETA: 4 days, 5:20:55\n",
            "Epoch:1/250 || Epochiter: 184/403 || Iter: 184/100750 || Loc: 4.0297 Cla: 3.2558 Landm: 16.4344 || LR: 0.00100000 || Batchtime: 0.5106 s || ETA: 14:15:50\n",
            "Epoch:1/250 || Epochiter: 185/403 || Iter: 185/100750 || Loc: 3.6843 Cla: 3.2045 Landm: 14.5235 || LR: 0.00100000 || Batchtime: 0.6125 s || ETA: 17:06:41\n",
            "Epoch:1/250 || Epochiter: 186/403 || Iter: 186/100750 || Loc: 3.5545 Cla: 3.2150 Landm: 15.4413 || LR: 0.00100000 || Batchtime: 0.5947 s || ETA: 16:36:46\n",
            "Epoch:1/250 || Epochiter: 187/403 || Iter: 187/100750 || Loc: 3.8843 Cla: 3.1963 Landm: 15.8629 || LR: 0.00100000 || Batchtime: 2.6732 s || ETA: 3 days, 2:40:24\n",
            "Epoch:1/250 || Epochiter: 188/403 || Iter: 188/100750 || Loc: 3.7608 Cla: 3.2020 Landm: 15.4937 || LR: 0.00100000 || Batchtime: 0.6251 s || ETA: 17:27:37\n",
            "Epoch:1/250 || Epochiter: 189/403 || Iter: 189/100750 || Loc: 3.8118 Cla: 3.2360 Landm: 15.2070 || LR: 0.00100000 || Batchtime: 0.3998 s || ETA: 11:10:05\n",
            "Epoch:1/250 || Epochiter: 190/403 || Iter: 190/100750 || Loc: 3.7043 Cla: 3.2549 Landm: 14.1513 || LR: 0.00100000 || Batchtime: 0.5590 s || ETA: 15:36:51\n",
            "Epoch:1/250 || Epochiter: 191/403 || Iter: 191/100750 || Loc: 3.9585 Cla: 3.2080 Landm: 16.1145 || LR: 0.00100000 || Batchtime: 2.7760 s || ETA: 3 days, 5:32:38\n",
            "Epoch:1/250 || Epochiter: 192/403 || Iter: 192/100750 || Loc: 3.8599 Cla: 3.2563 Landm: 16.2092 || LR: 0.00100000 || Batchtime: 0.6933 s || ETA: 19:22:01\n",
            "Epoch:1/250 || Epochiter: 193/403 || Iter: 193/100750 || Loc: 3.8551 Cla: 3.2033 Landm: 15.9239 || LR: 0.00100000 || Batchtime: 1.0709 s || ETA: 1 day, 5:54:45\n",
            "Epoch:1/250 || Epochiter: 194/403 || Iter: 194/100750 || Loc: 3.8119 Cla: 3.2502 Landm: 14.5830 || LR: 0.00100000 || Batchtime: 0.5139 s || ETA: 14:21:13\n",
            "Epoch:1/250 || Epochiter: 195/403 || Iter: 195/100750 || Loc: 3.7456 Cla: 3.2310 Landm: 16.3066 || LR: 0.00100000 || Batchtime: 1.9107 s || ETA: 2 days, 5:22:13\n",
            "Epoch:1/250 || Epochiter: 196/403 || Iter: 196/100750 || Loc: 3.8383 Cla: 3.2174 Landm: 15.0986 || LR: 0.00100000 || Batchtime: 1.0481 s || ETA: 1 day, 5:16:29\n",
            "Epoch:1/250 || Epochiter: 197/403 || Iter: 197/100750 || Loc: 4.0482 Cla: 3.2244 Landm: 15.2836 || LR: 0.00100000 || Batchtime: 1.1663 s || ETA: 1 day, 8:34:31\n",
            "Epoch:1/250 || Epochiter: 198/403 || Iter: 198/100750 || Loc: 4.0442 Cla: 3.1988 Landm: 15.8974 || LR: 0.00100000 || Batchtime: 0.6381 s || ETA: 17:49:19\n",
            "Epoch:1/250 || Epochiter: 199/403 || Iter: 199/100750 || Loc: 3.8045 Cla: 3.2245 Landm: 16.4147 || LR: 0.00100000 || Batchtime: 2.3639 s || ETA: 2 days, 18:01:30\n",
            "Epoch:1/250 || Epochiter: 200/403 || Iter: 200/100750 || Loc: 3.8860 Cla: 3.2653 Landm: 15.6887 || LR: 0.00100000 || Batchtime: 0.9535 s || ETA: 1 day, 2:37:52\n",
            "Epoch:1/250 || Epochiter: 201/403 || Iter: 201/100750 || Loc: 3.7449 Cla: 3.2332 Landm: 16.5981 || LR: 0.00100000 || Batchtime: 0.6249 s || ETA: 17:27:09\n",
            "Epoch:1/250 || Epochiter: 202/403 || Iter: 202/100750 || Loc: 3.7913 Cla: 3.2287 Landm: 15.0231 || LR: 0.00100000 || Batchtime: 0.5216 s || ETA: 14:34:06\n",
            "Epoch:1/250 || Epochiter: 203/403 || Iter: 203/100750 || Loc: 3.7332 Cla: 3.1894 Landm: 15.2763 || LR: 0.00100000 || Batchtime: 2.6437 s || ETA: 3 days, 1:50:19\n",
            "Epoch:1/250 || Epochiter: 204/403 || Iter: 204/100750 || Loc: 3.6070 Cla: 3.2167 Landm: 15.0894 || LR: 0.00100000 || Batchtime: 1.4891 s || ETA: 1 day, 17:35:27\n",
            "Epoch:1/250 || Epochiter: 205/403 || Iter: 205/100750 || Loc: 3.7379 Cla: 3.2340 Landm: 15.0114 || LR: 0.00100000 || Batchtime: 0.5387 s || ETA: 15:02:45\n",
            "Epoch:1/250 || Epochiter: 206/403 || Iter: 206/100750 || Loc: 3.6278 Cla: 3.2313 Landm: 15.3398 || LR: 0.00100000 || Batchtime: 0.5673 s || ETA: 15:50:42\n",
            "Epoch:1/250 || Epochiter: 207/403 || Iter: 207/100750 || Loc: 3.7420 Cla: 3.2092 Landm: 14.8203 || LR: 0.00100000 || Batchtime: 1.8125 s || ETA: 2 days, 2:37:12\n",
            "Epoch:1/250 || Epochiter: 208/403 || Iter: 208/100750 || Loc: 3.5390 Cla: 3.2377 Landm: 14.7511 || LR: 0.00100000 || Batchtime: 1.3739 s || ETA: 1 day, 14:22:18\n",
            "Epoch:1/250 || Epochiter: 209/403 || Iter: 209/100750 || Loc: 3.6640 Cla: 3.2368 Landm: 14.6437 || LR: 0.00100000 || Batchtime: 0.5292 s || ETA: 14:46:42\n",
            "Epoch:1/250 || Epochiter: 210/403 || Iter: 210/100750 || Loc: 3.7748 Cla: 3.2095 Landm: 14.8362 || LR: 0.00100000 || Batchtime: 0.6040 s || ETA: 16:52:02\n",
            "Epoch:1/250 || Epochiter: 211/403 || Iter: 211/100750 || Loc: 3.4591 Cla: 3.2109 Landm: 15.3776 || LR: 0.00100000 || Batchtime: 2.3597 s || ETA: 2 days, 17:54:00\n",
            "Epoch:1/250 || Epochiter: 212/403 || Iter: 212/100750 || Loc: 3.4742 Cla: 3.2143 Landm: 14.7801 || LR: 0.00100000 || Batchtime: 0.9112 s || ETA: 1 day, 1:26:54\n",
            "Epoch:1/250 || Epochiter: 213/403 || Iter: 213/100750 || Loc: 3.4429 Cla: 3.1927 Landm: 14.4074 || LR: 0.00100000 || Batchtime: 0.5654 s || ETA: 15:47:27\n",
            "Epoch:1/250 || Epochiter: 214/403 || Iter: 214/100750 || Loc: 3.5963 Cla: 3.1967 Landm: 15.0311 || LR: 0.00100000 || Batchtime: 0.6976 s || ETA: 19:28:52\n",
            "Epoch:1/250 || Epochiter: 215/403 || Iter: 215/100750 || Loc: 3.7526 Cla: 3.2270 Landm: 15.0813 || LR: 0.00100000 || Batchtime: 2.7959 s || ETA: 3 days, 6:04:53\n",
            "Epoch:1/250 || Epochiter: 216/403 || Iter: 216/100750 || Loc: 3.5669 Cla: 3.1998 Landm: 15.1270 || LR: 0.00100000 || Batchtime: 0.7171 s || ETA: 20:01:31\n",
            "Epoch:1/250 || Epochiter: 217/403 || Iter: 217/100750 || Loc: 3.5517 Cla: 3.1643 Landm: 14.2836 || LR: 0.00100000 || Batchtime: 0.5693 s || ETA: 15:53:52\n",
            "Epoch:1/250 || Epochiter: 218/403 || Iter: 218/100750 || Loc: 3.6392 Cla: 3.2024 Landm: 14.0248 || LR: 0.00100000 || Batchtime: 0.5893 s || ETA: 16:27:19\n",
            "Epoch:1/250 || Epochiter: 219/403 || Iter: 219/100750 || Loc: 3.5314 Cla: 3.1680 Landm: 14.4075 || LR: 0.00100000 || Batchtime: 3.4464 s || ETA: 4 days, 0:14:35\n",
            "Epoch:1/250 || Epochiter: 220/403 || Iter: 220/100750 || Loc: 3.4992 Cla: 3.1963 Landm: 14.8106 || LR: 0.00100000 || Batchtime: 0.5929 s || ETA: 16:33:20\n",
            "Epoch:1/250 || Epochiter: 221/403 || Iter: 221/100750 || Loc: 3.4628 Cla: 3.2146 Landm: 14.0519 || LR: 0.00100000 || Batchtime: 0.5431 s || ETA: 15:09:55\n",
            "Epoch:1/250 || Epochiter: 222/403 || Iter: 222/100750 || Loc: 3.6896 Cla: 3.2047 Landm: 14.2774 || LR: 0.00100000 || Batchtime: 0.5605 s || ETA: 15:39:04\n",
            "Epoch:1/250 || Epochiter: 223/403 || Iter: 223/100750 || Loc: 3.7664 Cla: 3.2285 Landm: 15.6981 || LR: 0.00100000 || Batchtime: 3.5991 s || ETA: 4 days, 4:30:14\n",
            "Epoch:1/250 || Epochiter: 224/403 || Iter: 224/100750 || Loc: 3.5225 Cla: 3.2150 Landm: 14.6890 || LR: 0.00100000 || Batchtime: 0.4775 s || ETA: 13:20:04\n",
            "Epoch:1/250 || Epochiter: 225/403 || Iter: 225/100750 || Loc: 3.5238 Cla: 3.1774 Landm: 14.3962 || LR: 0.00100000 || Batchtime: 0.5544 s || ETA: 15:28:54\n",
            "Epoch:1/250 || Epochiter: 226/403 || Iter: 226/100750 || Loc: 3.1063 Cla: 3.1460 Landm: 13.5048 || LR: 0.00100000 || Batchtime: 0.5817 s || ETA: 16:14:38\n",
            "Epoch:1/250 || Epochiter: 227/403 || Iter: 227/100750 || Loc: 3.5961 Cla: 3.2348 Landm: 15.9403 || LR: 0.00100000 || Batchtime: 2.1627 s || ETA: 2 days, 12:23:20\n",
            "Epoch:1/250 || Epochiter: 228/403 || Iter: 228/100750 || Loc: 3.5055 Cla: 3.2335 Landm: 15.1480 || LR: 0.00100000 || Batchtime: 0.5242 s || ETA: 14:38:16\n",
            "Epoch:1/250 || Epochiter: 229/403 || Iter: 229/100750 || Loc: 3.6928 Cla: 3.2196 Landm: 14.9849 || LR: 0.00100000 || Batchtime: 0.4415 s || ETA: 12:19:37\n",
            "Epoch:1/250 || Epochiter: 230/403 || Iter: 230/100750 || Loc: 3.4610 Cla: 3.2184 Landm: 14.3753 || LR: 0.00100000 || Batchtime: 0.6851 s || ETA: 19:07:42\n",
            "Epoch:1/250 || Epochiter: 231/403 || Iter: 231/100750 || Loc: 3.4025 Cla: 3.2276 Landm: 13.4810 || LR: 0.00100000 || Batchtime: 2.6424 s || ETA: 3 days, 1:46:49\n",
            "Epoch:1/250 || Epochiter: 232/403 || Iter: 232/100750 || Loc: 3.7127 Cla: 3.1973 Landm: 14.8639 || LR: 0.00100000 || Batchtime: 0.6962 s || ETA: 19:26:24\n",
            "Epoch:1/250 || Epochiter: 233/403 || Iter: 233/100750 || Loc: 3.3408 Cla: 3.1877 Landm: 14.0168 || LR: 0.00100000 || Batchtime: 0.6398 s || ETA: 17:51:51\n",
            "Epoch:1/250 || Epochiter: 234/403 || Iter: 234/100750 || Loc: 3.4626 Cla: 3.1831 Landm: 14.2084 || LR: 0.00100000 || Batchtime: 0.5012 s || ETA: 13:59:37\n",
            "Epoch:1/250 || Epochiter: 235/403 || Iter: 235/100750 || Loc: 3.3984 Cla: 3.2282 Landm: 14.4505 || LR: 0.00100000 || Batchtime: 3.5955 s || ETA: 4 days, 4:23:26\n",
            "Epoch:1/250 || Epochiter: 236/403 || Iter: 236/100750 || Loc: 3.7599 Cla: 3.2386 Landm: 14.7744 || LR: 0.00100000 || Batchtime: 0.6710 s || ETA: 18:44:09\n",
            "Epoch:1/250 || Epochiter: 237/403 || Iter: 237/100750 || Loc: 3.9226 Cla: 3.2575 Landm: 15.6671 || LR: 0.00100000 || Batchtime: 0.7762 s || ETA: 21:40:19\n",
            "Epoch:1/250 || Epochiter: 238/403 || Iter: 238/100750 || Loc: 3.1690 Cla: 3.2674 Landm: 13.7985 || LR: 0.00100000 || Batchtime: 0.5650 s || ETA: 15:46:26\n",
            "Epoch:1/250 || Epochiter: 239/403 || Iter: 239/100750 || Loc: 3.7560 Cla: 3.1836 Landm: 14.9609 || LR: 0.00100000 || Batchtime: 2.6244 s || ETA: 3 days, 1:16:27\n",
            "Epoch:1/250 || Epochiter: 240/403 || Iter: 240/100750 || Loc: 3.3530 Cla: 3.1457 Landm: 14.7004 || LR: 0.00100000 || Batchtime: 0.5184 s || ETA: 14:28:23\n",
            "Epoch:1/250 || Epochiter: 241/403 || Iter: 241/100750 || Loc: 3.4267 Cla: 3.1822 Landm: 14.7227 || LR: 0.00100000 || Batchtime: 0.5786 s || ETA: 16:09:19\n",
            "Epoch:1/250 || Epochiter: 242/403 || Iter: 242/100750 || Loc: 3.9501 Cla: 3.2188 Landm: 15.1652 || LR: 0.00100000 || Batchtime: 0.6443 s || ETA: 17:59:21\n",
            "Epoch:1/250 || Epochiter: 243/403 || Iter: 243/100750 || Loc: 3.4655 Cla: 3.2203 Landm: 13.8425 || LR: 0.00100000 || Batchtime: 2.9353 s || ETA: 3 days, 9:56:57\n",
            "Epoch:1/250 || Epochiter: 244/403 || Iter: 244/100750 || Loc: 3.4066 Cla: 3.1974 Landm: 14.1148 || LR: 0.00100000 || Batchtime: 0.7262 s || ETA: 20:16:24\n",
            "Epoch:1/250 || Epochiter: 245/403 || Iter: 245/100750 || Loc: 3.3993 Cla: 3.2048 Landm: 13.2731 || LR: 0.00100000 || Batchtime: 0.8355 s || ETA: 23:19:32\n",
            "Epoch:1/250 || Epochiter: 246/403 || Iter: 246/100750 || Loc: 3.7491 Cla: 3.1798 Landm: 15.4472 || LR: 0.00100000 || Batchtime: 0.8803 s || ETA: 1 day, 0:34:38\n",
            "Epoch:1/250 || Epochiter: 247/403 || Iter: 247/100750 || Loc: 3.5815 Cla: 3.1735 Landm: 14.0018 || LR: 0.00100000 || Batchtime: 3.6257 s || ETA: 4 days, 5:13:13\n",
            "Epoch:1/250 || Epochiter: 248/403 || Iter: 248/100750 || Loc: 3.4828 Cla: 3.2714 Landm: 15.3299 || LR: 0.00100000 || Batchtime: 0.6805 s || ETA: 18:59:52\n",
            "Epoch:1/250 || Epochiter: 249/403 || Iter: 249/100750 || Loc: 3.4067 Cla: 3.1952 Landm: 13.6644 || LR: 0.00100000 || Batchtime: 0.5599 s || ETA: 15:37:54\n",
            "Epoch:1/250 || Epochiter: 250/403 || Iter: 250/100750 || Loc: 3.5374 Cla: 3.1823 Landm: 13.7901 || LR: 0.00100000 || Batchtime: 0.7058 s || ETA: 19:42:16\n",
            "Epoch:1/250 || Epochiter: 251/403 || Iter: 251/100750 || Loc: 3.4833 Cla: 3.2064 Landm: 15.4828 || LR: 0.00100000 || Batchtime: 2.0785 s || ETA: 2 days, 10:01:29\n",
            "Epoch:1/250 || Epochiter: 252/403 || Iter: 252/100750 || Loc: 3.2937 Cla: 3.1846 Landm: 13.4828 || LR: 0.00100000 || Batchtime: 0.5730 s || ETA: 15:59:48\n",
            "Epoch:1/250 || Epochiter: 253/403 || Iter: 253/100750 || Loc: 3.6278 Cla: 3.1799 Landm: 14.6469 || LR: 0.00100000 || Batchtime: 0.6309 s || ETA: 17:36:44\n",
            "Epoch:1/250 || Epochiter: 254/403 || Iter: 254/100750 || Loc: 3.8513 Cla: 3.1855 Landm: 14.3873 || LR: 0.00100000 || Batchtime: 0.7458 s || ETA: 20:49:06\n",
            "Epoch:1/250 || Epochiter: 255/403 || Iter: 255/100750 || Loc: 3.7181 Cla: 3.1545 Landm: 14.2182 || LR: 0.00100000 || Batchtime: 2.7015 s || ETA: 3 days, 3:24:52\n",
            "Epoch:1/250 || Epochiter: 256/403 || Iter: 256/100750 || Loc: 3.4674 Cla: 3.2194 Landm: 15.5571 || LR: 0.00100000 || Batchtime: 0.6003 s || ETA: 16:45:31\n",
            "Epoch:1/250 || Epochiter: 257/403 || Iter: 257/100750 || Loc: 3.8211 Cla: 3.2540 Landm: 15.5613 || LR: 0.00100000 || Batchtime: 0.5772 s || ETA: 16:06:41\n",
            "Epoch:1/250 || Epochiter: 258/403 || Iter: 258/100750 || Loc: 3.2872 Cla: 3.1968 Landm: 14.1035 || LR: 0.00100000 || Batchtime: 0.5178 s || ETA: 14:27:15\n",
            "Epoch:1/250 || Epochiter: 259/403 || Iter: 259/100750 || Loc: 3.4099 Cla: 3.2099 Landm: 12.8203 || LR: 0.00100000 || Batchtime: 3.0151 s || ETA: 3 days, 12:09:58\n",
            "Epoch:1/250 || Epochiter: 260/403 || Iter: 260/100750 || Loc: 3.0428 Cla: 3.1506 Landm: 12.6106 || LR: 0.00100000 || Batchtime: 0.4952 s || ETA: 13:49:24\n",
            "Epoch:1/250 || Epochiter: 261/403 || Iter: 261/100750 || Loc: 3.4484 Cla: 3.2170 Landm: 14.7582 || LR: 0.00100000 || Batchtime: 0.5782 s || ETA: 16:08:22\n",
            "Epoch:1/250 || Epochiter: 262/403 || Iter: 262/100750 || Loc: 3.7584 Cla: 3.2287 Landm: 15.4177 || LR: 0.00100000 || Batchtime: 0.7537 s || ETA: 21:02:21\n",
            "Epoch:1/250 || Epochiter: 263/403 || Iter: 263/100750 || Loc: 3.7302 Cla: 3.1946 Landm: 14.3765 || LR: 0.00100000 || Batchtime: 2.6604 s || ETA: 3 days, 2:15:35\n",
            "Epoch:1/250 || Epochiter: 264/403 || Iter: 264/100750 || Loc: 3.4002 Cla: 3.1940 Landm: 14.0096 || LR: 0.00100000 || Batchtime: 0.5418 s || ETA: 15:07:22\n",
            "Epoch:1/250 || Epochiter: 265/403 || Iter: 265/100750 || Loc: 3.2317 Cla: 3.1847 Landm: 13.6494 || LR: 0.00100000 || Batchtime: 0.4173 s || ETA: 11:38:53\n",
            "Epoch:1/250 || Epochiter: 266/403 || Iter: 266/100750 || Loc: 3.8128 Cla: 3.2477 Landm: 14.6688 || LR: 0.00100000 || Batchtime: 0.5594 s || ETA: 15:36:55\n",
            "Epoch:1/250 || Epochiter: 267/403 || Iter: 267/100750 || Loc: 2.9563 Cla: 3.2077 Landm: 13.0178 || LR: 0.00100000 || Batchtime: 3.0097 s || ETA: 3 days, 12:00:29\n",
            "Epoch:1/250 || Epochiter: 268/403 || Iter: 268/100750 || Loc: 3.1557 Cla: 3.1820 Landm: 12.9516 || LR: 0.00100000 || Batchtime: 0.4548 s || ETA: 12:41:40\n",
            "Epoch:1/250 || Epochiter: 269/403 || Iter: 269/100750 || Loc: 3.5271 Cla: 3.2141 Landm: 14.8875 || LR: 0.00100000 || Batchtime: 0.5416 s || ETA: 15:06:58\n",
            "Epoch:1/250 || Epochiter: 270/403 || Iter: 270/100750 || Loc: 3.5868 Cla: 3.1880 Landm: 14.0563 || LR: 0.00100000 || Batchtime: 0.6273 s || ETA: 17:30:33\n",
            "Epoch:1/250 || Epochiter: 271/403 || Iter: 271/100750 || Loc: 3.0241 Cla: 3.1219 Landm: 13.9019 || LR: 0.00100000 || Batchtime: 2.9822 s || ETA: 3 days, 11:14:07\n",
            "Epoch:1/250 || Epochiter: 272/403 || Iter: 272/100750 || Loc: 3.2852 Cla: 3.1803 Landm: 13.8129 || LR: 0.00100000 || Batchtime: 0.4005 s || ETA: 11:10:39\n",
            "Epoch:1/250 || Epochiter: 273/403 || Iter: 273/100750 || Loc: 3.3953 Cla: 3.1810 Landm: 14.3617 || LR: 0.00100000 || Batchtime: 0.7384 s || ETA: 20:36:34\n",
            "Epoch:1/250 || Epochiter: 274/403 || Iter: 274/100750 || Loc: 2.7957 Cla: 3.1774 Landm: 12.7215 || LR: 0.00100000 || Batchtime: 0.8194 s || ETA: 22:52:08\n",
            "Epoch:1/250 || Epochiter: 275/403 || Iter: 275/100750 || Loc: 3.1573 Cla: 3.2179 Landm: 13.3275 || LR: 0.00100000 || Batchtime: 3.4016 s || ETA: 3 days, 22:56:23\n",
            "Epoch:1/250 || Epochiter: 276/403 || Iter: 276/100750 || Loc: 3.6135 Cla: 3.2501 Landm: 13.3039 || LR: 0.00100000 || Batchtime: 0.3931 s || ETA: 10:58:16\n",
            "Epoch:1/250 || Epochiter: 277/403 || Iter: 277/100750 || Loc: 3.4729 Cla: 3.2248 Landm: 14.7356 || LR: 0.00100000 || Batchtime: 0.4475 s || ETA: 12:29:22\n",
            "Epoch:1/250 || Epochiter: 278/403 || Iter: 278/100750 || Loc: 3.6114 Cla: 3.2344 Landm: 14.3050 || LR: 0.00100000 || Batchtime: 0.5512 s || ETA: 15:23:04\n",
            "Epoch:1/250 || Epochiter: 279/403 || Iter: 279/100750 || Loc: 3.3186 Cla: 3.1652 Landm: 14.2106 || LR: 0.00100000 || Batchtime: 2.3393 s || ETA: 2 days, 17:17:14\n",
            "Epoch:1/250 || Epochiter: 280/403 || Iter: 280/100750 || Loc: 3.4444 Cla: 3.1923 Landm: 15.1574 || LR: 0.00100000 || Batchtime: 0.5219 s || ETA: 14:33:52\n",
            "Epoch:1/250 || Epochiter: 281/403 || Iter: 281/100750 || Loc: 3.5771 Cla: 3.2041 Landm: 14.4166 || LR: 0.00100000 || Batchtime: 0.4600 s || ETA: 12:50:12\n",
            "Epoch:1/250 || Epochiter: 282/403 || Iter: 282/100750 || Loc: 3.1122 Cla: 3.1415 Landm: 13.1623 || LR: 0.00100000 || Batchtime: 0.4981 s || ETA: 13:54:06\n",
            "Epoch:1/250 || Epochiter: 283/403 || Iter: 283/100750 || Loc: 3.5873 Cla: 3.1961 Landm: 14.6152 || LR: 0.00100000 || Batchtime: 3.8747 s || ETA: 4 days, 12:07:59\n",
            "Epoch:1/250 || Epochiter: 284/403 || Iter: 284/100750 || Loc: 3.2348 Cla: 3.2299 Landm: 13.6035 || LR: 0.00100000 || Batchtime: 0.5781 s || ETA: 16:07:58\n",
            "Epoch:1/250 || Epochiter: 285/403 || Iter: 285/100750 || Loc: 3.0814 Cla: 3.1531 Landm: 12.4915 || LR: 0.00100000 || Batchtime: 0.5108 s || ETA: 14:15:13\n",
            "Epoch:1/250 || Epochiter: 286/403 || Iter: 286/100750 || Loc: 3.1607 Cla: 3.1461 Landm: 12.7627 || LR: 0.00100000 || Batchtime: 0.6183 s || ETA: 17:15:16\n",
            "Epoch:1/250 || Epochiter: 287/403 || Iter: 287/100750 || Loc: 2.9846 Cla: 3.1831 Landm: 13.4212 || LR: 0.00100000 || Batchtime: 2.9650 s || ETA: 3 days, 10:44:35\n",
            "Epoch:1/250 || Epochiter: 288/403 || Iter: 288/100750 || Loc: 3.1321 Cla: 3.2152 Landm: 13.0114 || LR: 0.00100000 || Batchtime: 0.4558 s || ETA: 12:43:07\n",
            "Epoch:1/250 || Epochiter: 289/403 || Iter: 289/100750 || Loc: 3.0691 Cla: 3.2264 Landm: 13.1334 || LR: 0.00100000 || Batchtime: 0.6206 s || ETA: 17:19:04\n",
            "Epoch:1/250 || Epochiter: 290/403 || Iter: 290/100750 || Loc: 3.1579 Cla: 3.2387 Landm: 13.3203 || LR: 0.00100000 || Batchtime: 0.5695 s || ETA: 15:53:29\n",
            "Epoch:1/250 || Epochiter: 291/403 || Iter: 291/100750 || Loc: 3.1281 Cla: 3.1868 Landm: 13.0798 || LR: 0.00100000 || Batchtime: 3.8719 s || ETA: 4 days, 12:02:55\n",
            "Epoch:1/250 || Epochiter: 292/403 || Iter: 292/100750 || Loc: 3.2191 Cla: 3.2005 Landm: 13.1140 || LR: 0.00100000 || Batchtime: 0.3736 s || ETA: 10:25:32\n",
            "Epoch:1/250 || Epochiter: 293/403 || Iter: 293/100750 || Loc: 3.3544 Cla: 3.1536 Landm: 12.8170 || LR: 0.00100000 || Batchtime: 0.4081 s || ETA: 11:23:12\n",
            "Epoch:1/250 || Epochiter: 294/403 || Iter: 294/100750 || Loc: 3.2589 Cla: 3.2300 Landm: 14.5013 || LR: 0.00100000 || Batchtime: 0.5656 s || ETA: 15:46:55\n",
            "Epoch:1/250 || Epochiter: 295/403 || Iter: 295/100750 || Loc: 3.3418 Cla: 3.1833 Landm: 14.6139 || LR: 0.00100000 || Batchtime: 2.1447 s || ETA: 2 days, 11:50:50\n",
            "Epoch:1/250 || Epochiter: 296/403 || Iter: 296/100750 || Loc: 2.7977 Cla: 3.1725 Landm: 12.4271 || LR: 0.00100000 || Batchtime: 0.4669 s || ETA: 13:01:43\n",
            "Epoch:1/250 || Epochiter: 297/403 || Iter: 297/100750 || Loc: 3.1091 Cla: 3.1365 Landm: 12.2194 || LR: 0.00100000 || Batchtime: 0.5598 s || ETA: 15:37:17\n",
            "Epoch:1/250 || Epochiter: 298/403 || Iter: 298/100750 || Loc: 3.2966 Cla: 3.2128 Landm: 13.7401 || LR: 0.00100000 || Batchtime: 0.5555 s || ETA: 15:30:05\n",
            "Epoch:1/250 || Epochiter: 299/403 || Iter: 299/100750 || Loc: 2.9747 Cla: 3.1785 Landm: 11.8348 || LR: 0.00100000 || Batchtime: 2.8978 s || ETA: 3 days, 8:51:25\n",
            "Epoch:1/250 || Epochiter: 300/403 || Iter: 300/100750 || Loc: 3.2882 Cla: 3.1570 Landm: 12.6152 || LR: 0.00100000 || Batchtime: 0.5351 s || ETA: 14:55:50\n",
            "Epoch:1/250 || Epochiter: 301/403 || Iter: 301/100750 || Loc: 2.8583 Cla: 3.1463 Landm: 11.9273 || LR: 0.00100000 || Batchtime: 0.4721 s || ETA: 13:10:21\n",
            "Epoch:1/250 || Epochiter: 302/403 || Iter: 302/100750 || Loc: 3.1361 Cla: 3.2247 Landm: 12.4488 || LR: 0.00100000 || Batchtime: 0.6389 s || ETA: 17:49:31\n",
            "Epoch:1/250 || Epochiter: 303/403 || Iter: 303/100750 || Loc: 2.7398 Cla: 3.1803 Landm: 10.7743 || LR: 0.00100000 || Batchtime: 2.8038 s || ETA: 3 days, 6:13:59\n",
            "Epoch:1/250 || Epochiter: 304/403 || Iter: 304/100750 || Loc: 2.6595 Cla: 3.1708 Landm: 11.8631 || LR: 0.00100000 || Batchtime: 0.6431 s || ETA: 17:56:32\n",
            "Epoch:1/250 || Epochiter: 305/403 || Iter: 305/100750 || Loc: 3.4609 Cla: 3.2197 Landm: 15.9924 || LR: 0.00100000 || Batchtime: 0.5628 s || ETA: 15:42:07\n",
            "Epoch:1/250 || Epochiter: 306/403 || Iter: 306/100750 || Loc: 3.1553 Cla: 3.2170 Landm: 12.6500 || LR: 0.00100000 || Batchtime: 0.6077 s || ETA: 16:57:21\n",
            "Epoch:1/250 || Epochiter: 307/403 || Iter: 307/100750 || Loc: 3.1114 Cla: 3.1366 Landm: 13.2143 || LR: 0.00100000 || Batchtime: 3.2682 s || ETA: 3 days, 19:11:10\n",
            "Epoch:1/250 || Epochiter: 308/403 || Iter: 308/100750 || Loc: 3.4099 Cla: 3.1606 Landm: 12.7457 || LR: 0.00100000 || Batchtime: 0.6036 s || ETA: 16:50:23\n",
            "Epoch:1/250 || Epochiter: 309/403 || Iter: 309/100750 || Loc: 3.2316 Cla: 3.1927 Landm: 12.7093 || LR: 0.00100000 || Batchtime: 0.6712 s || ETA: 18:43:37\n",
            "Epoch:1/250 || Epochiter: 310/403 || Iter: 310/100750 || Loc: 3.4248 Cla: 3.1544 Landm: 13.5071 || LR: 0.00100000 || Batchtime: 0.7781 s || ETA: 21:42:30\n",
            "Epoch:1/250 || Epochiter: 311/403 || Iter: 311/100750 || Loc: 3.2564 Cla: 3.1656 Landm: 13.4995 || LR: 0.00100000 || Batchtime: 2.6457 s || ETA: 3 days, 1:48:50\n",
            "Epoch:1/250 || Epochiter: 312/403 || Iter: 312/100750 || Loc: 2.7527 Cla: 3.1576 Landm: 12.2067 || LR: 0.00100000 || Batchtime: 0.5618 s || ETA: 15:40:29\n",
            "Epoch:1/250 || Epochiter: 313/403 || Iter: 313/100750 || Loc: 2.7848 Cla: 3.1569 Landm: 14.2210 || LR: 0.00100000 || Batchtime: 0.5167 s || ETA: 14:25:01\n",
            "Epoch:1/250 || Epochiter: 314/403 || Iter: 314/100750 || Loc: 3.2062 Cla: 3.2018 Landm: 14.5842 || LR: 0.00100000 || Batchtime: 0.6613 s || ETA: 18:27:00\n",
            "Epoch:1/250 || Epochiter: 315/403 || Iter: 315/100750 || Loc: 3.3622 Cla: 3.2340 Landm: 13.8693 || LR: 0.00100000 || Batchtime: 2.6400 s || ETA: 3 days, 1:39:06\n",
            "Epoch:1/250 || Epochiter: 316/403 || Iter: 316/100750 || Loc: 2.9166 Cla: 3.1385 Landm: 12.3232 || LR: 0.00100000 || Batchtime: 0.5530 s || ETA: 15:25:36\n",
            "Epoch:1/250 || Epochiter: 317/403 || Iter: 317/100750 || Loc: 3.1528 Cla: 3.1185 Landm: 14.6140 || LR: 0.00100000 || Batchtime: 0.7571 s || ETA: 21:07:15\n",
            "Epoch:1/250 || Epochiter: 318/403 || Iter: 318/100750 || Loc: 3.1487 Cla: 3.1795 Landm: 12.9169 || LR: 0.00100000 || Batchtime: 0.6023 s || ETA: 16:48:14\n",
            "Epoch:1/250 || Epochiter: 319/403 || Iter: 319/100750 || Loc: 3.5240 Cla: 3.1939 Landm: 13.9737 || LR: 0.00100000 || Batchtime: 2.7142 s || ETA: 3 days, 3:43:08\n",
            "Epoch:1/250 || Epochiter: 320/403 || Iter: 320/100750 || Loc: 3.1042 Cla: 3.1674 Landm: 13.0524 || LR: 0.00100000 || Batchtime: 0.5494 s || ETA: 15:19:36\n",
            "Epoch:1/250 || Epochiter: 321/403 || Iter: 321/100750 || Loc: 3.0919 Cla: 3.1366 Landm: 12.5237 || LR: 0.00100000 || Batchtime: 0.5605 s || ETA: 15:38:13\n",
            "Epoch:1/250 || Epochiter: 322/403 || Iter: 322/100750 || Loc: 2.8907 Cla: 3.1778 Landm: 11.1845 || LR: 0.00100000 || Batchtime: 0.5757 s || ETA: 16:03:34\n",
            "Epoch:1/250 || Epochiter: 323/403 || Iter: 323/100750 || Loc: 2.7201 Cla: 3.1558 Landm: 11.3351 || LR: 0.00100000 || Batchtime: 2.5420 s || ETA: 2 days, 22:54:48\n",
            "Epoch:1/250 || Epochiter: 324/403 || Iter: 324/100750 || Loc: 3.3165 Cla: 3.1401 Landm: 13.4909 || LR: 0.00100000 || Batchtime: 0.7453 s || ETA: 20:47:30\n",
            "Epoch:1/250 || Epochiter: 325/403 || Iter: 325/100750 || Loc: 3.0354 Cla: 3.1895 Landm: 13.1778 || LR: 0.00100000 || Batchtime: 0.6765 s || ETA: 18:52:17\n",
            "Epoch:1/250 || Epochiter: 326/403 || Iter: 326/100750 || Loc: 3.3414 Cla: 3.1497 Landm: 14.6986 || LR: 0.00100000 || Batchtime: 0.6377 s || ETA: 17:47:19\n",
            "Epoch:1/250 || Epochiter: 327/403 || Iter: 327/100750 || Loc: 2.8154 Cla: 3.1532 Landm: 12.0878 || LR: 0.00100000 || Batchtime: 1.9981 s || ETA: 2 days, 7:44:18\n",
            "Epoch:1/250 || Epochiter: 328/403 || Iter: 328/100750 || Loc: 2.5487 Cla: 3.1111 Landm: 10.8173 || LR: 0.00100000 || Batchtime: 0.3521 s || ETA: 9:49:22\n",
            "Epoch:1/250 || Epochiter: 329/403 || Iter: 329/100750 || Loc: 3.2214 Cla: 3.1095 Landm: 12.0659 || LR: 0.00100000 || Batchtime: 0.4245 s || ETA: 11:50:26\n",
            "Epoch:1/250 || Epochiter: 330/403 || Iter: 330/100750 || Loc: 2.8536 Cla: 3.1113 Landm: 11.4912 || LR: 0.00100000 || Batchtime: 0.6115 s || ETA: 17:03:23\n",
            "Epoch:1/250 || Epochiter: 331/403 || Iter: 331/100750 || Loc: 3.1889 Cla: 3.1504 Landm: 13.1197 || LR: 0.00100000 || Batchtime: 3.1262 s || ETA: 3 days, 15:12:09\n",
            "Epoch:1/250 || Epochiter: 332/403 || Iter: 332/100750 || Loc: 3.3817 Cla: 3.1580 Landm: 12.8315 || LR: 0.00100000 || Batchtime: 0.5828 s || ETA: 16:15:21\n",
            "Epoch:1/250 || Epochiter: 333/403 || Iter: 333/100750 || Loc: 3.0381 Cla: 3.1850 Landm: 13.5273 || LR: 0.00100000 || Batchtime: 0.6065 s || ETA: 16:54:59\n",
            "Epoch:1/250 || Epochiter: 334/403 || Iter: 334/100750 || Loc: 3.2858 Cla: 3.2297 Landm: 13.2682 || LR: 0.00100000 || Batchtime: 0.7217 s || ETA: 20:07:50\n",
            "Epoch:1/250 || Epochiter: 335/403 || Iter: 335/100750 || Loc: 3.0987 Cla: 3.1886 Landm: 12.9580 || LR: 0.00100000 || Batchtime: 3.5212 s || ETA: 4 days, 2:13:01\n",
            "Epoch:1/250 || Epochiter: 336/403 || Iter: 336/100750 || Loc: 3.2108 Cla: 3.1796 Landm: 13.3727 || LR: 0.00100000 || Batchtime: 0.6029 s || ETA: 16:49:03\n",
            "Epoch:1/250 || Epochiter: 337/403 || Iter: 337/100750 || Loc: 3.1549 Cla: 3.1394 Landm: 13.5634 || LR: 0.00100000 || Batchtime: 0.5900 s || ETA: 16:27:27\n",
            "Epoch:1/250 || Epochiter: 338/403 || Iter: 338/100750 || Loc: 3.1538 Cla: 3.1663 Landm: 13.1773 || LR: 0.00100000 || Batchtime: 0.6875 s || ETA: 19:10:34\n",
            "Epoch:1/250 || Epochiter: 339/403 || Iter: 339/100750 || Loc: 2.8842 Cla: 3.1001 Landm: 11.7410 || LR: 0.00100000 || Batchtime: 2.8719 s || ETA: 3 days, 8:06:15\n",
            "Epoch:1/250 || Epochiter: 340/403 || Iter: 340/100750 || Loc: 3.3658 Cla: 3.2344 Landm: 12.7747 || LR: 0.00100000 || Batchtime: 0.5197 s || ETA: 14:29:44\n",
            "Epoch:1/250 || Epochiter: 341/403 || Iter: 341/100750 || Loc: 3.0648 Cla: 3.1284 Landm: 12.9564 || LR: 0.00100000 || Batchtime: 0.5445 s || ETA: 15:11:09\n",
            "Epoch:1/250 || Epochiter: 342/403 || Iter: 342/100750 || Loc: 2.8054 Cla: 3.1155 Landm: 11.8691 || LR: 0.00100000 || Batchtime: 0.5734 s || ETA: 15:59:30\n",
            "Epoch:1/250 || Epochiter: 343/403 || Iter: 343/100750 || Loc: 2.9629 Cla: 3.1388 Landm: 12.9830 || LR: 0.00100000 || Batchtime: 2.6029 s || ETA: 3 days, 0:35:50\n",
            "Epoch:1/250 || Epochiter: 344/403 || Iter: 344/100750 || Loc: 2.9110 Cla: 3.1267 Landm: 11.4719 || LR: 0.00100000 || Batchtime: 0.6962 s || ETA: 19:24:59\n",
            "Epoch:1/250 || Epochiter: 345/403 || Iter: 345/100750 || Loc: 3.2414 Cla: 3.2165 Landm: 14.1494 || LR: 0.00100000 || Batchtime: 0.6246 s || ETA: 17:25:15\n",
            "Epoch:1/250 || Epochiter: 346/403 || Iter: 346/100750 || Loc: 3.2931 Cla: 3.2159 Landm: 13.9952 || LR: 0.00100000 || Batchtime: 0.6858 s || ETA: 19:07:38\n",
            "Epoch:1/250 || Epochiter: 347/403 || Iter: 347/100750 || Loc: 3.0240 Cla: 3.1435 Landm: 11.6288 || LR: 0.00100000 || Batchtime: 3.1943 s || ETA: 3 days, 17:05:16\n",
            "Epoch:1/250 || Epochiter: 348/403 || Iter: 348/100750 || Loc: 3.3104 Cla: 3.1699 Landm: 12.7954 || LR: 0.00100000 || Batchtime: 0.4775 s || ETA: 13:18:58\n",
            "Epoch:1/250 || Epochiter: 349/403 || Iter: 349/100750 || Loc: 3.1055 Cla: 3.1716 Landm: 12.2874 || LR: 0.00100000 || Batchtime: 0.6284 s || ETA: 17:31:33\n",
            "Epoch:1/250 || Epochiter: 350/403 || Iter: 350/100750 || Loc: 3.1336 Cla: 3.1562 Landm: 12.0801 || LR: 0.00100000 || Batchtime: 0.6217 s || ETA: 17:20:17\n",
            "Epoch:1/250 || Epochiter: 351/403 || Iter: 351/100750 || Loc: 3.4477 Cla: 3.1170 Landm: 13.9929 || LR: 0.00100000 || Batchtime: 1.6886 s || ETA: 1 day, 23:05:38\n",
            "Epoch:1/250 || Epochiter: 352/403 || Iter: 352/100750 || Loc: 2.5955 Cla: 3.0590 Landm: 11.2069 || LR: 0.00100000 || Batchtime: 0.6262 s || ETA: 17:27:54\n",
            "Epoch:1/250 || Epochiter: 353/403 || Iter: 353/100750 || Loc: 3.0310 Cla: 3.1500 Landm: 12.7865 || LR: 0.00100000 || Batchtime: 0.4878 s || ETA: 13:36:10\n",
            "Epoch:1/250 || Epochiter: 354/403 || Iter: 354/100750 || Loc: 2.5656 Cla: 3.0766 Landm: 11.0928 || LR: 0.00100000 || Batchtime: 0.7608 s || ETA: 21:12:58\n",
            "Epoch:1/250 || Epochiter: 355/403 || Iter: 355/100750 || Loc: 2.5341 Cla: 3.0304 Landm: 10.8626 || LR: 0.00100000 || Batchtime: 3.6138 s || ETA: 4 days, 4:46:53\n",
            "Epoch:1/250 || Epochiter: 356/403 || Iter: 356/100750 || Loc: 2.9275 Cla: 3.1281 Landm: 12.7075 || LR: 0.00100000 || Batchtime: 0.4240 s || ETA: 11:49:26\n",
            "Epoch:1/250 || Epochiter: 357/403 || Iter: 357/100750 || Loc: 3.3139 Cla: 3.1752 Landm: 14.2988 || LR: 0.00100000 || Batchtime: 0.5941 s || ETA: 16:34:06\n",
            "Epoch:1/250 || Epochiter: 358/403 || Iter: 358/100750 || Loc: 2.8747 Cla: 3.1880 Landm: 12.6926 || LR: 0.00100000 || Batchtime: 0.6104 s || ETA: 17:01:17\n",
            "Epoch:1/250 || Epochiter: 359/403 || Iter: 359/100750 || Loc: 3.1453 Cla: 3.2240 Landm: 12.8141 || LR: 0.00100000 || Batchtime: 2.1649 s || ETA: 2 days, 12:22:15\n",
            "Epoch:1/250 || Epochiter: 360/403 || Iter: 360/100750 || Loc: 3.3035 Cla: 3.1574 Landm: 13.6640 || LR: 0.00100000 || Batchtime: 0.7093 s || ETA: 19:46:49\n",
            "Epoch:1/250 || Epochiter: 361/403 || Iter: 361/100750 || Loc: 3.0172 Cla: 3.1716 Landm: 13.4048 || LR: 0.00100000 || Batchtime: 0.7667 s || ETA: 21:22:45\n",
            "Epoch:1/250 || Epochiter: 362/403 || Iter: 362/100750 || Loc: 2.8020 Cla: 3.0662 Landm: 11.5378 || LR: 0.00100000 || Batchtime: 0.8207 s || ETA: 22:53:05\n",
            "Epoch:1/250 || Epochiter: 363/403 || Iter: 363/100750 || Loc: 2.7085 Cla: 3.0730 Landm: 11.4967 || LR: 0.00100000 || Batchtime: 2.2023 s || ETA: 2 days, 13:24:41\n",
            "Epoch:1/250 || Epochiter: 364/403 || Iter: 364/100750 || Loc: 3.0111 Cla: 3.1689 Landm: 12.7953 || LR: 0.00100000 || Batchtime: 0.5942 s || ETA: 16:34:06\n",
            "Epoch:1/250 || Epochiter: 365/403 || Iter: 365/100750 || Loc: 3.3185 Cla: 3.2235 Landm: 13.1871 || LR: 0.00100000 || Batchtime: 0.6651 s || ETA: 18:32:49\n",
            "Epoch:1/250 || Epochiter: 366/403 || Iter: 366/100750 || Loc: 2.9761 Cla: 3.1145 Landm: 13.6757 || LR: 0.00100000 || Batchtime: 0.5966 s || ETA: 16:38:05\n",
            "Epoch:1/250 || Epochiter: 367/403 || Iter: 367/100750 || Loc: 2.8962 Cla: 3.0852 Landm: 11.8526 || LR: 0.00100000 || Batchtime: 2.3151 s || ETA: 2 days, 16:33:22\n",
            "Epoch:1/250 || Epochiter: 368/403 || Iter: 368/100750 || Loc: 3.0645 Cla: 3.1392 Landm: 11.9956 || LR: 0.00100000 || Batchtime: 0.5439 s || ETA: 15:09:54\n",
            "Epoch:1/250 || Epochiter: 369/403 || Iter: 369/100750 || Loc: 2.8035 Cla: 3.0956 Landm: 12.0665 || LR: 0.00100000 || Batchtime: 0.5722 s || ETA: 15:57:14\n",
            "Epoch:1/250 || Epochiter: 370/403 || Iter: 370/100750 || Loc: 2.4197 Cla: 3.0650 Landm: 11.2612 || LR: 0.00100000 || Batchtime: 0.6532 s || ETA: 18:12:47\n",
            "Epoch:1/250 || Epochiter: 371/403 || Iter: 371/100750 || Loc: 3.0538 Cla: 3.1496 Landm: 12.8257 || LR: 0.00100000 || Batchtime: 2.8110 s || ETA: 3 days, 6:22:50\n",
            "Epoch:1/250 || Epochiter: 372/403 || Iter: 372/100750 || Loc: 2.8330 Cla: 3.0731 Landm: 12.0066 || LR: 0.00100000 || Batchtime: 0.5535 s || ETA: 15:25:54\n",
            "Epoch:1/250 || Epochiter: 373/403 || Iter: 373/100750 || Loc: 2.7784 Cla: 3.1023 Landm: 12.0212 || LR: 0.00100000 || Batchtime: 0.6148 s || ETA: 17:08:35\n",
            "Epoch:1/250 || Epochiter: 374/403 || Iter: 374/100750 || Loc: 2.9351 Cla: 3.1187 Landm: 12.0344 || LR: 0.00100000 || Batchtime: 0.5397 s || ETA: 15:02:51\n",
            "Epoch:1/250 || Epochiter: 375/403 || Iter: 375/100750 || Loc: 2.8665 Cla: 3.0665 Landm: 12.3067 || LR: 0.00100000 || Batchtime: 3.1307 s || ETA: 3 days, 15:17:28\n",
            "Epoch:1/250 || Epochiter: 376/403 || Iter: 376/100750 || Loc: 2.5280 Cla: 3.0867 Landm: 11.3803 || LR: 0.00100000 || Batchtime: 0.4428 s || ETA: 12:20:46\n",
            "Epoch:1/250 || Epochiter: 377/403 || Iter: 377/100750 || Loc: 2.9109 Cla: 3.1110 Landm: 11.8677 || LR: 0.00100000 || Batchtime: 0.5472 s || ETA: 15:15:27\n",
            "Epoch:1/250 || Epochiter: 378/403 || Iter: 378/100750 || Loc: 2.7694 Cla: 3.0509 Landm: 12.1464 || LR: 0.00100000 || Batchtime: 0.7579 s || ETA: 21:07:49\n",
            "Epoch:1/250 || Epochiter: 379/403 || Iter: 379/100750 || Loc: 3.0172 Cla: 3.0829 Landm: 12.1382 || LR: 0.00100000 || Batchtime: 2.5678 s || ETA: 2 days, 23:35:34\n",
            "Epoch:1/250 || Epochiter: 380/403 || Iter: 380/100750 || Loc: 3.0492 Cla: 3.2045 Landm: 12.5872 || LR: 0.00100000 || Batchtime: 0.4335 s || ETA: 12:05:06\n",
            "Epoch:1/250 || Epochiter: 381/403 || Iter: 381/100750 || Loc: 3.3442 Cla: 3.1528 Landm: 13.2626 || LR: 0.00100000 || Batchtime: 0.5641 s || ETA: 15:43:43\n",
            "Epoch:1/250 || Epochiter: 382/403 || Iter: 382/100750 || Loc: 2.5238 Cla: 3.1073 Landm: 10.7653 || LR: 0.00100000 || Batchtime: 0.7401 s || ETA: 20:38:03\n",
            "Epoch:1/250 || Epochiter: 383/403 || Iter: 383/100750 || Loc: 2.5443 Cla: 3.0587 Landm: 11.1075 || LR: 0.00100000 || Batchtime: 3.7498 s || ETA: 4 days, 8:32:35\n",
            "Epoch:1/250 || Epochiter: 384/403 || Iter: 384/100750 || Loc: 2.5817 Cla: 3.1197 Landm: 10.3306 || LR: 0.00100000 || Batchtime: 0.4211 s || ETA: 11:44:22\n",
            "Epoch:1/250 || Epochiter: 385/403 || Iter: 385/100750 || Loc: 2.6699 Cla: 3.0776 Landm: 10.8252 || LR: 0.00100000 || Batchtime: 0.4138 s || ETA: 11:32:13\n",
            "Epoch:1/250 || Epochiter: 386/403 || Iter: 386/100750 || Loc: 2.8315 Cla: 3.1689 Landm: 11.7440 || LR: 0.00100000 || Batchtime: 0.4921 s || ETA: 13:43:08\n",
            "Epoch:1/250 || Epochiter: 387/403 || Iter: 387/100750 || Loc: 2.2998 Cla: 2.9734 Landm: 9.9581 || LR: 0.00100000 || Batchtime: 3.5568 s || ETA: 4 days, 3:09:39\n",
            "Epoch:1/250 || Epochiter: 388/403 || Iter: 388/100750 || Loc: 2.7836 Cla: 3.1235 Landm: 10.9699 || LR: 0.00100000 || Batchtime: 0.4346 s || ETA: 12:06:54\n",
            "Epoch:1/250 || Epochiter: 389/403 || Iter: 389/100750 || Loc: 2.8297 Cla: 3.1189 Landm: 11.1589 || LR: 0.00100000 || Batchtime: 0.5831 s || ETA: 16:15:22\n",
            "Epoch:1/250 || Epochiter: 390/403 || Iter: 390/100750 || Loc: 2.5048 Cla: 3.0362 Landm: 11.1311 || LR: 0.00100000 || Batchtime: 0.5979 s || ETA: 16:40:09\n",
            "Epoch:1/250 || Epochiter: 391/403 || Iter: 391/100750 || Loc: 3.0442 Cla: 3.1252 Landm: 10.9836 || LR: 0.00100000 || Batchtime: 3.5496 s || ETA: 4 days, 2:57:15\n",
            "Epoch:1/250 || Epochiter: 392/403 || Iter: 392/100750 || Loc: 2.9541 Cla: 3.1356 Landm: 10.9450 || LR: 0.00100000 || Batchtime: 0.6599 s || ETA: 18:23:44\n",
            "Epoch:1/250 || Epochiter: 393/403 || Iter: 393/100750 || Loc: 2.6264 Cla: 3.0391 Landm: 11.4170 || LR: 0.00100000 || Batchtime: 0.7140 s || ETA: 19:54:15\n",
            "Epoch:1/250 || Epochiter: 394/403 || Iter: 394/100750 || Loc: 3.1400 Cla: 3.1430 Landm: 12.5737 || LR: 0.00100000 || Batchtime: 0.8675 s || ETA: 1 day, 0:11:02\n",
            "Epoch:1/250 || Epochiter: 395/403 || Iter: 395/100750 || Loc: 2.8627 Cla: 3.1056 Landm: 12.5543 || LR: 0.00100000 || Batchtime: 2.5428 s || ETA: 2 days, 22:53:03\n",
            "Epoch:1/250 || Epochiter: 396/403 || Iter: 396/100750 || Loc: 3.3549 Cla: 3.1708 Landm: 13.6657 || LR: 0.00100000 || Batchtime: 0.3527 s || ETA: 9:49:56\n",
            "Epoch:1/250 || Epochiter: 397/403 || Iter: 397/100750 || Loc: 2.5423 Cla: 3.0104 Landm: 10.3069 || LR: 0.00100000 || Batchtime: 0.2890 s || ETA: 8:03:22\n",
            "Epoch:1/250 || Epochiter: 398/403 || Iter: 398/100750 || Loc: 3.3018 Cla: 3.1934 Landm: 13.6670 || LR: 0.00100000 || Batchtime: 0.4506 s || ETA: 12:33:35\n",
            "Epoch:1/250 || Epochiter: 399/403 || Iter: 399/100750 || Loc: 2.9529 Cla: 3.1105 Landm: 12.4779 || LR: 0.00100000 || Batchtime: 1.1329 s || ETA: 1 day, 7:34:47\n",
            "Epoch:1/250 || Epochiter: 400/403 || Iter: 400/100750 || Loc: 2.6196 Cla: 3.0842 Landm: 10.1978 || LR: 0.00100000 || Batchtime: 0.2752 s || ETA: 7:40:16\n",
            "Epoch:1/250 || Epochiter: 401/403 || Iter: 401/100750 || Loc: 2.8135 Cla: 3.0827 Landm: 11.7370 || LR: 0.00100000 || Batchtime: 0.2649 s || ETA: 7:22:58\n",
            "Epoch:1/250 || Epochiter: 402/403 || Iter: 402/100750 || Loc: 2.8924 Cla: 3.0617 Landm: 12.4981 || LR: 0.00100000 || Batchtime: 0.2083 s || ETA: 5:48:25\n",
            "Epoch:1/250 || Epochiter: 403/403 || Iter: 403/100750 || Loc: 3.2132 Cla: 3.1945 Landm: 12.5002 || LR: 0.00100000 || Batchtime: 0.9960 s || ETA: 1 day, 3:45:42\n",
            "Epoch:2/250 || Epochiter: 1/403 || Iter: 404/100750 || Loc: 3.0041 Cla: 3.0682 Landm: 11.8242 || LR: 0.00100000 || Batchtime: 5.1569 s || ETA: 5 days, 23:44:34\n",
            "Epoch:2/250 || Epochiter: 2/403 || Iter: 405/100750 || Loc: 2.7561 Cla: 3.0775 Landm: 10.6590 || LR: 0.00100000 || Batchtime: 0.9801 s || ETA: 1 day, 3:19:04\n",
            "Epoch:2/250 || Epochiter: 3/403 || Iter: 406/100750 || Loc: 2.7797 Cla: 3.0024 Landm: 10.2052 || LR: 0.00100000 || Batchtime: 0.7034 s || ETA: 19:36:21\n",
            "Epoch:2/250 || Epochiter: 4/403 || Iter: 407/100750 || Loc: 2.7400 Cla: 3.0741 Landm: 11.3112 || LR: 0.00100000 || Batchtime: 0.6262 s || ETA: 17:27:12\n",
            "Epoch:2/250 || Epochiter: 5/403 || Iter: 408/100750 || Loc: 3.1494 Cla: 3.0771 Landm: 10.5633 || LR: 0.00100000 || Batchtime: 2.9058 s || ETA: 3 days, 8:59:33\n",
            "Epoch:2/250 || Epochiter: 6/403 || Iter: 409/100750 || Loc: 2.6433 Cla: 3.0592 Landm: 11.3069 || LR: 0.00100000 || Batchtime: 0.5744 s || ETA: 16:00:41\n",
            "Epoch:2/250 || Epochiter: 7/403 || Iter: 410/100750 || Loc: 3.0155 Cla: 3.1347 Landm: 11.5304 || LR: 0.00100000 || Batchtime: 0.5510 s || ETA: 15:21:30\n",
            "Epoch:2/250 || Epochiter: 8/403 || Iter: 411/100750 || Loc: 3.2293 Cla: 3.1834 Landm: 12.9249 || LR: 0.00100000 || Batchtime: 0.7095 s || ETA: 19:46:30\n",
            "Epoch:2/250 || Epochiter: 9/403 || Iter: 412/100750 || Loc: 2.7382 Cla: 3.1417 Landm: 11.0400 || LR: 0.00100000 || Batchtime: 2.9928 s || ETA: 3 days, 11:24:59\n",
            "Epoch:2/250 || Epochiter: 10/403 || Iter: 413/100750 || Loc: 2.7617 Cla: 3.0615 Landm: 11.5336 || LR: 0.00100000 || Batchtime: 0.6046 s || ETA: 16:51:00\n",
            "Epoch:2/250 || Epochiter: 11/403 || Iter: 414/100750 || Loc: 2.6134 Cla: 3.0838 Landm: 10.7905 || LR: 0.00100000 || Batchtime: 0.6152 s || ETA: 17:08:51\n",
            "Epoch:2/250 || Epochiter: 12/403 || Iter: 415/100750 || Loc: 3.2245 Cla: 3.1208 Landm: 11.8416 || LR: 0.00100000 || Batchtime: 0.5482 s || ETA: 15:16:43\n",
            "Epoch:2/250 || Epochiter: 13/403 || Iter: 416/100750 || Loc: 2.7896 Cla: 3.0163 Landm: 10.6246 || LR: 0.00100000 || Batchtime: 3.1169 s || ETA: 3 days, 14:52:10\n",
            "Epoch:2/250 || Epochiter: 14/403 || Iter: 417/100750 || Loc: 2.2085 Cla: 2.8822 Landm: 9.1888 || LR: 0.00100000 || Batchtime: 0.5528 s || ETA: 15:24:25\n",
            "Epoch:2/250 || Epochiter: 15/403 || Iter: 418/100750 || Loc: 2.6926 Cla: 3.0173 Landm: 10.0696 || LR: 0.00100000 || Batchtime: 0.5350 s || ETA: 14:54:41\n",
            "Epoch:2/250 || Epochiter: 16/403 || Iter: 419/100750 || Loc: 2.8084 Cla: 3.1285 Landm: 12.1940 || LR: 0.00100000 || Batchtime: 0.5876 s || ETA: 16:22:34\n",
            "Epoch:2/250 || Epochiter: 17/403 || Iter: 420/100750 || Loc: 2.7586 Cla: 3.2046 Landm: 11.9882 || LR: 0.00100000 || Batchtime: 2.9214 s || ETA: 3 days, 9:25:02\n",
            "Epoch:2/250 || Epochiter: 18/403 || Iter: 421/100750 || Loc: 2.2469 Cla: 2.9200 Landm: 9.7767 || LR: 0.00100000 || Batchtime: 0.6257 s || ETA: 17:26:19\n",
            "Epoch:2/250 || Epochiter: 19/403 || Iter: 422/100750 || Loc: 3.1306 Cla: 3.0546 Landm: 11.4545 || LR: 0.00100000 || Batchtime: 0.5761 s || ETA: 16:03:18\n",
            "Epoch:2/250 || Epochiter: 20/403 || Iter: 423/100750 || Loc: 2.6497 Cla: 3.0374 Landm: 11.4849 || LR: 0.00100000 || Batchtime: 0.5864 s || ETA: 16:20:35\n",
            "Epoch:2/250 || Epochiter: 21/403 || Iter: 424/100750 || Loc: 2.7787 Cla: 3.2045 Landm: 12.4454 || LR: 0.00100000 || Batchtime: 3.1420 s || ETA: 3 days, 15:33:45\n",
            "Epoch:2/250 || Epochiter: 22/403 || Iter: 425/100750 || Loc: 2.4465 Cla: 3.0325 Landm: 9.8509 || LR: 0.00100000 || Batchtime: 0.6609 s || ETA: 18:25:09\n",
            "Epoch:2/250 || Epochiter: 23/403 || Iter: 426/100750 || Loc: 3.2181 Cla: 3.1218 Landm: 10.4477 || LR: 0.00100000 || Batchtime: 0.7160 s || ETA: 19:57:10\n",
            "Epoch:2/250 || Epochiter: 24/403 || Iter: 427/100750 || Loc: 2.5614 Cla: 2.9647 Landm: 10.1012 || LR: 0.00100000 || Batchtime: 0.5718 s || ETA: 15:56:07\n",
            "Epoch:2/250 || Epochiter: 25/403 || Iter: 428/100750 || Loc: 2.7697 Cla: 3.0765 Landm: 10.6547 || LR: 0.00100000 || Batchtime: 2.6963 s || ETA: 3 days, 3:08:23\n",
            "Epoch:2/250 || Epochiter: 26/403 || Iter: 429/100750 || Loc: 2.0934 Cla: 2.9721 Landm: 10.3812 || LR: 0.00100000 || Batchtime: 0.5667 s || ETA: 15:47:29\n",
            "Epoch:2/250 || Epochiter: 27/403 || Iter: 430/100750 || Loc: 2.6536 Cla: 3.0060 Landm: 11.3377 || LR: 0.00100000 || Batchtime: 0.4376 s || ETA: 12:11:43\n",
            "Epoch:2/250 || Epochiter: 28/403 || Iter: 431/100750 || Loc: 2.6107 Cla: 3.0714 Landm: 11.1327 || LR: 0.00100000 || Batchtime: 0.5684 s || ETA: 15:50:23\n",
            "Epoch:2/250 || Epochiter: 29/403 || Iter: 432/100750 || Loc: 2.5390 Cla: 3.0353 Landm: 12.2549 || LR: 0.00100000 || Batchtime: 3.0106 s || ETA: 3 days, 11:53:41\n",
            "Epoch:2/250 || Epochiter: 30/403 || Iter: 433/100750 || Loc: 2.7314 Cla: 3.0948 Landm: 10.4028 || LR: 0.00100000 || Batchtime: 0.5394 s || ETA: 15:01:49\n",
            "Epoch:2/250 || Epochiter: 31/403 || Iter: 434/100750 || Loc: 2.6958 Cla: 2.9517 Landm: 10.4886 || LR: 0.00100000 || Batchtime: 0.5212 s || ETA: 14:31:23\n",
            "Epoch:2/250 || Epochiter: 32/403 || Iter: 435/100750 || Loc: 2.7593 Cla: 3.0924 Landm: 11.1777 || LR: 0.00100000 || Batchtime: 0.6104 s || ETA: 17:00:30\n",
            "Epoch:2/250 || Epochiter: 33/403 || Iter: 436/100750 || Loc: 2.3805 Cla: 3.0237 Landm: 10.4355 || LR: 0.00100000 || Batchtime: 2.1999 s || ETA: 2 days, 13:17:58\n",
            "Epoch:2/250 || Epochiter: 34/403 || Iter: 437/100750 || Loc: 2.5999 Cla: 2.9496 Landm: 10.1686 || LR: 0.00100000 || Batchtime: 0.6616 s || ETA: 18:26:10\n",
            "Epoch:2/250 || Epochiter: 35/403 || Iter: 438/100750 || Loc: 2.5926 Cla: 3.0461 Landm: 10.8111 || LR: 0.00100000 || Batchtime: 0.7596 s || ETA: 21:09:54\n",
            "Epoch:2/250 || Epochiter: 36/403 || Iter: 439/100750 || Loc: 2.5689 Cla: 3.0324 Landm: 9.2753 || LR: 0.00100000 || Batchtime: 0.5171 s || ETA: 14:24:26\n",
            "Epoch:2/250 || Epochiter: 37/403 || Iter: 440/100750 || Loc: 2.3512 Cla: 2.9598 Landm: 9.5663 || LR: 0.00100000 || Batchtime: 2.8325 s || ETA: 3 days, 6:55:30\n",
            "Epoch:2/250 || Epochiter: 38/403 || Iter: 441/100750 || Loc: 2.9136 Cla: 3.0825 Landm: 10.4264 || LR: 0.00100000 || Batchtime: 0.6978 s || ETA: 19:26:35\n",
            "Epoch:2/250 || Epochiter: 39/403 || Iter: 442/100750 || Loc: 2.8966 Cla: 3.0239 Landm: 11.6777 || LR: 0.00100000 || Batchtime: 1.1872 s || ETA: 1 day, 9:04:48\n",
            "Epoch:2/250 || Epochiter: 40/403 || Iter: 443/100750 || Loc: 2.7757 Cla: 3.0192 Landm: 10.9594 || LR: 0.00100000 || Batchtime: 0.5254 s || ETA: 14:38:18\n",
            "Epoch:2/250 || Epochiter: 41/403 || Iter: 444/100750 || Loc: 2.8572 Cla: 3.0083 Landm: 11.6136 || LR: 0.00100000 || Batchtime: 1.2167 s || ETA: 1 day, 9:53:59\n",
            "Epoch:2/250 || Epochiter: 42/403 || Iter: 445/100750 || Loc: 2.8146 Cla: 3.0379 Landm: 11.1108 || LR: 0.00100000 || Batchtime: 0.6767 s || ETA: 18:51:20\n",
            "Epoch:2/250 || Epochiter: 43/403 || Iter: 446/100750 || Loc: 2.4326 Cla: 2.9860 Landm: 10.1233 || LR: 0.00100000 || Batchtime: 1.9789 s || ETA: 2 days, 7:08:10\n",
            "Epoch:2/250 || Epochiter: 44/403 || Iter: 447/100750 || Loc: 2.8008 Cla: 3.0519 Landm: 10.5603 || LR: 0.00100000 || Batchtime: 0.5744 s || ETA: 16:00:16\n",
            "Epoch:2/250 || Epochiter: 45/403 || Iter: 448/100750 || Loc: 3.0177 Cla: 3.0640 Landm: 12.4434 || LR: 0.00100000 || Batchtime: 0.7112 s || ETA: 19:48:50\n",
            "Epoch:2/250 || Epochiter: 46/403 || Iter: 449/100750 || Loc: 2.7991 Cla: 3.0314 Landm: 12.7473 || LR: 0.00100000 || Batchtime: 0.6142 s || ETA: 17:06:46\n",
            "Epoch:2/250 || Epochiter: 47/403 || Iter: 450/100750 || Loc: 2.4008 Cla: 2.9486 Landm: 10.1862 || LR: 0.00100000 || Batchtime: 2.5211 s || ETA: 2 days, 22:14:27\n",
            "Epoch:2/250 || Epochiter: 48/403 || Iter: 451/100750 || Loc: 2.5686 Cla: 2.9993 Landm: 9.5918 || LR: 0.00100000 || Batchtime: 0.7068 s || ETA: 19:41:34\n",
            "Epoch:2/250 || Epochiter: 49/403 || Iter: 452/100750 || Loc: 2.5038 Cla: 3.0173 Landm: 10.7850 || LR: 0.00100000 || Batchtime: 0.7342 s || ETA: 20:27:17\n",
            "Epoch:2/250 || Epochiter: 50/403 || Iter: 453/100750 || Loc: 1.7642 Cla: 2.7791 Landm: 7.1406 || LR: 0.00100000 || Batchtime: 0.6315 s || ETA: 17:35:40\n",
            "Epoch:2/250 || Epochiter: 51/403 || Iter: 454/100750 || Loc: 3.0560 Cla: 2.9837 Landm: 12.4460 || LR: 0.00100000 || Batchtime: 2.2222 s || ETA: 2 days, 13:54:35\n",
            "Epoch:2/250 || Epochiter: 52/403 || Iter: 455/100750 || Loc: 2.6123 Cla: 3.0040 Landm: 11.4737 || LR: 0.00100000 || Batchtime: 0.5590 s || ETA: 15:34:23\n",
            "Epoch:2/250 || Epochiter: 53/403 || Iter: 456/100750 || Loc: 2.1739 Cla: 2.8249 Landm: 8.6752 || LR: 0.00100000 || Batchtime: 0.7176 s || ETA: 19:59:32\n",
            "Epoch:2/250 || Epochiter: 54/403 || Iter: 457/100750 || Loc: 2.6838 Cla: 3.0063 Landm: 9.7926 || LR: 0.00100000 || Batchtime: 0.8936 s || ETA: 1 day, 0:53:40\n",
            "Epoch:2/250 || Epochiter: 55/403 || Iter: 458/100750 || Loc: 2.5224 Cla: 2.9930 Landm: 11.0121 || LR: 0.00100000 || Batchtime: 2.4796 s || ETA: 2 days, 21:04:44\n",
            "Epoch:2/250 || Epochiter: 56/403 || Iter: 459/100750 || Loc: 2.7350 Cla: 2.9915 Landm: 10.1268 || LR: 0.00100000 || Batchtime: 0.6721 s || ETA: 18:43:30\n",
            "Epoch:2/250 || Epochiter: 57/403 || Iter: 460/100750 || Loc: 2.4771 Cla: 2.9930 Landm: 10.0303 || LR: 0.00100000 || Batchtime: 0.4497 s || ETA: 12:31:43\n",
            "Epoch:2/250 || Epochiter: 58/403 || Iter: 461/100750 || Loc: 2.8713 Cla: 3.0941 Landm: 13.2458 || LR: 0.00100000 || Batchtime: 1.0228 s || ETA: 1 day, 4:29:39\n",
            "Epoch:2/250 || Epochiter: 59/403 || Iter: 462/100750 || Loc: 2.5132 Cla: 3.0327 Landm: 11.2405 || LR: 0.00100000 || Batchtime: 1.9963 s || ETA: 2 days, 7:36:48\n",
            "Epoch:2/250 || Epochiter: 60/403 || Iter: 463/100750 || Loc: 2.6878 Cla: 3.0508 Landm: 9.6540 || LR: 0.00100000 || Batchtime: 0.5755 s || ETA: 16:01:58\n",
            "Epoch:2/250 || Epochiter: 61/403 || Iter: 464/100750 || Loc: 2.6195 Cla: 2.9824 Landm: 9.6353 || LR: 0.00100000 || Batchtime: 0.8424 s || ETA: 23:28:02\n",
            "Epoch:2/250 || Epochiter: 62/403 || Iter: 465/100750 || Loc: 2.9339 Cla: 3.1789 Landm: 11.5143 || LR: 0.00100000 || Batchtime: 1.2829 s || ETA: 1 day, 11:44:15\n",
            "Epoch:2/250 || Epochiter: 63/403 || Iter: 466/100750 || Loc: 2.6961 Cla: 3.0635 Landm: 10.1989 || LR: 0.00100000 || Batchtime: 2.4568 s || ETA: 2 days, 20:26:18\n",
            "Epoch:2/250 || Epochiter: 64/403 || Iter: 467/100750 || Loc: 2.9062 Cla: 3.0727 Landm: 11.9273 || LR: 0.00100000 || Batchtime: 0.5682 s || ETA: 15:49:40\n",
            "Epoch:2/250 || Epochiter: 65/403 || Iter: 468/100750 || Loc: 2.3383 Cla: 2.8713 Landm: 9.9067 || LR: 0.00100000 || Batchtime: 0.6761 s || ETA: 18:50:01\n",
            "Epoch:2/250 || Epochiter: 66/403 || Iter: 469/100750 || Loc: 2.6933 Cla: 3.0807 Landm: 10.3430 || LR: 0.00100000 || Batchtime: 0.9384 s || ETA: 1 day, 2:08:28\n",
            "Epoch:2/250 || Epochiter: 67/403 || Iter: 470/100750 || Loc: 2.3499 Cla: 2.9098 Landm: 10.1214 || LR: 0.00100000 || Batchtime: 2.5888 s || ETA: 3 days, 0:06:51\n",
            "Epoch:2/250 || Epochiter: 68/403 || Iter: 471/100750 || Loc: 2.4743 Cla: 3.0620 Landm: 10.8714 || LR: 0.00100000 || Batchtime: 0.6454 s || ETA: 17:58:37\n",
            "Epoch:2/250 || Epochiter: 69/403 || Iter: 472/100750 || Loc: 2.9464 Cla: 3.1172 Landm: 10.5592 || LR: 0.00100000 || Batchtime: 0.7994 s || ETA: 22:16:06\n",
            "Epoch:2/250 || Epochiter: 70/403 || Iter: 473/100750 || Loc: 3.0444 Cla: 3.1461 Landm: 12.7976 || LR: 0.00100000 || Batchtime: 0.5759 s || ETA: 16:02:30\n",
            "Epoch:2/250 || Epochiter: 71/403 || Iter: 474/100750 || Loc: 3.0016 Cla: 3.1162 Landm: 12.0320 || LR: 0.00100000 || Batchtime: 1.9531 s || ETA: 2 days, 6:24:11\n",
            "Epoch:2/250 || Epochiter: 72/403 || Iter: 475/100750 || Loc: 2.9572 Cla: 3.0570 Landm: 10.9516 || LR: 0.00100000 || Batchtime: 0.5878 s || ETA: 16:22:22\n",
            "Epoch:2/250 || Epochiter: 73/403 || Iter: 476/100750 || Loc: 2.2555 Cla: 2.8226 Landm: 8.8062 || LR: 0.00100000 || Batchtime: 0.5974 s || ETA: 16:38:29\n",
            "Epoch:2/250 || Epochiter: 74/403 || Iter: 477/100750 || Loc: 2.4601 Cla: 2.9935 Landm: 11.0802 || LR: 0.00100000 || Batchtime: 1.3589 s || ETA: 1 day, 13:51:05\n",
            "Epoch:2/250 || Epochiter: 75/403 || Iter: 478/100750 || Loc: 2.4837 Cla: 3.0352 Landm: 10.3055 || LR: 0.00100000 || Batchtime: 1.9385 s || ETA: 2 days, 5:59:43\n",
            "Epoch:2/250 || Epochiter: 76/403 || Iter: 479/100750 || Loc: 2.1669 Cla: 2.8678 Landm: 7.6675 || LR: 0.00100000 || Batchtime: 0.5642 s || ETA: 15:42:57\n",
            "Epoch:2/250 || Epochiter: 77/403 || Iter: 480/100750 || Loc: 2.6413 Cla: 3.0213 Landm: 11.2676 || LR: 0.00100000 || Batchtime: 0.5890 s || ETA: 16:24:23\n",
            "Epoch:2/250 || Epochiter: 78/403 || Iter: 481/100750 || Loc: 2.5791 Cla: 2.9800 Landm: 11.2801 || LR: 0.00100000 || Batchtime: 1.3548 s || ETA: 1 day, 13:44:06\n",
            "Epoch:2/250 || Epochiter: 79/403 || Iter: 482/100750 || Loc: 2.1892 Cla: 2.8152 Landm: 8.6622 || LR: 0.00100000 || Batchtime: 1.8358 s || ETA: 2 days, 3:07:56\n",
            "Epoch:2/250 || Epochiter: 80/403 || Iter: 483/100750 || Loc: 2.8152 Cla: 3.0159 Landm: 9.8825 || LR: 0.00100000 || Batchtime: 0.7269 s || ETA: 20:14:46\n",
            "Epoch:2/250 || Epochiter: 81/403 || Iter: 484/100750 || Loc: 2.3341 Cla: 2.8902 Landm: 10.5794 || LR: 0.00100000 || Batchtime: 0.6972 s || ETA: 19:25:07\n",
            "Epoch:2/250 || Epochiter: 82/403 || Iter: 485/100750 || Loc: 2.2516 Cla: 2.8951 Landm: 10.5613 || LR: 0.00100000 || Batchtime: 0.6160 s || ETA: 17:09:22\n",
            "Epoch:2/250 || Epochiter: 83/403 || Iter: 486/100750 || Loc: 2.9279 Cla: 3.1133 Landm: 10.4533 || LR: 0.00100000 || Batchtime: 1.8915 s || ETA: 2 days, 4:40:47\n",
            "Epoch:2/250 || Epochiter: 84/403 || Iter: 487/100750 || Loc: 2.7115 Cla: 3.0276 Landm: 9.4762 || LR: 0.00100000 || Batchtime: 0.7704 s || ETA: 21:27:22\n",
            "Epoch:2/250 || Epochiter: 85/403 || Iter: 488/100750 || Loc: 2.5481 Cla: 2.9192 Landm: 8.7714 || LR: 0.00100000 || Batchtime: 1.3719 s || ETA: 1 day, 14:12:29\n",
            "Epoch:2/250 || Epochiter: 86/403 || Iter: 489/100750 || Loc: 2.4545 Cla: 3.0552 Landm: 9.4102 || LR: 0.00100000 || Batchtime: 0.7187 s || ETA: 20:00:53\n",
            "Epoch:2/250 || Epochiter: 87/403 || Iter: 490/100750 || Loc: 2.6330 Cla: 2.9904 Landm: 10.5585 || LR: 0.00100000 || Batchtime: 2.0665 s || ETA: 2 days, 9:33:09\n",
            "Epoch:2/250 || Epochiter: 88/403 || Iter: 491/100750 || Loc: 2.4750 Cla: 2.9757 Landm: 10.2238 || LR: 0.00100000 || Batchtime: 0.6433 s || ETA: 17:55:02\n",
            "Epoch:2/250 || Epochiter: 89/403 || Iter: 492/100750 || Loc: 2.6624 Cla: 3.0054 Landm: 11.1483 || LR: 0.00100000 || Batchtime: 1.4819 s || ETA: 1 day, 17:16:17\n",
            "Epoch:2/250 || Epochiter: 90/403 || Iter: 493/100750 || Loc: 2.4052 Cla: 2.8967 Landm: 9.5498 || LR: 0.00100000 || Batchtime: 0.5212 s || ETA: 14:30:52\n",
            "Epoch:2/250 || Epochiter: 91/403 || Iter: 494/100750 || Loc: 2.3043 Cla: 2.9014 Landm: 9.2548 || LR: 0.00100000 || Batchtime: 2.6162 s || ETA: 3 days, 0:51:32\n",
            "Epoch:2/250 || Epochiter: 92/403 || Iter: 495/100750 || Loc: 2.8108 Cla: 3.0495 Landm: 11.0943 || LR: 0.00100000 || Batchtime: 0.7341 s || ETA: 20:26:42\n",
            "Epoch:2/250 || Epochiter: 93/403 || Iter: 496/100750 || Loc: 2.7614 Cla: 3.0955 Landm: 9.8922 || LR: 0.00100000 || Batchtime: 0.5646 s || ETA: 15:43:22\n",
            "Epoch:2/250 || Epochiter: 94/403 || Iter: 497/100750 || Loc: 2.6497 Cla: 2.9818 Landm: 10.6052 || LR: 0.00100000 || Batchtime: 0.6177 s || ETA: 17:12:07\n",
            "Epoch:2/250 || Epochiter: 95/403 || Iter: 498/100750 || Loc: 2.6326 Cla: 2.9574 Landm: 9.8967 || LR: 0.00100000 || Batchtime: 2.4835 s || ETA: 2 days, 21:09:33\n",
            "Epoch:2/250 || Epochiter: 96/403 || Iter: 499/100750 || Loc: 2.7755 Cla: 2.9684 Landm: 11.6298 || LR: 0.00100000 || Batchtime: 0.7220 s || ETA: 20:06:17\n",
            "Epoch:2/250 || Epochiter: 97/403 || Iter: 500/100750 || Loc: 2.5959 Cla: 2.8868 Landm: 9.6106 || LR: 0.00100000 || Batchtime: 0.6933 s || ETA: 19:18:19\n",
            "Epoch:2/250 || Epochiter: 98/403 || Iter: 501/100750 || Loc: 2.6893 Cla: 3.0045 Landm: 11.3122 || LR: 0.00100000 || Batchtime: 0.6278 s || ETA: 17:28:56\n",
            "Epoch:2/250 || Epochiter: 99/403 || Iter: 502/100750 || Loc: 2.2823 Cla: 2.9546 Landm: 9.2348 || LR: 0.00100000 || Batchtime: 2.9846 s || ETA: 3 days, 11:06:43\n",
            "Epoch:2/250 || Epochiter: 100/403 || Iter: 503/100750 || Loc: 2.2462 Cla: 2.9130 Landm: 9.4418 || LR: 0.00100000 || Batchtime: 0.7278 s || ETA: 20:16:01\n",
            "Epoch:2/250 || Epochiter: 101/403 || Iter: 504/100750 || Loc: 2.5461 Cla: 2.9388 Landm: 10.9404 || LR: 0.00100000 || Batchtime: 0.6329 s || ETA: 17:37:26\n",
            "Epoch:2/250 || Epochiter: 102/403 || Iter: 505/100750 || Loc: 2.1599 Cla: 2.8600 Landm: 8.1903 || LR: 0.00100000 || Batchtime: 0.5468 s || ETA: 15:13:33\n",
            "Epoch:2/250 || Epochiter: 103/403 || Iter: 506/100750 || Loc: 2.6615 Cla: 2.9300 Landm: 9.8061 || LR: 0.00100000 || Batchtime: 2.8844 s || ETA: 3 days, 8:19:05\n",
            "Epoch:2/250 || Epochiter: 104/403 || Iter: 507/100750 || Loc: 2.1339 Cla: 2.7661 Landm: 9.1165 || LR: 0.00100000 || Batchtime: 0.6147 s || ETA: 17:06:56\n",
            "Epoch:2/250 || Epochiter: 105/403 || Iter: 508/100750 || Loc: 2.7180 Cla: 2.9375 Landm: 10.3415 || LR: 0.00100000 || Batchtime: 0.5283 s || ETA: 14:42:33\n",
            "Epoch:2/250 || Epochiter: 106/403 || Iter: 509/100750 || Loc: 2.0787 Cla: 2.6619 Landm: 8.1653 || LR: 0.00100000 || Batchtime: 0.6075 s || ETA: 16:54:58\n",
            "Epoch:2/250 || Epochiter: 107/403 || Iter: 510/100750 || Loc: 2.4043 Cla: 3.0878 Landm: 10.4311 || LR: 0.00100000 || Batchtime: 4.0158 s || ETA: 4 days, 15:49:03\n",
            "Epoch:2/250 || Epochiter: 108/403 || Iter: 511/100750 || Loc: 2.9432 Cla: 2.9970 Landm: 10.8284 || LR: 0.00100000 || Batchtime: 0.5004 s || ETA: 13:56:00\n",
            "Epoch:2/250 || Epochiter: 109/403 || Iter: 512/100750 || Loc: 2.8765 Cla: 2.9485 Landm: 9.8131 || LR: 0.00100000 || Batchtime: 0.6287 s || ETA: 17:30:16\n",
            "Epoch:2/250 || Epochiter: 110/403 || Iter: 513/100750 || Loc: 2.5934 Cla: 2.9476 Landm: 9.6667 || LR: 0.00100000 || Batchtime: 0.6090 s || ETA: 16:57:25\n",
            "Epoch:2/250 || Epochiter: 111/403 || Iter: 514/100750 || Loc: 2.6014 Cla: 2.9760 Landm: 11.4851 || LR: 0.00100000 || Batchtime: 3.0467 s || ETA: 3 days, 12:49:47\n",
            "Epoch:2/250 || Epochiter: 112/403 || Iter: 515/100750 || Loc: 3.2385 Cla: 3.0761 Landm: 11.2812 || LR: 0.00100000 || Batchtime: 0.4339 s || ETA: 12:04:51\n",
            "Epoch:2/250 || Epochiter: 113/403 || Iter: 516/100750 || Loc: 2.4271 Cla: 3.0044 Landm: 9.4474 || LR: 0.00100000 || Batchtime: 0.5601 s || ETA: 15:35:45\n",
            "Epoch:2/250 || Epochiter: 114/403 || Iter: 517/100750 || Loc: 2.3093 Cla: 2.9234 Landm: 9.1703 || LR: 0.00100000 || Batchtime: 0.5533 s || ETA: 15:24:16\n",
            "Epoch:2/250 || Epochiter: 115/403 || Iter: 518/100750 || Loc: 2.6444 Cla: 2.8677 Landm: 9.4643 || LR: 0.00100000 || Batchtime: 3.7866 s || ETA: 4 days, 9:25:41\n",
            "Epoch:2/250 || Epochiter: 116/403 || Iter: 519/100750 || Loc: 2.6904 Cla: 2.9189 Landm: 10.6639 || LR: 0.00100000 || Batchtime: 0.7946 s || ETA: 22:07:23\n",
            "Epoch:2/250 || Epochiter: 117/403 || Iter: 520/100750 || Loc: 2.7290 Cla: 3.1377 Landm: 10.7259 || LR: 0.00100000 || Batchtime: 0.6651 s || ETA: 18:31:06\n",
            "Epoch:2/250 || Epochiter: 118/403 || Iter: 521/100750 || Loc: 2.3516 Cla: 2.9162 Landm: 11.8000 || LR: 0.00100000 || Batchtime: 0.5416 s || ETA: 15:04:48\n",
            "Epoch:2/250 || Epochiter: 119/403 || Iter: 522/100750 || Loc: 2.6581 Cla: 3.0532 Landm: 10.3180 || LR: 0.00100000 || Batchtime: 3.6548 s || ETA: 4 days, 5:45:15\n",
            "Epoch:2/250 || Epochiter: 120/403 || Iter: 523/100750 || Loc: 2.0937 Cla: 2.7968 Landm: 7.8592 || LR: 0.00100000 || Batchtime: 0.3517 s || ETA: 9:47:33\n",
            "Epoch:2/250 || Epochiter: 121/403 || Iter: 524/100750 || Loc: 2.8015 Cla: 2.9937 Landm: 11.0062 || LR: 0.00100000 || Batchtime: 0.5377 s || ETA: 14:58:07\n",
            "Epoch:2/250 || Epochiter: 122/403 || Iter: 525/100750 || Loc: 2.8466 Cla: 2.9762 Landm: 10.4531 || LR: 0.00100000 || Batchtime: 0.5176 s || ETA: 14:24:32\n",
            "Epoch:2/250 || Epochiter: 123/403 || Iter: 526/100750 || Loc: 2.4416 Cla: 2.9825 Landm: 8.5438 || LR: 0.00100000 || Batchtime: 2.5012 s || ETA: 2 days, 21:38:03\n",
            "Epoch:2/250 || Epochiter: 124/403 || Iter: 527/100750 || Loc: 2.5630 Cla: 2.9364 Landm: 10.4043 || LR: 0.00100000 || Batchtime: 0.5939 s || ETA: 16:32:06\n",
            "Epoch:2/250 || Epochiter: 125/403 || Iter: 528/100750 || Loc: 2.5071 Cla: 2.8584 Landm: 9.6893 || LR: 0.00100000 || Batchtime: 0.6029 s || ETA: 16:47:00\n",
            "Epoch:2/250 || Epochiter: 126/403 || Iter: 529/100750 || Loc: 2.6127 Cla: 2.9788 Landm: 10.4046 || LR: 0.00100000 || Batchtime: 0.5280 s || ETA: 14:41:56\n",
            "Epoch:2/250 || Epochiter: 127/403 || Iter: 530/100750 || Loc: 2.2773 Cla: 2.8024 Landm: 8.4996 || LR: 0.00100000 || Batchtime: 3.3621 s || ETA: 3 days, 21:35:54\n",
            "Epoch:2/250 || Epochiter: 128/403 || Iter: 531/100750 || Loc: 2.7003 Cla: 2.9664 Landm: 11.0421 || LR: 0.00100000 || Batchtime: 0.5527 s || ETA: 15:23:07\n",
            "Epoch:2/250 || Epochiter: 129/403 || Iter: 532/100750 || Loc: 2.2377 Cla: 2.9876 Landm: 9.1437 || LR: 0.00100000 || Batchtime: 0.6270 s || ETA: 17:27:21\n",
            "Epoch:2/250 || Epochiter: 130/403 || Iter: 533/100750 || Loc: 2.4183 Cla: 2.9483 Landm: 9.8321 || LR: 0.00100000 || Batchtime: 0.6533 s || ETA: 18:11:12\n",
            "Epoch:2/250 || Epochiter: 131/403 || Iter: 534/100750 || Loc: 2.4087 Cla: 2.9183 Landm: 8.7714 || LR: 0.00100000 || Batchtime: 2.6900 s || ETA: 3 days, 2:53:02\n",
            "Epoch:2/250 || Epochiter: 132/403 || Iter: 535/100750 || Loc: 2.0813 Cla: 2.9473 Landm: 8.4063 || LR: 0.00100000 || Batchtime: 0.5418 s || ETA: 15:04:58\n",
            "Epoch:2/250 || Epochiter: 133/403 || Iter: 536/100750 || Loc: 2.4090 Cla: 2.9365 Landm: 9.5133 || LR: 0.00100000 || Batchtime: 0.7292 s || ETA: 20:17:54\n",
            "Epoch:2/250 || Epochiter: 134/403 || Iter: 537/100750 || Loc: 2.0844 Cla: 2.7880 Landm: 8.2207 || LR: 0.00100000 || Batchtime: 0.5410 s || ETA: 15:03:31\n",
            "Epoch:2/250 || Epochiter: 135/403 || Iter: 538/100750 || Loc: 2.3674 Cla: 2.7778 Landm: 8.7870 || LR: 0.00100000 || Batchtime: 2.9236 s || ETA: 3 days, 9:23:07\n",
            "Epoch:2/250 || Epochiter: 136/403 || Iter: 539/100750 || Loc: 2.2561 Cla: 2.8450 Landm: 8.3776 || LR: 0.00100000 || Batchtime: 0.6092 s || ETA: 16:57:25\n",
            "Epoch:2/250 || Epochiter: 137/403 || Iter: 540/100750 || Loc: 2.6781 Cla: 2.9914 Landm: 10.5449 || LR: 0.00100000 || Batchtime: 0.5808 s || ETA: 16:10:04\n",
            "Epoch:2/250 || Epochiter: 138/403 || Iter: 541/100750 || Loc: 2.4894 Cla: 2.9346 Landm: 10.2683 || LR: 0.00100000 || Batchtime: 0.4227 s || ETA: 11:45:59\n",
            "Epoch:2/250 || Epochiter: 139/403 || Iter: 542/100750 || Loc: 1.9910 Cla: 2.8066 Landm: 8.8525 || LR: 0.00100000 || Batchtime: 2.7954 s || ETA: 3 days, 5:48:45\n",
            "Epoch:2/250 || Epochiter: 140/403 || Iter: 543/100750 || Loc: 2.4548 Cla: 3.0673 Landm: 9.0211 || LR: 0.00100000 || Batchtime: 0.5503 s || ETA: 15:19:06\n",
            "Epoch:2/250 || Epochiter: 141/403 || Iter: 544/100750 || Loc: 2.6010 Cla: 2.9067 Landm: 10.0969 || LR: 0.00100000 || Batchtime: 0.5555 s || ETA: 15:27:45\n",
            "Epoch:2/250 || Epochiter: 142/403 || Iter: 545/100750 || Loc: 2.1984 Cla: 2.7839 Landm: 8.7033 || LR: 0.00100000 || Batchtime: 0.5365 s || ETA: 14:56:03\n",
            "Epoch:2/250 || Epochiter: 143/403 || Iter: 546/100750 || Loc: 2.0795 Cla: 2.7704 Landm: 8.3241 || LR: 0.00100000 || Batchtime: 2.8839 s || ETA: 3 days, 8:16:17\n",
            "Epoch:2/250 || Epochiter: 144/403 || Iter: 547/100750 || Loc: 2.8742 Cla: 3.0453 Landm: 11.5075 || LR: 0.00100000 || Batchtime: 0.5138 s || ETA: 14:18:07\n",
            "Epoch:2/250 || Epochiter: 145/403 || Iter: 548/100750 || Loc: 2.6792 Cla: 2.9228 Landm: 10.9783 || LR: 0.00100000 || Batchtime: 0.6393 s || ETA: 17:47:43\n",
            "Epoch:2/250 || Epochiter: 146/403 || Iter: 549/100750 || Loc: 2.7277 Cla: 2.9388 Landm: 11.6690 || LR: 0.00100000 || Batchtime: 0.5830 s || ETA: 16:13:36\n",
            "Epoch:2/250 || Epochiter: 147/403 || Iter: 550/100750 || Loc: 2.4421 Cla: 2.8883 Landm: 9.8347 || LR: 0.00100000 || Batchtime: 2.7516 s || ETA: 3 days, 4:35:11\n",
            "Epoch:2/250 || Epochiter: 148/403 || Iter: 551/100750 || Loc: 2.5882 Cla: 2.9531 Landm: 10.9908 || LR: 0.00100000 || Batchtime: 0.5713 s || ETA: 15:54:04\n",
            "Epoch:2/250 || Epochiter: 149/403 || Iter: 552/100750 || Loc: 2.2001 Cla: 2.7787 Landm: 8.0024 || LR: 0.00100000 || Batchtime: 0.4605 s || ETA: 12:49:02\n",
            "Epoch:2/250 || Epochiter: 150/403 || Iter: 553/100750 || Loc: 2.2855 Cla: 2.7713 Landm: 8.8485 || LR: 0.00100000 || Batchtime: 0.6146 s || ETA: 17:06:17\n",
            "Epoch:2/250 || Epochiter: 151/403 || Iter: 554/100750 || Loc: 2.3846 Cla: 2.8655 Landm: 8.6696 || LR: 0.00100000 || Batchtime: 3.0642 s || ETA: 3 days, 13:17:01\n",
            "Epoch:2/250 || Epochiter: 152/403 || Iter: 555/100750 || Loc: 2.3375 Cla: 2.8372 Landm: 8.5567 || LR: 0.00100000 || Batchtime: 0.4875 s || ETA: 13:34:03\n",
            "Epoch:2/250 || Epochiter: 153/403 || Iter: 556/100750 || Loc: 2.5195 Cla: 2.9219 Landm: 10.5376 || LR: 0.00100000 || Batchtime: 0.6538 s || ETA: 18:11:49\n",
            "Epoch:2/250 || Epochiter: 154/403 || Iter: 557/100750 || Loc: 2.3200 Cla: 2.9576 Landm: 9.5277 || LR: 0.00100000 || Batchtime: 0.6014 s || ETA: 16:44:12\n",
            "Epoch:2/250 || Epochiter: 155/403 || Iter: 558/100750 || Loc: 2.2429 Cla: 2.7784 Landm: 7.9845 || LR: 0.00100000 || Batchtime: 3.0699 s || ETA: 3 days, 13:26:18\n",
            "Epoch:2/250 || Epochiter: 156/403 || Iter: 559/100750 || Loc: 2.9097 Cla: 2.9454 Landm: 11.3988 || LR: 0.00100000 || Batchtime: 0.6034 s || ETA: 16:47:35\n",
            "Epoch:2/250 || Epochiter: 157/403 || Iter: 560/100750 || Loc: 3.1679 Cla: 2.8804 Landm: 10.0120 || LR: 0.00100000 || Batchtime: 0.6294 s || ETA: 17:31:03\n",
            "Epoch:2/250 || Epochiter: 158/403 || Iter: 561/100750 || Loc: 2.5800 Cla: 3.0247 Landm: 10.0861 || LR: 0.00100000 || Batchtime: 0.6023 s || ETA: 16:45:40\n",
            "Epoch:2/250 || Epochiter: 159/403 || Iter: 562/100750 || Loc: 2.5411 Cla: 2.9014 Landm: 9.0010 || LR: 0.00100000 || Batchtime: 2.7290 s || ETA: 3 days, 3:56:55\n",
            "Epoch:2/250 || Epochiter: 160/403 || Iter: 563/100750 || Loc: 1.9984 Cla: 2.7575 Landm: 7.7330 || LR: 0.00100000 || Batchtime: 0.3619 s || ETA: 10:04:16\n",
            "Epoch:2/250 || Epochiter: 161/403 || Iter: 564/100750 || Loc: 2.3544 Cla: 2.7827 Landm: 8.2687 || LR: 0.00100000 || Batchtime: 0.5831 s || ETA: 16:13:38\n",
            "Epoch:2/250 || Epochiter: 162/403 || Iter: 565/100750 || Loc: 2.5535 Cla: 2.9804 Landm: 9.9016 || LR: 0.00100000 || Batchtime: 0.6184 s || ETA: 17:12:33\n",
            "Epoch:2/250 || Epochiter: 163/403 || Iter: 566/100750 || Loc: 2.5140 Cla: 2.9063 Landm: 8.8009 || LR: 0.00100000 || Batchtime: 2.8911 s || ETA: 3 days, 8:27:21\n",
            "Epoch:2/250 || Epochiter: 164/403 || Iter: 567/100750 || Loc: 2.4786 Cla: 2.8288 Landm: 9.4363 || LR: 0.00100000 || Batchtime: 0.6871 s || ETA: 19:07:12\n",
            "Epoch:2/250 || Epochiter: 165/403 || Iter: 568/100750 || Loc: 3.1121 Cla: 2.9434 Landm: 10.2321 || LR: 0.00100000 || Batchtime: 0.6886 s || ETA: 19:09:42\n",
            "Epoch:2/250 || Epochiter: 166/403 || Iter: 569/100750 || Loc: 2.5020 Cla: 2.8988 Landm: 9.8270 || LR: 0.00100000 || Batchtime: 0.5528 s || ETA: 15:23:01\n",
            "Epoch:2/250 || Epochiter: 167/403 || Iter: 570/100750 || Loc: 2.1824 Cla: 2.7243 Landm: 8.4485 || LR: 0.00100000 || Batchtime: 2.5160 s || ETA: 2 days, 22:00:52\n",
            "Epoch:2/250 || Epochiter: 168/403 || Iter: 571/100750 || Loc: 3.0117 Cla: 2.9013 Landm: 8.9248 || LR: 0.00100000 || Batchtime: 0.6936 s || ETA: 19:18:02\n",
            "Epoch:2/250 || Epochiter: 169/403 || Iter: 572/100750 || Loc: 2.7580 Cla: 2.9633 Landm: 10.2323 || LR: 0.00100000 || Batchtime: 0.6547 s || ETA: 18:13:08\n",
            "Epoch:2/250 || Epochiter: 170/403 || Iter: 573/100750 || Loc: 2.2265 Cla: 2.7761 Landm: 9.2454 || LR: 0.00100000 || Batchtime: 0.7075 s || ETA: 19:41:12\n",
            "Epoch:2/250 || Epochiter: 171/403 || Iter: 574/100750 || Loc: 2.6838 Cla: 3.0037 Landm: 9.9183 || LR: 0.00100000 || Batchtime: 2.5709 s || ETA: 2 days, 23:32:29\n",
            "Epoch:2/250 || Epochiter: 172/403 || Iter: 575/100750 || Loc: 2.1714 Cla: 2.7940 Landm: 8.5490 || LR: 0.00100000 || Batchtime: 0.6683 s || ETA: 18:35:48\n",
            "Epoch:2/250 || Epochiter: 173/403 || Iter: 576/100750 || Loc: 2.2594 Cla: 3.0041 Landm: 9.8563 || LR: 0.00100000 || Batchtime: 0.6281 s || ETA: 17:28:37\n",
            "Epoch:2/250 || Epochiter: 174/403 || Iter: 577/100750 || Loc: 2.3916 Cla: 2.8667 Landm: 9.4502 || LR: 0.00100000 || Batchtime: 0.5345 s || ETA: 14:52:22\n",
            "Epoch:2/250 || Epochiter: 175/403 || Iter: 578/100750 || Loc: 2.3401 Cla: 2.8837 Landm: 7.8288 || LR: 0.00100000 || Batchtime: 2.5241 s || ETA: 2 days, 22:14:06\n",
            "Epoch:2/250 || Epochiter: 176/403 || Iter: 579/100750 || Loc: 2.2909 Cla: 2.8625 Landm: 11.0278 || LR: 0.00100000 || Batchtime: 0.6096 s || ETA: 16:57:43\n",
            "Epoch:2/250 || Epochiter: 177/403 || Iter: 580/100750 || Loc: 2.5150 Cla: 2.7911 Landm: 7.0859 || LR: 0.00100000 || Batchtime: 0.6354 s || ETA: 17:40:52\n",
            "Epoch:2/250 || Epochiter: 178/403 || Iter: 581/100750 || Loc: 2.3769 Cla: 2.9136 Landm: 8.8374 || LR: 0.00100000 || Batchtime: 0.6130 s || ETA: 17:03:26\n",
            "Epoch:2/250 || Epochiter: 179/403 || Iter: 582/100750 || Loc: 2.2959 Cla: 2.7994 Landm: 8.8258 || LR: 0.00100000 || Batchtime: 3.2605 s || ETA: 3 days, 18:43:16\n",
            "Epoch:2/250 || Epochiter: 180/403 || Iter: 583/100750 || Loc: 2.1400 Cla: 2.7903 Landm: 7.5454 || LR: 0.00100000 || Batchtime: 0.6147 s || ETA: 17:06:17\n",
            "Epoch:2/250 || Epochiter: 181/403 || Iter: 584/100750 || Loc: 2.0593 Cla: 2.7170 Landm: 8.0139 || LR: 0.00100000 || Batchtime: 0.5456 s || ETA: 15:10:51\n",
            "Epoch:2/250 || Epochiter: 182/403 || Iter: 585/100750 || Loc: 2.3281 Cla: 2.8536 Landm: 8.6699 || LR: 0.00100000 || Batchtime: 0.6169 s || ETA: 17:09:55\n",
            "Epoch:2/250 || Epochiter: 183/403 || Iter: 586/100750 || Loc: 2.7033 Cla: 2.9272 Landm: 10.0303 || LR: 0.00100000 || Batchtime: 2.9138 s || ETA: 3 days, 9:04:22\n",
            "Epoch:2/250 || Epochiter: 184/403 || Iter: 587/100750 || Loc: 2.0973 Cla: 2.7448 Landm: 7.5797 || LR: 0.00100000 || Batchtime: 0.6020 s || ETA: 16:44:55\n",
            "Epoch:2/250 || Epochiter: 185/403 || Iter: 588/100750 || Loc: 2.1978 Cla: 2.8324 Landm: 7.6382 || LR: 0.00100000 || Batchtime: 0.5178 s || ETA: 14:24:27\n",
            "Epoch:2/250 || Epochiter: 186/403 || Iter: 589/100750 || Loc: 2.1387 Cla: 2.7469 Landm: 7.9091 || LR: 0.00100000 || Batchtime: 0.5398 s || ETA: 15:01:10\n",
            "Epoch:2/250 || Epochiter: 187/403 || Iter: 590/100750 || Loc: 2.7597 Cla: 2.8341 Landm: 8.5178 || LR: 0.00100000 || Batchtime: 2.3958 s || ETA: 2 days, 18:39:29\n",
            "Epoch:2/250 || Epochiter: 188/403 || Iter: 591/100750 || Loc: 2.5750 Cla: 2.9579 Landm: 10.4236 || LR: 0.00100000 || Batchtime: 0.5673 s || ETA: 15:46:58\n",
            "Epoch:2/250 || Epochiter: 189/403 || Iter: 592/100750 || Loc: 2.8238 Cla: 2.9275 Landm: 9.6447 || LR: 0.00100000 || Batchtime: 0.6942 s || ETA: 19:18:50\n",
            "Epoch:2/250 || Epochiter: 190/403 || Iter: 593/100750 || Loc: 2.6816 Cla: 2.8463 Landm: 8.4639 || LR: 0.00100000 || Batchtime: 0.7251 s || ETA: 20:10:22\n",
            "Epoch:2/250 || Epochiter: 191/403 || Iter: 594/100750 || Loc: 2.6612 Cla: 3.0269 Landm: 10.9042 || LR: 0.00100000 || Batchtime: 1.9592 s || ETA: 2 days, 6:30:29\n",
            "Epoch:2/250 || Epochiter: 192/403 || Iter: 595/100750 || Loc: 2.1988 Cla: 2.8836 Landm: 8.6287 || LR: 0.00100000 || Batchtime: 0.4938 s || ETA: 13:44:17\n",
            "Epoch:2/250 || Epochiter: 193/403 || Iter: 596/100750 || Loc: 2.2524 Cla: 2.8722 Landm: 8.3723 || LR: 0.00100000 || Batchtime: 0.5832 s || ETA: 16:13:33\n",
            "Epoch:2/250 || Epochiter: 194/403 || Iter: 597/100750 || Loc: 2.5659 Cla: 2.9912 Landm: 8.4826 || LR: 0.00100000 || Batchtime: 0.6323 s || ETA: 17:35:30\n",
            "Epoch:2/250 || Epochiter: 195/403 || Iter: 598/100750 || Loc: 2.3753 Cla: 2.8648 Landm: 10.4434 || LR: 0.00100000 || Batchtime: 3.0043 s || ETA: 3 days, 11:34:54\n",
            "Epoch:2/250 || Epochiter: 196/403 || Iter: 599/100750 || Loc: 2.8375 Cla: 2.9550 Landm: 11.1307 || LR: 0.00100000 || Batchtime: 0.5927 s || ETA: 16:29:19\n",
            "Epoch:2/250 || Epochiter: 197/403 || Iter: 600/100750 || Loc: 2.4723 Cla: 2.9684 Landm: 10.7953 || LR: 0.00100000 || Batchtime: 0.5597 s || ETA: 15:34:14\n",
            "Epoch:2/250 || Epochiter: 198/403 || Iter: 601/100750 || Loc: 2.7303 Cla: 2.8899 Landm: 9.1915 || LR: 0.00100000 || Batchtime: 0.6200 s || ETA: 17:14:55\n",
            "Epoch:2/250 || Epochiter: 199/403 || Iter: 602/100750 || Loc: 2.6229 Cla: 2.9410 Landm: 9.8219 || LR: 0.00100000 || Batchtime: 2.5194 s || ETA: 2 days, 22:05:18\n",
            "Epoch:2/250 || Epochiter: 200/403 || Iter: 603/100750 || Loc: 2.0061 Cla: 2.8421 Landm: 8.4923 || LR: 0.00100000 || Batchtime: 0.6022 s || ETA: 16:45:06\n",
            "Epoch:2/250 || Epochiter: 201/403 || Iter: 604/100750 || Loc: 2.5889 Cla: 2.8714 Landm: 9.5367 || LR: 0.00100000 || Batchtime: 0.6005 s || ETA: 16:42:21\n",
            "Epoch:2/250 || Epochiter: 202/403 || Iter: 605/100750 || Loc: 2.3179 Cla: 2.8334 Landm: 8.6847 || LR: 0.00100000 || Batchtime: 0.5282 s || ETA: 14:41:38\n",
            "Epoch:2/250 || Epochiter: 203/403 || Iter: 606/100750 || Loc: 2.5244 Cla: 2.9010 Landm: 8.6821 || LR: 0.00100000 || Batchtime: 3.7463 s || ETA: 4 days, 8:12:58\n",
            "Epoch:2/250 || Epochiter: 204/403 || Iter: 607/100750 || Loc: 2.8207 Cla: 2.8835 Landm: 10.0503 || LR: 0.00100000 || Batchtime: 0.5212 s || ETA: 14:29:51\n",
            "Epoch:2/250 || Epochiter: 205/403 || Iter: 608/100750 || Loc: 1.7911 Cla: 2.5792 Landm: 6.1349 || LR: 0.00100000 || Batchtime: 0.4667 s || ETA: 12:58:58\n",
            "Epoch:2/250 || Epochiter: 206/403 || Iter: 609/100750 || Loc: 2.1567 Cla: 2.7815 Landm: 8.6646 || LR: 0.00100000 || Batchtime: 0.4619 s || ETA: 12:50:53\n",
            "Epoch:2/250 || Epochiter: 207/403 || Iter: 610/100750 || Loc: 3.0061 Cla: 3.0259 Landm: 9.4269 || LR: 0.00100000 || Batchtime: 2.8860 s || ETA: 3 days, 8:16:51\n",
            "Epoch:2/250 || Epochiter: 208/403 || Iter: 611/100750 || Loc: 2.5606 Cla: 2.8989 Landm: 9.5410 || LR: 0.00100000 || Batchtime: 0.5377 s || ETA: 14:57:20\n",
            "Epoch:2/250 || Epochiter: 209/403 || Iter: 612/100750 || Loc: 2.3670 Cla: 2.8850 Landm: 9.5552 || LR: 0.00100000 || Batchtime: 0.5438 s || ETA: 15:07:39\n",
            "Epoch:2/250 || Epochiter: 210/403 || Iter: 613/100750 || Loc: 2.4161 Cla: 2.9212 Landm: 9.8916 || LR: 0.00100000 || Batchtime: 0.5361 s || ETA: 14:54:47\n",
            "Epoch:2/250 || Epochiter: 211/403 || Iter: 614/100750 || Loc: 2.3138 Cla: 2.8397 Landm: 8.0401 || LR: 0.00100000 || Batchtime: 3.1939 s || ETA: 3 days, 16:50:30\n",
            "Epoch:2/250 || Epochiter: 212/403 || Iter: 615/100750 || Loc: 2.7624 Cla: 2.9542 Landm: 9.2842 || LR: 0.00100000 || Batchtime: 0.5031 s || ETA: 13:59:34\n",
            "Epoch:2/250 || Epochiter: 213/403 || Iter: 616/100750 || Loc: 2.4283 Cla: 2.8131 Landm: 9.4885 || LR: 0.00100000 || Batchtime: 0.6921 s || ETA: 19:15:03\n",
            "Epoch:2/250 || Epochiter: 214/403 || Iter: 617/100750 || Loc: 2.3796 Cla: 2.8570 Landm: 8.9647 || LR: 0.00100000 || Batchtime: 0.6314 s || ETA: 17:33:44\n",
            "Epoch:2/250 || Epochiter: 215/403 || Iter: 618/100750 || Loc: 2.2120 Cla: 2.7384 Landm: 7.3967 || LR: 0.00100000 || Batchtime: 3.2590 s || ETA: 3 days, 18:38:53\n",
            "Epoch:2/250 || Epochiter: 216/403 || Iter: 619/100750 || Loc: 2.0408 Cla: 2.7784 Landm: 6.7142 || LR: 0.00100000 || Batchtime: 0.4622 s || ETA: 12:51:24\n",
            "Epoch:2/250 || Epochiter: 217/403 || Iter: 620/100750 || Loc: 1.9805 Cla: 2.7278 Landm: 7.5923 || LR: 0.00100000 || Batchtime: 0.4535 s || ETA: 12:36:50\n",
            "Epoch:2/250 || Epochiter: 218/403 || Iter: 621/100750 || Loc: 2.2657 Cla: 2.7974 Landm: 8.9127 || LR: 0.00100000 || Batchtime: 0.7027 s || ETA: 19:32:40\n",
            "Epoch:2/250 || Epochiter: 219/403 || Iter: 622/100750 || Loc: 2.1629 Cla: 2.7816 Landm: 7.7852 || LR: 0.00100000 || Batchtime: 2.4546 s || ETA: 2 days, 20:16:15\n",
            "Epoch:2/250 || Epochiter: 220/403 || Iter: 623/100750 || Loc: 2.1220 Cla: 2.7605 Landm: 8.0104 || LR: 0.00100000 || Batchtime: 0.5083 s || ETA: 14:08:17\n",
            "Epoch:2/250 || Epochiter: 221/403 || Iter: 624/100750 || Loc: 2.3029 Cla: 2.7981 Landm: 8.1083 || LR: 0.00100000 || Batchtime: 0.6660 s || ETA: 18:31:22\n",
            "Epoch:2/250 || Epochiter: 222/403 || Iter: 625/100750 || Loc: 2.1503 Cla: 2.7336 Landm: 11.2125 || LR: 0.00100000 || Batchtime: 0.5898 s || ETA: 16:24:17\n",
            "Epoch:2/250 || Epochiter: 223/403 || Iter: 626/100750 || Loc: 2.6228 Cla: 2.9338 Landm: 10.2019 || LR: 0.00100000 || Batchtime: 2.4892 s || ETA: 2 days, 21:13:50\n",
            "Epoch:2/250 || Epochiter: 224/403 || Iter: 627/100750 || Loc: 2.8415 Cla: 2.9959 Landm: 10.8665 || LR: 0.00100000 || Batchtime: 0.6318 s || ETA: 17:34:18\n",
            "Epoch:2/250 || Epochiter: 225/403 || Iter: 628/100750 || Loc: 2.1167 Cla: 2.7581 Landm: 9.5492 || LR: 0.00100000 || Batchtime: 0.5156 s || ETA: 14:20:21\n",
            "Epoch:2/250 || Epochiter: 226/403 || Iter: 629/100750 || Loc: 2.4365 Cla: 2.8287 Landm: 9.3490 || LR: 0.00100000 || Batchtime: 0.6588 s || ETA: 18:19:20\n",
            "Epoch:2/250 || Epochiter: 227/403 || Iter: 630/100750 || Loc: 2.2168 Cla: 2.7079 Landm: 6.6969 || LR: 0.00100000 || Batchtime: 2.4710 s || ETA: 2 days, 20:43:19\n",
            "Epoch:2/250 || Epochiter: 228/403 || Iter: 631/100750 || Loc: 2.7285 Cla: 2.9225 Landm: 9.9976 || LR: 0.00100000 || Batchtime: 0.6807 s || ETA: 18:55:52\n",
            "Epoch:2/250 || Epochiter: 229/403 || Iter: 632/100750 || Loc: 2.5306 Cla: 2.8573 Landm: 10.6289 || LR: 0.00100000 || Batchtime: 0.7033 s || ETA: 19:33:37\n",
            "Epoch:2/250 || Epochiter: 230/403 || Iter: 633/100750 || Loc: 2.1692 Cla: 2.7374 Landm: 8.3513 || LR: 0.00100000 || Batchtime: 0.5976 s || ETA: 16:37:07\n",
            "Epoch:2/250 || Epochiter: 231/403 || Iter: 634/100750 || Loc: 2.4588 Cla: 2.8833 Landm: 9.8805 || LR: 0.00100000 || Batchtime: 2.9911 s || ETA: 3 days, 11:11:03\n",
            "Epoch:2/250 || Epochiter: 232/403 || Iter: 635/100750 || Loc: 2.5712 Cla: 2.9529 Landm: 9.5982 || LR: 0.00100000 || Batchtime: 0.4528 s || ETA: 12:35:33\n",
            "Epoch:2/250 || Epochiter: 233/403 || Iter: 636/100750 || Loc: 2.5019 Cla: 2.7647 Landm: 7.9588 || LR: 0.00100000 || Batchtime: 0.5221 s || ETA: 14:31:08\n",
            "Epoch:2/250 || Epochiter: 234/403 || Iter: 637/100750 || Loc: 2.5750 Cla: 2.9967 Landm: 9.2233 || LR: 0.00100000 || Batchtime: 0.6412 s || ETA: 17:49:57\n",
            "Epoch:2/250 || Epochiter: 235/403 || Iter: 638/100750 || Loc: 2.6229 Cla: 2.9760 Landm: 9.7343 || LR: 0.00100000 || Batchtime: 3.0256 s || ETA: 3 days, 12:08:23\n",
            "Epoch:2/250 || Epochiter: 236/403 || Iter: 639/100750 || Loc: 2.2584 Cla: 2.7698 Landm: 9.1749 || LR: 0.00100000 || Batchtime: 0.5359 s || ETA: 14:54:10\n",
            "Epoch:2/250 || Epochiter: 237/403 || Iter: 640/100750 || Loc: 2.0830 Cla: 2.7489 Landm: 8.6562 || LR: 0.00100000 || Batchtime: 0.4150 s || ETA: 11:32:22\n",
            "Epoch:2/250 || Epochiter: 238/403 || Iter: 641/100750 || Loc: 2.5380 Cla: 2.9122 Landm: 10.0670 || LR: 0.00100000 || Batchtime: 0.5460 s || ETA: 15:11:00\n",
            "Epoch:2/250 || Epochiter: 239/403 || Iter: 642/100750 || Loc: 2.4961 Cla: 2.8966 Landm: 10.3950 || LR: 0.00100000 || Batchtime: 2.1951 s || ETA: 2 days, 13:02:32\n",
            "Epoch:2/250 || Epochiter: 240/403 || Iter: 643/100750 || Loc: 2.4477 Cla: 2.9654 Landm: 10.0602 || LR: 0.00100000 || Batchtime: 0.5877 s || ETA: 16:20:28\n",
            "Epoch:2/250 || Epochiter: 241/403 || Iter: 644/100750 || Loc: 2.4085 Cla: 2.9562 Landm: 8.7547 || LR: 0.00100000 || Batchtime: 0.5657 s || ETA: 15:43:46\n",
            "Epoch:2/250 || Epochiter: 242/403 || Iter: 645/100750 || Loc: 2.3658 Cla: 2.9132 Landm: 9.4418 || LR: 0.00100000 || Batchtime: 0.6791 s || ETA: 18:53:01\n",
            "Epoch:2/250 || Epochiter: 243/403 || Iter: 646/100750 || Loc: 2.1421 Cla: 2.8056 Landm: 9.0151 || LR: 0.00100000 || Batchtime: 2.5985 s || ETA: 3 days, 0:15:25\n",
            "Epoch:2/250 || Epochiter: 244/403 || Iter: 647/100750 || Loc: 2.4539 Cla: 2.9686 Landm: 9.7641 || LR: 0.00100000 || Batchtime: 0.5415 s || ETA: 15:03:28\n",
            "Epoch:2/250 || Epochiter: 245/403 || Iter: 648/100750 || Loc: 2.6248 Cla: 2.8689 Landm: 9.0091 || LR: 0.00100000 || Batchtime: 0.6756 s || ETA: 18:47:11\n",
            "Epoch:2/250 || Epochiter: 246/403 || Iter: 649/100750 || Loc: 2.1900 Cla: 2.7531 Landm: 8.0633 || LR: 0.00100000 || Batchtime: 0.6068 s || ETA: 16:52:24\n",
            "Epoch:2/250 || Epochiter: 247/403 || Iter: 650/100750 || Loc: 2.0797 Cla: 2.9281 Landm: 7.9942 || LR: 0.00100000 || Batchtime: 2.9101 s || ETA: 3 days, 8:55:02\n",
            "Epoch:2/250 || Epochiter: 248/403 || Iter: 651/100750 || Loc: 2.2193 Cla: 2.7409 Landm: 8.1368 || LR: 0.00100000 || Batchtime: 0.6189 s || ETA: 17:12:34\n",
            "Epoch:2/250 || Epochiter: 249/403 || Iter: 652/100750 || Loc: 2.3954 Cla: 2.8073 Landm: 8.5841 || LR: 0.00100000 || Batchtime: 0.6593 s || ETA: 18:19:57\n",
            "Epoch:2/250 || Epochiter: 250/403 || Iter: 653/100750 || Loc: 2.5434 Cla: 2.9964 Landm: 9.9993 || LR: 0.00100000 || Batchtime: 0.4481 s || ETA: 12:27:31\n",
            "Epoch:2/250 || Epochiter: 251/403 || Iter: 654/100750 || Loc: 2.3664 Cla: 2.9754 Landm: 9.1038 || LR: 0.00100000 || Batchtime: 2.3811 s || ETA: 2 days, 18:12:24\n",
            "Epoch:2/250 || Epochiter: 252/403 || Iter: 655/100750 || Loc: 2.2689 Cla: 2.8140 Landm: 9.6474 || LR: 0.00100000 || Batchtime: 0.6992 s || ETA: 19:26:23\n",
            "Epoch:2/250 || Epochiter: 253/403 || Iter: 656/100750 || Loc: 2.5493 Cla: 2.8037 Landm: 9.1688 || LR: 0.00100000 || Batchtime: 0.9506 s || ETA: 1 day, 2:25:54\n",
            "Epoch:2/250 || Epochiter: 254/403 || Iter: 657/100750 || Loc: 2.5818 Cla: 3.0738 Landm: 10.7615 || LR: 0.00100000 || Batchtime: 0.9282 s || ETA: 1 day, 1:48:23\n",
            "Epoch:2/250 || Epochiter: 255/403 || Iter: 658/100750 || Loc: 2.1915 Cla: 2.6186 Landm: 6.7851 || LR: 0.00100000 || Batchtime: 3.9254 s || ETA: 4 days, 13:08:25\n",
            "Epoch:2/250 || Epochiter: 256/403 || Iter: 659/100750 || Loc: 1.9014 Cla: 2.6069 Landm: 7.0419 || LR: 0.00100000 || Batchtime: 0.4218 s || ETA: 11:43:37\n",
            "Epoch:2/250 || Epochiter: 257/403 || Iter: 660/100750 || Loc: 2.3328 Cla: 2.8638 Landm: 8.1175 || LR: 0.00100000 || Batchtime: 0.5136 s || ETA: 14:16:46\n",
            "Epoch:2/250 || Epochiter: 258/403 || Iter: 661/100750 || Loc: 2.6950 Cla: 2.9492 Landm: 10.3364 || LR: 0.00100000 || Batchtime: 0.5194 s || ETA: 14:26:23\n",
            "Epoch:2/250 || Epochiter: 259/403 || Iter: 662/100750 || Loc: 2.6841 Cla: 2.8755 Landm: 8.8433 || LR: 0.00100000 || Batchtime: 3.2029 s || ETA: 3 days, 17:02:59\n",
            "Epoch:2/250 || Epochiter: 260/403 || Iter: 663/100750 || Loc: 2.7695 Cla: 3.0580 Landm: 8.5484 || LR: 0.00100000 || Batchtime: 0.5886 s || ETA: 16:21:52\n",
            "Epoch:2/250 || Epochiter: 261/403 || Iter: 664/100750 || Loc: 2.3543 Cla: 2.7462 Landm: 7.7800 || LR: 0.00100000 || Batchtime: 0.5203 s || ETA: 14:27:57\n",
            "Epoch:2/250 || Epochiter: 262/403 || Iter: 665/100750 || Loc: 2.1058 Cla: 2.6715 Landm: 7.7706 || LR: 0.00100000 || Batchtime: 0.5733 s || ETA: 15:56:14\n",
            "Epoch:2/250 || Epochiter: 263/403 || Iter: 666/100750 || Loc: 1.9253 Cla: 2.5925 Landm: 7.7716 || LR: 0.00100000 || Batchtime: 2.1459 s || ETA: 2 days, 11:39:35\n",
            "Epoch:2/250 || Epochiter: 264/403 || Iter: 667/100750 || Loc: 2.1677 Cla: 2.7004 Landm: 8.7996 || LR: 0.00100000 || Batchtime: 0.5155 s || ETA: 14:19:55\n",
            "Epoch:2/250 || Epochiter: 265/403 || Iter: 668/100750 || Loc: 2.1552 Cla: 2.6735 Landm: 7.7084 || LR: 0.00100000 || Batchtime: 0.4826 s || ETA: 13:24:56\n",
            "Epoch:2/250 || Epochiter: 266/403 || Iter: 669/100750 || Loc: 2.1401 Cla: 2.7195 Landm: 8.2541 || LR: 0.00100000 || Batchtime: 0.7028 s || ETA: 19:32:16\n",
            "Epoch:2/250 || Epochiter: 267/403 || Iter: 670/100750 || Loc: 2.6835 Cla: 2.8827 Landm: 9.1772 || LR: 0.00100000 || Batchtime: 3.4495 s || ETA: 3 days, 23:53:49\n",
            "Epoch:2/250 || Epochiter: 268/403 || Iter: 671/100750 || Loc: 2.1196 Cla: 2.7349 Landm: 8.1350 || LR: 0.00100000 || Batchtime: 0.4749 s || ETA: 13:12:12\n",
            "Epoch:2/250 || Epochiter: 269/403 || Iter: 672/100750 || Loc: 2.3204 Cla: 2.7805 Landm: 8.5613 || LR: 0.00100000 || Batchtime: 0.7067 s || ETA: 19:38:50\n",
            "Epoch:2/250 || Epochiter: 270/403 || Iter: 673/100750 || Loc: 2.3996 Cla: 2.8412 Landm: 9.3807 || LR: 0.00100000 || Batchtime: 0.5506 s || ETA: 15:18:19\n",
            "Epoch:2/250 || Epochiter: 271/403 || Iter: 674/100750 || Loc: 2.4973 Cla: 2.8714 Landm: 8.8488 || LR: 0.00100000 || Batchtime: 2.7309 s || ETA: 3 days, 3:55:02\n",
            "Epoch:2/250 || Epochiter: 272/403 || Iter: 675/100750 || Loc: 2.4040 Cla: 2.7528 Landm: 9.4779 || LR: 0.00100000 || Batchtime: 0.6535 s || ETA: 18:10:00\n",
            "Epoch:2/250 || Epochiter: 273/403 || Iter: 676/100750 || Loc: 2.4348 Cla: 2.8726 Landm: 8.2074 || LR: 0.00100000 || Batchtime: 0.5815 s || ETA: 16:09:54\n",
            "Epoch:2/250 || Epochiter: 274/403 || Iter: 677/100750 || Loc: 2.0454 Cla: 2.8068 Landm: 8.7641 || LR: 0.00100000 || Batchtime: 0.5280 s || ETA: 14:40:43\n",
            "Epoch:2/250 || Epochiter: 275/403 || Iter: 678/100750 || Loc: 2.3048 Cla: 2.7733 Landm: 8.3501 || LR: 0.00100000 || Batchtime: 2.7776 s || ETA: 3 days, 5:12:44\n",
            "Epoch:2/250 || Epochiter: 276/403 || Iter: 679/100750 || Loc: 2.0191 Cla: 2.6623 Landm: 6.7123 || LR: 0.00100000 || Batchtime: 0.4110 s || ETA: 11:25:30\n",
            "Epoch:2/250 || Epochiter: 277/403 || Iter: 680/100750 || Loc: 2.7030 Cla: 2.9703 Landm: 9.1765 || LR: 0.00100000 || Batchtime: 0.4666 s || ETA: 12:58:11\n",
            "Epoch:2/250 || Epochiter: 278/403 || Iter: 681/100750 || Loc: 2.0958 Cla: 2.7178 Landm: 7.8538 || LR: 0.00100000 || Batchtime: 0.5704 s || ETA: 15:51:23\n",
            "Epoch:2/250 || Epochiter: 279/403 || Iter: 682/100750 || Loc: 2.2321 Cla: 2.8123 Landm: 8.5366 || LR: 0.00100000 || Batchtime: 3.4361 s || ETA: 3 days, 23:30:48\n",
            "Epoch:2/250 || Epochiter: 280/403 || Iter: 683/100750 || Loc: 2.4860 Cla: 2.8612 Landm: 9.4208 || LR: 0.00100000 || Batchtime: 0.4340 s || ETA: 12:03:47\n",
            "Epoch:2/250 || Epochiter: 281/403 || Iter: 684/100750 || Loc: 2.2104 Cla: 2.6974 Landm: 7.4608 || LR: 0.00100000 || Batchtime: 0.6104 s || ETA: 16:57:58\n",
            "Epoch:2/250 || Epochiter: 282/403 || Iter: 685/100750 || Loc: 2.1322 Cla: 2.6855 Landm: 8.2274 || LR: 0.00100000 || Batchtime: 0.5220 s || ETA: 14:30:37\n",
            "Epoch:2/250 || Epochiter: 283/403 || Iter: 686/100750 || Loc: 2.0681 Cla: 2.6736 Landm: 9.3101 || LR: 0.00100000 || Batchtime: 2.8116 s || ETA: 3 days, 6:09:07\n",
            "Epoch:2/250 || Epochiter: 284/403 || Iter: 687/100750 || Loc: 1.8777 Cla: 2.8632 Landm: 8.0507 || LR: 0.00100000 || Batchtime: 0.5002 s || ETA: 13:54:16\n",
            "Epoch:2/250 || Epochiter: 285/403 || Iter: 688/100750 || Loc: 1.8160 Cla: 2.7507 Landm: 7.0680 || LR: 0.00100000 || Batchtime: 0.6227 s || ETA: 17:18:25\n",
            "Epoch:2/250 || Epochiter: 286/403 || Iter: 689/100750 || Loc: 2.6269 Cla: 2.9614 Landm: 10.0301 || LR: 0.00100000 || Batchtime: 0.7106 s || ETA: 19:45:05\n"
          ]
        }
      ]
    }
  ]
}